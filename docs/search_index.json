[["nltk.html", "Chapter 16 NLTK 16.1 Installation 16.2 Corpora Data 16.3 WordNet 16.4 Discovering Word Collocations", " Chapter 16 NLTK The almighty nltk package! 16.1 Installation Install package in the terminal !pip install nltk Download nltk data in python import nltk nltk.download(&#39;all&#39;, halt_on_error=False) import nltk # nltk.download() The complete collection of the nltk.corpus is huge. You probably donâ€™t need all of the corpora data. You can use nltk.download() to initialize a User Window for installation of specific datasets. 16.2 Corpora Data The package includes a lot of pre-loaded corpora datasets The default nltk_data directory is in /Users/YOUT_NAME/nltk_data/ Selective Examples Brown Corpus Reuters Corpus WordNet from nltk.corpus import gutenberg, brown, reuters # brown corpus ## Categories (topics?) print(&#39;Brown Corpus Total Categories: &#39;, len(brown.categories())) Brown Corpus Total Categories: 15 print(&#39;Categories List: &#39;, brown.categories()) Categories List: [&#39;adventure&#39;, &#39;belles_lettres&#39;, &#39;editorial&#39;, &#39;fiction&#39;, &#39;government&#39;, &#39;hobbies&#39;, &#39;humor&#39;, &#39;learned&#39;, &#39;lore&#39;, &#39;mystery&#39;, &#39;news&#39;, &#39;religion&#39;, &#39;reviews&#39;, &#39;romance&#39;, &#39;science_fiction&#39;] # Sentences print(brown.sents()[0]) ## first sentence [&#39;The&#39;, &#39;Fulton&#39;, &#39;County&#39;, &#39;Grand&#39;, &#39;Jury&#39;, &#39;said&#39;, &#39;Friday&#39;, &#39;an&#39;, &#39;investigation&#39;, &#39;of&#39;, &quot;Atlanta&#39;s&quot;, &#39;recent&#39;, &#39;primary&#39;, &#39;election&#39;, &#39;produced&#39;, &#39;``&#39;, &#39;no&#39;, &#39;evidence&#39;, &quot;&#39;&#39;&quot;, &#39;that&#39;, &#39;any&#39;, &#39;irregularities&#39;, &#39;took&#39;, &#39;place&#39;, &#39;.&#39;] print(brown.sents(categories=&#39;fiction&#39;)) ## first sentence for fiction texts [[&#39;Thirty-three&#39;], [&#39;Scotty&#39;, &#39;did&#39;, &#39;not&#39;, &#39;go&#39;, &#39;back&#39;, &#39;to&#39;, &#39;school&#39;, &#39;.&#39;], ...] ## Tagged Sentences print(brown.tagged_sents()[0]) [(&#39;The&#39;, &#39;AT&#39;), (&#39;Fulton&#39;, &#39;NP-TL&#39;), (&#39;County&#39;, &#39;NN-TL&#39;), (&#39;Grand&#39;, &#39;JJ-TL&#39;), (&#39;Jury&#39;, &#39;NN-TL&#39;), (&#39;said&#39;, &#39;VBD&#39;), (&#39;Friday&#39;, &#39;NR&#39;), (&#39;an&#39;, &#39;AT&#39;), (&#39;investigation&#39;, &#39;NN&#39;), (&#39;of&#39;, &#39;IN&#39;), (&quot;Atlanta&#39;s&quot;, &#39;NP$&#39;), (&#39;recent&#39;, &#39;JJ&#39;), (&#39;primary&#39;, &#39;NN&#39;), (&#39;election&#39;, &#39;NN&#39;), (&#39;produced&#39;, &#39;VBD&#39;), (&#39;``&#39;, &#39;``&#39;), (&#39;no&#39;, &#39;AT&#39;), (&#39;evidence&#39;, &#39;NN&#39;), (&quot;&#39;&#39;&quot;, &quot;&#39;&#39;&quot;), (&#39;that&#39;, &#39;CS&#39;), (&#39;any&#39;, &#39;DTI&#39;), (&#39;irregularities&#39;, &#39;NNS&#39;), (&#39;took&#39;, &#39;VBD&#39;), (&#39;place&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)] ## Sentence in natural forms sents = brown.sents(categories=&#39;fiction&#39;) [&#39; &#39;.join(sent) for sent in sents[1:5]] [&#39;Scotty did not go back to school .&#39;, &#39;His parents talked seriously and lengthily to their own doctor and to a specialist at the University Hospital -- Mr. McKinley was entitled to a discount for members of his family -- and it was decided it would be best for him to take the remainder of the term off , spend a lot of time in bed and , for the rest , do pretty much as he chose -- provided , of course , he chose to do nothing too exciting or too debilitating .&#39;, &#39;His teacher and his school principal were conferred with and everyone agreed that , if he kept up with a certain amount of work at home , there was little danger of his losing a term .&#39;, &#39;Scotty accepted the decision with indifference and did not enter the arguments .&#39;] ## Get tagged words tagged_words = brown.tagged_words(categories=&#39;fiction&#39;) #print(tagged_words[1]) ## a tuple ## Get all nouns nouns = [(word, tag) for word, tag in tagged_words if any (noun_tag in tag for noun_tag in [&#39;NP&#39;,&#39;NN&#39;])] ## Check first ten nouns nouns[:10] [(&#39;Scotty&#39;, &#39;NP&#39;), (&#39;school&#39;, &#39;NN&#39;), (&#39;parents&#39;, &#39;NNS&#39;), (&#39;doctor&#39;, &#39;NN&#39;), (&#39;specialist&#39;, &#39;NN&#39;), (&#39;University&#39;, &#39;NN-TL&#39;), (&#39;Hospital&#39;, &#39;NN-TL&#39;), (&#39;Mr.&#39;, &#39;NP&#39;), (&#39;McKinley&#39;, &#39;NP&#39;), (&#39;discount&#39;, &#39;NN&#39;)] ## Creating Freq list nouns_freq = nltk.FreqDist([w for w, t in nouns]) sorted(nouns_freq.items(),key=lambda x:x[1], reverse=True)[:20] [(&#39;man&#39;, 111), (&#39;time&#39;, 99), (&#39;men&#39;, 72), (&#39;room&#39;, 63), (&#39;way&#39;, 62), (&#39;eyes&#39;, 60), (&#39;face&#39;, 55), (&#39;house&#39;, 54), (&#39;head&#39;, 54), (&#39;night&#39;, 53), (&#39;day&#39;, 52), (&#39;hand&#39;, 50), (&#39;door&#39;, 47), (&#39;life&#39;, 44), (&#39;years&#39;, 44), (&#39;Mrs.&#39;, 41), (&#39;God&#39;, 41), (&#39;Kate&#39;, 40), (&#39;Mr.&#39;, 39), (&#39;people&#39;, 39)] sorted(nouns_freq.items(),key=lambda x:x[0], reverse=True)[:20] [(&#39;zoo&#39;, 2), (&#39;zlotys&#39;, 1), (&#39;zenith&#39;, 1), (&#39;youth&#39;, 5), (&#39;yelling&#39;, 1), (&#39;years&#39;, 44), (&#39;yearning&#39;, 1), (&quot;year&#39;s&quot;, 1), (&#39;year&#39;, 9), (&#39;yards&#39;, 4), (&#39;yard&#39;, 7), (&#39;yachts&#39;, 1), (&#39;writing&#39;, 2), (&#39;writers&#39;, 1), (&#39;writer&#39;, 4), (&#39;wrists&#39;, 1), (&#39;wrist&#39;, 2), (&#39;wrinkles&#39;, 1), (&#39;wrinkle&#39;, 1), (&#39;wretch&#39;, 1)] nouns_freq.most_common(10) [(&#39;man&#39;, 111), (&#39;time&#39;, 99), (&#39;men&#39;, 72), (&#39;room&#39;, 63), (&#39;way&#39;, 62), (&#39;eyes&#39;, 60), (&#39;face&#39;, 55), (&#39;house&#39;, 54), (&#39;head&#39;, 54), (&#39;night&#39;, 53)] ## Accsess data via fileid brown.fileids(categories=&#39;fiction&#39;)[0] &#39;ck01&#39; brown.sents(fileids=&#39;ck01&#39;) [[&#39;Thirty-three&#39;], [&#39;Scotty&#39;, &#39;did&#39;, &#39;not&#39;, &#39;go&#39;, &#39;back&#39;, &#39;to&#39;, &#39;school&#39;, &#39;.&#39;], ...] 16.3 WordNet A dictionary resource from nltk.corpus import wordnet as wn word = &#39;walk&#39; # get synsets word_synsets = wn.synsets(word) word_synsets [Synset(&#39;walk.n.01&#39;), Synset(&#39;base_on_balls.n.01&#39;), Synset(&#39;walk.n.03&#39;), Synset(&#39;walk.n.04&#39;), Synset(&#39;walk.n.05&#39;), Synset(&#39;walk.n.06&#39;), Synset(&#39;walk_of_life.n.01&#39;), Synset(&#39;walk.v.01&#39;), Synset(&#39;walk.v.02&#39;), Synset(&#39;walk.v.03&#39;), Synset(&#39;walk.v.04&#39;), Synset(&#39;walk.v.05&#39;), Synset(&#39;walk.v.06&#39;), Synset(&#39;walk.v.07&#39;), Synset(&#39;walk.v.08&#39;), Synset(&#39;walk.v.09&#39;), Synset(&#39;walk.v.10&#39;)] ## Get details of each synset for s in word_synsets: if str(s.name()).startswith(&#39;walk.v&#39;): print( &#39;Syset ID: %s \\n&#39; &#39;POS Tag: %s \\n&#39; &#39;Definition: %s \\n&#39; &#39;Examples: %s \\n&#39; % (s.name(), s.pos(), s.definition(),s.examples()) ) Syset ID: walk.v.01 POS Tag: v Definition: use one&#39;s feet to advance; advance by steps Examples: [&quot;Walk, don&#39;t run!&quot;, &#39;We walked instead of driving&#39;, &#39;She walks with a slight limp&#39;, &#39;The patient cannot walk yet&#39;, &#39;Walk over to the cabinet&#39;] Syset ID: walk.v.02 POS Tag: v Definition: accompany or escort Examples: [&quot;I&#39;ll walk you to your car&quot;] Syset ID: walk.v.03 POS Tag: v Definition: obtain a base on balls Examples: [] Syset ID: walk.v.04 POS Tag: v Definition: traverse or cover by walking Examples: [&#39;Walk the tightrope&#39;, &#39;Paul walked the streets of Damascus&#39;, &#39;She walks 3 miles every day&#39;] Syset ID: walk.v.05 POS Tag: v Definition: give a base on balls to Examples: [] Syset ID: walk.v.06 POS Tag: v Definition: live or behave in a specified manner Examples: [&#39;walk in sadness&#39;] Syset ID: walk.v.07 POS Tag: v Definition: be or act in association with Examples: [&#39;We must walk with our dispossessed brothers and sisters&#39;, &#39;Walk with God&#39;] Syset ID: walk.v.08 POS Tag: v Definition: walk at a pace Examples: [&#39;The horses walked across the meadow&#39;] Syset ID: walk.v.09 POS Tag: v Definition: make walk Examples: [&#39;He walks the horse up the mountain&#39;, &#39;Walk the dog twice a day&#39;] Syset ID: walk.v.10 POS Tag: v Definition: take a walk; go for a walk; walk for pleasure Examples: [&#39;The lovers held hands while walking&#39;, &#39;We like to walk every Sunday&#39;] 16.4 Discovering Word Collocations from nltk.corpus import brown from nltk.collocations import BigramCollocationFinder from nltk.metrics import BigramAssocMeasures words = [w.lower() for w in brown.words()] bcf = BigramCollocationFinder.from_words(words) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4) [(&#39;;&#39;, &#39;;&#39;), (&#39;?&#39;, &#39;?&#39;), (&#39;of&#39;, &#39;the&#39;), (&#39;.&#39;, &#39;``&#39;)] # deal with stopwords from nltk.corpus import stopwords stopset = set(stopwords.words(&#39;english&#39;)) ## Fitler critera: ## remove words whose length &lt; 3 or which are on the stop word list filter_stops = lambda w: len(w) &lt; 3 or w in stopset bcf.apply_word_filter(filter_stops) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10) [(&#39;united&#39;, &#39;states&#39;), (&#39;new&#39;, &#39;york&#39;), (&#39;per&#39;, &#39;cent&#39;), (&#39;years&#39;, &#39;ago&#39;), (&#39;rhode&#39;, &#39;island&#39;), (&#39;los&#39;, &#39;angeles&#39;), (&#39;peace&#39;, &#39;corps&#39;), (&#39;san&#39;, &#39;francisco&#39;), (&#39;high&#39;, &#39;school&#39;), (&#39;fiscal&#39;, &#39;year&#39;)] ## apply freq-based filter bcf.apply_freq_filter(3) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10) [(&#39;united&#39;, &#39;states&#39;), (&#39;new&#39;, &#39;york&#39;), (&#39;per&#39;, &#39;cent&#39;), (&#39;years&#39;, &#39;ago&#39;), (&#39;rhode&#39;, &#39;island&#39;), (&#39;los&#39;, &#39;angeles&#39;), (&#39;peace&#39;, &#39;corps&#39;), (&#39;san&#39;, &#39;francisco&#39;), (&#39;high&#39;, &#39;school&#39;), (&#39;fiscal&#39;, &#39;year&#39;)] Exercise 16.1 Try to identify bigram collocations in another corpus. Exercise 16.2 Following the same strategy, please try to extract trigrams from the brown corpus. Remove stop words and short words as we did in the lecture notes. Include only trigrams whose frequency &gt; 5. "]]
