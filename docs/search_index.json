[["index.html", "ENC2055: Introduction to Programming Languages for Linguistic Analysis Preface Course Objective Textbook Course Website Course Demo Data Questions? Necessary Packages", " ENC2055: Introduction to Programming Languages for Linguistic Analysis Alvin Chen (陳正賢） 2021-06-20 Preface Welcome to ENC2055 Introduction to Programming Languages for Linguistic Analysis. This is a graduate-level course tailored to those who are interested in computational text analytics and data science in general. Have you decided to embark upon a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offering necessary skills and knowledge in important disciplines. This introductory course in basic computational coding would be a prerequisite course for many other advance-level courses. In particular, our faculty in the Linguistics track is dedicated to research on both theoretical and applied linguistics. Along with courses offered by other faculty members, this course provides a necessary foundation for the burgeoning discipline of computational linguistics. Please note that while this course has no prerequisite from the students, it will turn out to be a prerequisite for a lot of advanced courses, such as Corpus Linguistics and Computational Linguistics. If you plan to step into this computational way of language processing, you need to take this course (or at least you need to learn how to code.) Course Objective The objective of this course is to provide a comprehensive introduction to programming languages with a special focus on its application in linguistic analyses. This course is especially tailored to those who do not have any background or experiences in coding. We will start from the very basic concepts, such as data types, variable assignments, control structures, to more complex procedures such as routines, functions, and other exploratory project-based tasks. The course consists of a series of theme-based hands-on tutorials, which demonstrate how the flexibility of the programming language can help you become a more efficient and productive data scientist. Specifically, this course will use the language R as our featuring programming language and introduce you to , Rstudio, and a collection of R packages designed to work together to make linguistic analyses fast, fluent, and fun. In addition, we will briefly touch upon a few important constructs with another popular language in data science, Python . By the end of the course, students should have a working knowledge of coding and an initial ability to advance a project independently as a data scientist. In this course, we will not be dealing with complex maths like: \\[ f(x)=\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} x^{2}} \\] \\[ P(A) = \\sum P(\\{ (e_1,\\dotsc,e_N) \\}) = \\binom{N}{k} \\cdot p^kq^{N-k} \\] We will not be dealing with lingusitic theories as well. No transformations. No movements. No bindings. This course is all about computational coding. library(tidyverse) summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean = mean(temp), std_dev = sd(temp)) In the lecture notes, the text boxes in light blue refer to codes that you need to run in either terminal or your R/Python console. The text boxes in black background show the outputs of the code processing. We will follow this presentation convention throughout the entire lecture notes. print(&quot;Hello! R!&quot;) [1] &quot;Hello! R!&quot; Textbook Throughout the semester, we will follow the materials provided on our course website (see below). We will not use a particular textbook for the course. However, I do like to recommend Wickham and Grolemund (2017) for its simplicity. Also, another great book for R lovers, by Davies (2016): And two more comprehensive books for Python basics: Gerrard (2016) and Sweigart (2020): Course Website We have a course website. You may need a password to get access to the course materials. If you are an officially enrolled student, please ask the instructor for the access code. Course Demo Data Dropbox Demo Data Directory Questions? For more information related to this course, please see the FAQ on our course website or write me at any time at: alvinchen@ntnu.edu.tw Necessary Packages In this course, we will need the following R packages for tutorials and exercises. library(devtools) library(dplyr) library(foreign) library(gganimate) library(ggplot2) library(ggrepel) library(Hmisc) library(lubridate) library(maps) library(purrr) library(quanteda) library(readr) library(readtext) library(scales) library(showtext) library(stringr) library(tibble) library(tidyr) library(tidytext) library(tidyverse) References "],["intro-ds.html", "Chapter 1 Data Science and R 1.1 What is Data Science? 1.2 Working Pipeline for Data Science 1.3 Why R? 1.4 tidyverse 1.5 More Skills", " Chapter 1 Data Science and R 1.1 What is Data Science? Data Science is an interdisciplinary subject, which integrates knowledge of statistics, computer science and other domina-specific areas. A graph by Drew Conway may summarize the essense of Data Science: 1.2 Working Pipeline for Data Science Hadley Wickham’s R for Data Science describes six important steps for data analysis: 1.3 Why R? According to a report by KDnuggets, among all the languages used by data scientists, python and R are the most popular two languages: It is true that Python now seems much more popular among developers. That being said, you may consult this article, &lt;&lt; Why R is the Best Data Science Language to Learn Today? &gt;&gt;, for a more comprehensive review of the strengths of R. The general tendency is that: if you want to go into the industry and take developers or programmers as your future career, you can choose python; if you are planning to settle yourself in the academia, I would definitely recommend R. Here are a list of strengths for R language: powerful statistical analysis data visualization exploratory analysis re-usable reports tidyverse consistent grammar/syntax high readability of the codes, similar to human languages ( %&gt;% is a unique R feature!) In this course, our main objective is to introduce you to the world of coding. A high-level programming language like R would be a very friendly start, especially for those who have no background of computing. So, let us enjoy the journey of a simple yet powerful language learning! 1.4 tidyverse In this course, we will be working on a collection of packages included in tidyverse. This is a unique package in R, which can help you deal with data in a massively convenient way. It is hoped that the user can easily call particular functions and make use of the pipe operator %&gt;% to concatenate all your procedures serially, just like our natural languages. In particular, we will work on the following major libraries included in tidyverse: ggplot2: Data visualization dplyr: Data wrangling tidyr: Data wrangling stringr: String manipulation readr: Data importing purrr: Functional programming to avoid loops tibble: Powerful data structure 1.5 More Skills Data scientists are now becoming more and more popular. To know more about this job, one thing you may want to know is what kinds of skills are needed? The following two graphs were taken from &lt;The Most in Demand Skills for Data Scientists&gt;: While people still have various definitions regarding what data science encompasses, there are indeed several practical fields that have been commonly regarded as part of the definitions of Data Science. According to Mason and Wiggins (2010) A Taxonomy of Data Science, data science can be defined according to five crucial steps: Obtain: pointing and clicking does not scale. Scrub: the world is a messy place Explore: You can see a lot by looking Models: always bad, sometimes ugly iNterpret: “The purpose of computing is insight, not numbers.” This OSEMN (awesome!!) model should give you a much clearer picture of what you need to become a proficient data scientist. What we do here in this course is just a start….Take a deep breath:) "],["r-fundamentals.html", "Chapter 2 R Fundamentals 2.1 Installing R 2.2 Installing RStudio 2.3 The Interface of Rstudio 2.4 Assignment 2.5 Data Structure 2.6 Function 2.7 Vectorization 2.8 Script 2.9 Library 2.10 Setting 2.11 Seeking Help 2.12 Language Learning Ain’t Easy! 2.13 Keyboard Shortcuts", " Chapter 2 R Fundamentals Download R: R-Project IDE: RStudio 2.1 Installing R Download the installation file: http://cran.r-project.org 2.2 Installing RStudio After you install R, you may install RStudio. RStudio is an editor which can help you write R codes. A good analogy is that R is the engine and Rstudio is the dashboard of the car. Please download the right version that is compatible with your PC operating system. https://www.rstudio.com/download Choose RStudio Desktop Important notes: Do not have Chinese characters in your directory names or on the path to the files Do not have spaces and weird symbols in your file path: D:/R D:/Rstudio /User/Alvinchen/ 2.3 The Interface of Rstudio When you start Rstudio, you will see an interface as follows: Figure 2.1: Rstudio Interface Rstudio Interface: Editor: You creat and edit R-related files here (e.g., *.r, *.Rmd etc.) Console: This is the R engine, which runs the codes we send out either from the R-script file or directly from the console input Output: You can view graphic outputs here The R console is like a calculator. You can type any R code in the console after the prompt &gt; and run the code line by line by pressing enter. 1 + 1 [1] 2 log(10) [1] 2.302585 1:5 [1] 1 2 3 4 5 Or alternatively, we can create an R script in Rstudio and write down lines of R codes to be passed to the R console. This way, we can run the whole script all at once. This is the idea of writing a program. In the above example (Figure 2.1), I wrote a few lines of codes in a R script file (cf. the Editor frame) and asked R to run these lines of codes in the R Console. And the graphic output of the R script was printed in the Output frame. Exercise 2.1 Please create a new R script in Rstudio. You may name the script as “ch2.R.” Please write the following codes in the script and pass the whole script to the R Console. scores &lt;- rnorm(1000, mean = 75, sd = 5.8) plot(density(scores)) hist(scores) boxplot(scores) Exercise 2.2 Find the answer to the following mathematical calculation in R. \\(2^{2+1}-4+64^{(-2)^{2.25-\\frac{1}{4}}}\\) = 16777220 2.4 Assignment R works with objects of many different classes, some of which are defined in the base R while others are defined by specific libaries/environments/users. You can assign any object created in R to a variable name using &lt;-: x &lt;- 5 y &lt;- &quot;wonderful&quot; Now the objects are stored in the variables. You can print out the variables by either making use of the auto-printing (i.e., the variable name itself auto-prints its content) or print(): x [1] 5 print(x) [1] 5 y [1] &quot;wonderful&quot; print(y) [1] &quot;wonderful&quot; 2.5 Data Structure In R, the most primitive object is a vector. There are two types of primitive vectors: (a) numeric and (b) character vectors. In our previous examples, x is a numeric vector of one element; y is a character vector of one element. All elements in the vector have to be of the same data type. You use c() to create a vector of multiple elments. Within the parenthesis, you concatenate each element of the vector by ,: x2 &lt;- c(1, 2, 3, 4, 5, 6) x2 [1] 1 2 3 4 5 6 y2 &lt;- c(&quot;wonderful&quot;, &quot;excellent&quot;, &quot;brilliant&quot;) y2 [1] &quot;wonderful&quot; &quot;excellent&quot; &quot;brilliant&quot; Other data structures that we often work with include: List: a vector-like structure, but can consist of elements of different data types Matrix: a two-dimensional vector, where all elements have to be of the same data type Data Frame: a spreadsheet-like table, where columns can be of different data types ex_list &lt;- list(&quot;First element&quot;, 5:10, TRUE) print(ex_list) [[1]] [1] &quot;First element&quot; [[2]] [1] 5 6 7 8 9 10 [[3]] [1] TRUE ex_array &lt;- matrix(c(1, 5, 6, 3, 8, 19), byrow = T, nrow = 2) ex_array [,1] [,2] [,3] [1,] 1 5 6 [2,] 3 8 19 ex_df &lt;- data.frame(WORD = c(&quot;the&quot;, &quot;boy&quot;, &quot;you&quot;, &quot;him&quot;), POS = c(&quot;ART&quot;, &quot;N&quot;, &quot;PRO&quot;, &quot;PRO&quot;), FREQ = c(1104, 35, 104, 34)) ex_df The following graph shows you an intuitive understanding of the data structures in R. We will discuss more on data structures in Chapter 4. 2.6 Function Function is also an object class. There are many functions pre-defined in the R-base libraries. class(c) [1] &quot;function&quot; class(vector) [1] &quot;function&quot; class(print) [1] &quot;function&quot; To instruct R to do things more precisely, a function call usually has many parameters to specify. Take the earlier function matrix() for example. It is a pre-defined function in the R base library. ex_array &lt;- matrix(c(1, 5, 6, 3, 8, 19), byrow = T, nrow = 2) ex_array [,1] [,2] [,3] [1,] 1 5 6 [2,] 3 8 19 When creating a matrix, we specify the values for the parameters, byrow = and nrow =. These specifications tell R to create a matrix with N rows and arrange the numbers by rows. The actual values of the parameters that we use, i.e., T and 2, are referred to as arguments. Parameter is a variable in the declaration of function. Argument is the actual value of this variable that gets passed to function. Most importantly, we can define our own function, which is tailored to perform specific tasks. All self-created functions need to be defined first in the R environment before you can call them. Define own functions: print_out_user_name &lt;- function(name = &quot;&quot;) { cat(&quot;The current username is: &quot;, name, &quot;\\n&quot;) } Call own functions: print_out_user_name(name = &quot;Alvin Cheng-Hsien Chen&quot;) The current username is: Alvin Cheng-Hsien Chen print_out_user_name(name = &quot;Ada Lovelace&quot;) The current username is: Ada Lovelace Exercise 2.3 Please define a function called make_students_happy(), which takes a multi-element numeric vector, and returns also a numeric vector, with the value of each element to be the square root of the original value multiplied by 10. student_current_scores &lt;- c(20, 34, 60, 87, 100) make_students_happy(old_scores = student_current_scores) [1] 44.72136 58.30952 77.45967 93.27379 100.00000 2.7 Vectorization Many operations in R are vectorized, meaning that operations occur in parallel in certain R objects. This allows you to write code that is efficient, concise, and easier to read than in non-vectorized languages. The simplest example of vectorized functions is when adding two vectors together. x &lt;- 1:4 y &lt;- 6:9 z &lt;- x + y z [1] 7 9 11 13 Without vectorization, you mean need to do the vector adding as follows: z &lt;- numeric(length = length(x)) for (i in 1:length(x)) { z[i] &lt;- x[i] + y[i] } # endfor z [1] 7 9 11 13 Other common vectorized functions include: x &gt;= 5 [1] FALSE FALSE FALSE FALSE x &lt; 2 [1] TRUE FALSE FALSE FALSE y == 8 [1] FALSE FALSE TRUE FALSE For more information on vectorization, please watch the following youtube clip from Roger Peng. 2.8 Script In our earlier demonstrations, we ran R codes by entering each procedure line by line. We can create one script file with the Editor of Rstudio and include all our R codes in the file, which usually has the file extension of .R. And then we can run the whole script in the R script all at once in the Rstudio. First you open the *.R script file in Rstudio, which should appear in the Editor frame of the Rstudio. To run the whole script from start to the end, select all lines in the script file and press ctrl/cmd + shift + enter. To run a particular line of the script, put your mouse in the line and press ctrl/cmd + enter. 2.9 Library R, like other programming languages, comes with a huge database of packages and extensions, allowing us to do many different tasks without worrying about writing the codes all from the scratch. In CRAN Task Views, you can find specific packages that you need for particular topics/tasks. To install a package (i.e., library): install.packages(&quot;tidyverse&quot;) install.packages(c(&quot;ggplot2&quot;, &quot;devtools&quot;, &quot;dplyr&quot;)) In this course, I would like to recommend all of you to install the package tidyverse, which is a bundle including several useful packages for data analysis. During the installation, if you are asked about whether to install the package from source, please enter yes (See below for more detail). During the R package installation, if you see messages like installation of package XXX had non-zero exit status, this indicates that the package has NOT been properly installed in your R environment. That is, something is WRONG (See below as well). You need to figure out a way to solve the issues indicated in the error messages so that you can successfully install the package in your R system. Before you install R packages from source, you need to install a few R tools for your operating system. These tools are necessary for you to build the R packages from the source. For MacOS Catalina users, please install the following applications on your own. They are necessary for building R packages from source. Command Line Tools for Xcode 11.X You may need to log in with your Apple ID and find the download page. clang7 from https://cran.r-project.org/bin/macosx/tools/ gfortran6.1 from https://cran.r-project.org/bin/macosx/tools/ For Windows users, please install Rtools from CRAN (Please install the version according to your R version). After you install all the above source-building tools, you can now install the package tidyverse from source. Please install the package from the source. This step is very important because some of the dependent packages require you to do so. However, for the other packages, I would still recommend you to install the pakcages in a normal way, i.e., installing NOT from source, but from the compiled version on CRAN. 2.10 Setting Always set your default encoding of the files to UTF-8: 2.11 Seeking Help In the process of (learning) programming, one thing you will never be able to dodge is a strong desire for help. Here are the sources from which you may get additional assistance. Within Rstudio, in the R console, you can always use ? to check the documentation of a particular function (cf. Figure 2.2). When you run the command, you will see the documentation popping up in the output frame of the Rstudio. `?`(log) `?`(read.table) Figure 2.2: Help 1 If you need help from others, the first step is to create a reproducible example. The goal of a reproducible example is to package your problematic code in such a way that other people can run it and feel your pain. Then, hopefully, they can provide a solution and put you out of your misery. Figure 2.3: Help 2 So before you seek help from others (or before you yell at others for help, cf. Figure 2.3) : First, you need to make your code reproducible. This means that you need to capture everything, i.e., include any library() calls and create all necessary objects (e.g., files). The easiest way is to check the objects listed in the Environment tab of the Rstudio and identify objects that are relevant to your problematic code chunk. Second, you need to make it minimal. Strip away everything that is not directly related to your problem. This usually involves creating a much smaller and simpler R object than the one you’re facing in real life or even using built-in data. That sounds like a lot of work! And it can be, but it has a great payoff: 80% of the time creating an excellent reproducible example reveals the source of your problem. It’s amazing how often the process of writing up a self-contained and minimal example allows you to answer your own question. The other 20% of time you will have captured the essence of your problem in a way that is easy for others to play with. This substantially improves your chances of getting help! The following is a list of resources where people usually get external assistance quickly: http://www.r-project.org/mail.html http://stackoverflow.com/ Quick R: http://www.statmethods.net/ R CRAN Task Views: https://cran.r-project.org/web/views/ R for Data Science Text Mining with R R communities: R-Bloggers: https://www.r-bloggers.com/ kaggle: https://www.kaggle.com/ stackoverflow: https://stackoverflow.com/questions/tagged/r rstudio: https://community.rstudio.com/ 2.12 Language Learning Ain’t Easy! Learning R is like learning another foreign language. It should be a long journey. You can’t expect yourself to learn all the vocabuary of the new language in one day. Also, you will forget things you learn all the time. Everyone’s been there. When your script does not work as expected, don’t be frustrated. Take a break and resume later. What I can say is that: it is always NORMAL to debug a script for hours or even days via endless searches on Google. That being said, here I would like to share with you some of the most common problems we may run into: You created an R script file (*.r) and opened it in the Rstudio, but the script didn’t work simply because you didn’t execute the script in R console (i.e., you didn’t send the script to R console.) If you get an error message, saying \"object not found\", check the object name again and see if you have mistyped the name of the object. If not, check your current environment and see if you have forgot to execute some assignment commands in the previous parts of the script (i.e., the object has NOT even been created yet). If you get an error message, saying \"function not found\", check the function and see if you have the correct name. Or more often, check if you have properly loaded the necessary libraries where the function is defined. To understand the meaning of the error messages is crucial to your development of R proficiency. To achieve this, you have to know very well every object name you have created in your script (as well as in your environment). For example: What type of object is it? (i.e., the class of the object, e.g., vector, list, data.frame?) For primitive vectors, what data type does the vector belong to? (e.g., numeric, character, boolean,factor?) What is the dimensionality of the object? (nrows, ncols?) Sometimes the script fails simply because of the obvious syntactic errors. Pay attention to all the punctuations in every R command. They are far more important (or lethal) than you think. They include: ,: commas between arguments inside a function \": quotes for strings/characters (): parentheses for functions {}: curly brackets for control structures From my experiences, about 80 percent of the errors may in the end boil down to a simple typo. No kidding. Copy-and-paste helps. DO NOT assume that your R script always works as intended! Always keep two questions in mind: Did R produce the intended result? What is included in the R object name? 2.13 Keyboard Shortcuts The best way to talk to a computer is via the keyboard. Scripting requires a lot of typing. Keyboard shortcuts may save you a lot of time. Here are some of the handy shortcuts: Crtl/Command + Enter: run the current line (send from the script to the console) Crtl/Command + A: select all Crtl/Command + C: copy Ctrl/Command + X: cut Ctrl/Command + V: paste Ctrl/Command + Z: undo (Mac) Alt/Option + Left/Right: move cursor by a word (Windows) Ctrl + Left/Right: move cursor by a word (Mac) Command + Left/Right: move cursor to the beginning/end of line (Windows) Home/End: move cursor to the beginning/end of line (Mac) Command + Tab: switch in-between different working windows/apps Ctrl/Command + S: save file Command + Shift + C: comment/uncomment selected lines Exercise 2.4 Make yourself familiar with the iris data set, which is included in R. Exercise 2.5 Use ? to make youself familiar with the following commands: str,summary, dim, colnames, names, nrow, ncol, head, and tail. What information can you get with all these commands? Exercise 2.6 Write a function to compute the factorial of a non-negative integer, x, expressed as x!. The factorial of x refers to x multiplied by the product of all integers less than x, down to 1. For example, 3! = 3 x 2 x 1 = 6. The special case, zero factorial is always defined as 1. Confirm that your function produce the same results as below: (i) 5! = 120; (ii) 120! = 479,001,600; (iii) 0! = 1. # A Sample Format for your Function myfac &lt;- function(x){ } ## (i) myfac(5) [1] 120 ## (ii) myfac(12) [1] 479001600 ## (iii) myfac(0) [1] 1 "],["code-format-convetion.html", "Chapter 3 Code Format Convention 3.1 Assignment &lt;- 3.2 Comment # 3.3 Script Naming 3.4 Object Naming 3.5 Whitespace 3.6 Indention and Linebreaks 3.7 More References 3.8 Template for Script Assignments", " Chapter 3 Code Format Convention Like the first time we learn English writing, we need to know the conventional writing styles and formats in coding as well. This is very important because scripts of good formats would increase their readability. This would save us a lot of time in case of future debugging and maintenace. This chapter will discuss common practices among most R users. 3.1 Assignment &lt;- In R, people normally use &lt;- to assign values to object names. In other languages such as Python, people often use =. Although R still understands the value-assignment when you use =, I would still suggest to use &lt;- just to avoid the chance of confusing your R. x1 &lt;- &quot;This is a sentence.&quot; x2 = &quot;This is a sentence.&quot; x1 [1] &quot;This is a sentence.&quot; x2 [1] &quot;This is a sentence.&quot; 3.2 Comment # When you write codes, you would need to commit your code extensively. This is very important because we often forget why and how we write it this way. In your R script, any strings after the # will be treated as comments, which will NOT be processed by R. We can often add additional - and = after the # to separate different code chunks. # ==================== Variable Assignment ==================== x &lt;- &quot;This is a sentence&quot; # ==================== Variable Printing ==================== x [1] &quot;This is a sentence&quot; 3.3 Script Naming When you name your R script files, don’t be TOO creative. Use meaningul strings. Most importantly, use alphanumeric characters ONLY. Never use Chinese characters. For multiword names, it is suggested to connect words with -. # Recommended my-first-script.R my-first-assignment.R # NOT Recommended my first script.R 語料庫assingment1.R 3.4 Object Naming In your script, you will create lots of objects. Spend some time thinking about how to name all these objects. Choose names that are intuitive and meaningful. It is often the case that you want to keep the names simple (as typing is really annoying) but easy to understand as well. There are some principles: Use nouns for the object names (e.g., PTT_corpus) Use verbs for the function names (e.g., generate_ngrams()) Connect multiword names with _ (e.g., PTT_corpus_segmented) Avoid using characters/strings that have been used by R (e.g., vector, c, mean, sum, T etc.) 3.5 Whitespace For operators (i.e., =, +, -, &lt;-), they are usually embraced by whitespaces, which would make your script easier to read: # Recommended grade_average &lt;- mean(midterm * 0.5 + final * 0.5) # NOT Recommended grade_average &lt;- mean(midterm * 0.5 + final * 0.5) For : and ::, usually we do not put whitespaces around them: # Recommended x &lt;- c(1:10) tidyr::separate() # NOT Recommended x &lt;- c(1:10) tidyr::separate() : is an expression in R to create a sequence of numbers. For example, c(1:10) is the same as c(1,2,3,4,5,6,7,8,9,10). :: is an expression to access a particular object/function from a library without having the entire library loaded in your current R environment. For example, tidyr::separate() calls the function separate() from the library tidyr but the other objects in tidyr are still NOT included. You cannot use the other objects defined in tidyr. For parentheses (, if it is in the control structure, we usually put a whitespace before the initial (: # for-loop for (i in 1:10) { print(i) } [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 [1] 7 [1] 8 [1] 9 [1] 10 # if-conditional x &lt;- 2 if (x == 1) { print(&quot;The answer is 1!&quot;) } else { print(&quot;The answer is greater than 1!&quot;) } [1] &quot;The answer is greater than 1!&quot; But if the parenthesis is in the function call (i.e., where we specify the arguments of the parameters), we don’t put a whitespace before the initial (: mean(x) ggplot(aes(x = money, y = achievement)) For curely brackets, we usually put a linebreak after the initial { and the ending } should be one single line. Also, as sometimes you would embed many different control structures at the same time, leading to many ending } lines, it is always good to commit properly which ending } goes with which control structure. for (i in 1:10) { if (i &lt; 5) { print(i) } else { print(i + 10) } #endif } #endfor [1] 1 [1] 2 [1] 3 [1] 4 [1] 15 [1] 16 [1] 17 [1] 18 [1] 19 [1] 20 3.6 Indention and Linebreaks R does not care about line breaks, whitespaces, or tabs in your R script. But these formating characters are important because you need all these characters to help you quickly keep track of the script’s structure. Make good use of the indention to increase the readability of your script. long_function_name &lt;- function(a = &quot;a long argument&quot;, b = &quot;another argument&quot;, c = &quot;another long argument&quot;) { # As usual code is indented by two spaces. } y &lt;- matrix(data = c(2,5,7,8), # datasource nrow = 2, # two rows byrow = TRUE) # filling values by row y [,1] [,2] [1,] 2 5 [2,] 7 8 3.7 More References Readability of your code is an art. Please consult the following recommended readings if you are interested in more principles of clean code. 3.8 Template for Script Assignments When you submit your R scripts, please follow the format specified below. Important notes include: Please include the practice codes discussed in each chapter. Please specify the start and end of each of your exercise solution. Please indicate very clearly the chapter number and title as well as the exercise number in your script. Please name your R script as follows: ch2-alvin.R, ch3-alvin.R Please provide your descriptions/explanations in comments # (when you are asked to get familiar with packages, functions, or datasets). "],["subsetting.html", "Chapter 4 Subsetting 4.1 Vector 4.2 Factor 4.3 List 4.4 Data Frame 4.5 Tibble", " Chapter 4 Subsetting Subsetting is very important. To subset is to select a particular subset of elements from a data structure (e.g., vecotr, matrix, data.frame, list). In Chapter 2, we discuss very briefly about data structures. Here we will look at each type of data structure in more detail and introduce ways of subsetting them. 4.1 Vector As we have shown in Chapter 2 R Fundamentals, there are three types of primitive vectors in R: character vectors numeric vectors boolean vectors You can assess a particular subset of a vector by using[ ] right after the object name. Within the [], you can make use of at least three types of indexes: Subsetting with numeric index char.vec &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;) char.vec[1] [1] &quot;one&quot; You can also assess several elements of a vector by putting in several indices, c(), in the []: char.vec[c(1, 4)] [1] &quot;one&quot; &quot;four&quot; Subsetting with boolean index You can also use a boolean vector as the index: whether.to.extract &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE) char.vec[whether.to.extract] [1] &quot;one&quot; &quot;three&quot; You may use different logical operators to check each element of the vector according to some criteria. R will decide whether elements of the vector satisfy the condition given in the logical expression and return a boolean vector of the same length. Common logical operators include: ==: equal to &amp;: and |: or &gt;: greater than &gt;=: greater than or equal to &lt;: less than &lt;=: less than or equal to !=: not equal This can be very useful for vector subsetting: num.vec &lt;- c(1:20) num.vec &gt; 10 [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [13] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE num.vec[num.vec &gt; 10] [1] 11 12 13 14 15 16 17 18 19 20 num.vec[num.vec != 10] [1] 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 19 20 num.vec[num.vec &gt; 18 | num.vec &lt; 2] [1] 1 19 20 Subsetting with negative numeric index If you use negative numbers in the index [], you will get a new vector printed on the console, with those indexed elements removed: char.vec[-2] [1] &quot;one&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; However, please note that the original vector is still the same in length: char.vec [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; If you want to save the shortened vector, you can either (a) assign the shortened vector to a new object name or (b) assign the shortened vector to the same object name: char.vec.short &lt;- char.vec[-2] char.vec.short [1] &quot;one&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; char.vec [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; char.vec &lt;- char.vec[-2] char.vec [1] &quot;one&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; For the two alternatives, which one would be better? Why? Exercise 4.1 Create a vector m with the small letters from a to j (in alphabetical order). hint: check the object letters in R. m [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; Exercise 4.2 Create a vector n with the samll letters from a to j (in random order). hint: check sample() n [1] &quot;e&quot; &quot;h&quot; &quot;d&quot; &quot;i&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;g&quot; &quot;f&quot; &quot;j&quot; Exercise 4.3 Determine whether letters in n are at the same positions as in m. How many such cases do you find? Which letters are these? hint: check table() FALSE TRUE 9 1 [1] &quot;j&quot; 4.2 Factor In Chapter 2, we did not talk about this data structure–factor. There is an obvious reason for that. A factor works pretty much similarly to a vector in R. One of the key features for a factor is that its values are limited to a finite number of dinstinct categories (i.e., levels). In many statistical experimental designs, a factor is usually a grouping factor, i.e., a factor that groups the subjects into sub-groups. We usually create a factor from a numeric or character vector. To create a factor, use factor(): sbj_gender_num &lt;- c(1, 0, 0, 1, 1, 0, 1) sbj_gender_num [1] 1 0 0 1 1 0 1 sbj_gender_char &lt;- c(&quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;) sbj_gender_char [1] &quot;female&quot; &quot;male&quot; &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;male&quot; &quot;female&quot; sbj_gender_num_fac &lt;- factor(x = sbj_gender_num) sbj_gender_num_fac [1] 1 0 0 1 1 0 1 Levels: 0 1 sbj_gender_char_fac &lt;- factor(x = sbj_gender_char) sbj_gender_char_fac [1] female male male female female male female Levels: female male For a factor, the most important information is its levels, i.e., the limit set of all possible values this factor can take. We can extract the levels as a vector of character strings using levels(): levels(sbj_gender_num_fac) [1] &quot;0&quot; &quot;1&quot; levels(sbj_gender_char_fac) [1] &quot;female&quot; &quot;male&quot; When do we need a factor? Sometimes when we do the annotation of the data, we use arbitrary numbers as labels for certain categorical variables. For example, we may use arbitrary numbers from 1 to 4 to label learners of varying proficiency levels: 1 = beginners, 2 = low-intermediate, 3 = upper-intermediate, 4 = advanced. When we load the data into R, R may first treat the data as a numeric vector: sbj_prof_num &lt;- c(1, 2, 4, 4, 2, 3, 3, 1, 1) sbj_prof_num [1] 1 2 4 4 2 3 3 1 1 However, these numbers may be confusing to the extent that (a) R may even consider them really to be numbers, or (b) they are not intuitive at all as numbers do not have meanings. In this case, we can create a factor from this numeric vector and re-label these numeric values into categorical labels that are more intuitive. We can do this by setting more argments in factor(), such as levels=..., labels=.... sbj_prof_fac &lt;- factor(x = sbj_prof_num, levels = c(1:4), labels = c(&quot;beginner&quot;, &quot;low-inter&quot;, &quot;upper-inter&quot;, &quot;advanced&quot;)) sbj_prof_fac [1] beginner low-inter advanced advanced low-inter upper-inter [7] upper-inter beginner beginner Levels: beginner low-inter upper-inter advanced levels = ...: this argument specifies all possible values this factor can take labels = ...: this argument provides own intuitive labels for each level It should now therefore be clear that labels = ... is a good way for us to re-label any artitrary annotations into meaningful labels. What’s even more brilliant is that we can decide whether the ranking of the levels is meaningful. If the order of the levels of the factor is meaningful, we can set the argument ordered = TRUE: sbj_prof_fac_ordered &lt;- factor(x = sbj_prof_num, levels = c(1:4), labels = c(&quot;beginner&quot;, &quot;low-inter&quot;, &quot;upper-inter&quot;, &quot;advanced&quot;), ordered = T) sbj_prof_fac_ordered [1] beginner low-inter advanced advanced low-inter upper-inter [7] upper-inter beginner beginner Levels: beginner &lt; low-inter &lt; upper-inter &lt; advanced Now from the R console we can see not only the levels of the factor but also the signs &lt;, indicating their order. Using this ordered factor, we can perform relational comparison: sbj_prof_fac_ordered[1] [1] beginner Levels: beginner &lt; low-inter &lt; upper-inter &lt; advanced sbj_prof_fac_ordered[4] [1] advanced Levels: beginner &lt; low-inter &lt; upper-inter &lt; advanced sbj_prof_fac_ordered[1] &lt; sbj_prof_fac_ordered[4] [1] TRUE But we cannot do the comparison for unordered factors (characters neither): sbj_prof_fac[1] [1] beginner Levels: beginner low-inter upper-inter advanced sbj_prof_fac[4] [1] advanced Levels: beginner low-inter upper-inter advanced sbj_prof_fac[1] &lt; sbj_prof_fac[4] Warning in Ops.factor(sbj_prof_fac[1], sbj_prof_fac[4]): &#39;&lt;&#39; not meaningful for factors [1] NA The difference between vector and factor may look trivial for the moment but they are statistically very crucial. The choice of whether to instruct R to treat a vector as a factor, or even an ordered factor, will have important consequences in the implementation of many statistical methods, such as regression or othe generalized linear modeling. Rule of thumb: Always pay attention to what kind of object class you are dealing with:) 4.3 List A List is like a vector, which is a one-dimensional data structure. However, the main difference is that a List can include a series of objects of different classes: # A list consists of (i) numeric vector, (ii) character vector, (iii) boolean # vector list.example &lt;- list(one = c(1, 2, 3), two = c(&quot;Joe&quot;, &quot;Mary&quot;, &quot;John&quot;, &quot;Angela&quot;), three = c(TRUE, TRUE)) list.example $one [1] 1 2 3 $two [1] &quot;Joe&quot; &quot;Mary&quot; &quot;John&quot; &quot;Angela&quot; $three [1] TRUE TRUE Please note that not only the class of each object in the List does not have to be the same; the length of each list element may also vary. You can subset a List in two ways: [: This always returns a List back [[: This returns the object of the List element, which is NOT NECESSARILY a List list.example[1] $one [1] 1 2 3 list.example[[1]] [1] 1 2 3 list.example[[&quot;one&quot;]] [1] 1 2 3 Before you try the following codes on the R console, could you first predict the outputs? ind &lt;- c(&quot;one&quot;, &quot;three&quot;) list.example[ind] list.example[[ind]] Exercise 4.4 Create a list that contains, in this order: a sequence of 20 evenly spaced numbers between -4 and 4; (hint: check seq()) a 3 x 3 matrix of the logical vector c(F,T,T,T,F,T,T,F,F) filled column-wise; a character vector with the two strings “don,” and “quixote”; a factor containing the observations `c(“LOW,”“MID,”“LOW,”“MID,”“MID,”“HIGH.” [[1]] [1] -4.0000000 -3.5789474 -3.1578947 -2.7368421 -2.3157895 -1.8947368 [7] -1.4736842 -1.0526316 -0.6315789 -0.2105263 0.2105263 0.6315789 [13] 1.0526316 1.4736842 1.8947368 2.3157895 2.7368421 3.1578947 [19] 3.5789474 4.0000000 [[2]] [,1] [,2] [,3] [1,] FALSE TRUE TRUE [2,] TRUE FALSE FALSE [3,] TRUE TRUE FALSE [[3]] [1] &quot;don&quot; &quot;quixote&quot; [[4]] [1] LOW MID LOW MID MID HIGH Levels: HIGH LOW MID Exercise 4.5 Based on Exercise 4.4, extract row elements 2 and 1 of columns 2 and 3, in that order, of the logical matrix. [,1] [,2] [1,] FALSE FALSE [2,] TRUE TRUE Exercise 4.6 Based on Exercise 4.4, obtain all values from the sequence between -4 and 4 that are greater than 1. [1] 1.052632 1.473684 1.894737 2.315789 2.736842 3.157895 3.578947 4.000000 Exercise 4.7 Make yourself familar with the function which(). Based on Exercise 4.4, using which(), determine which indexes in the factor are assigned the “MID” level. [1] 2 4 5 4.4 Data Frame data.frame is the most frequently used object that we will work with in data analysis. It is a typical two-dimensional spreadsheet-like table. Normally, the rows are the subjects or tokens we are analyzing; the columns are the variables or factors we are interested in. We can also use [ , ] to subset a data frame. The indexes in [ , ] are Row-by-Column. ex_df &lt;- data.frame(WORD = c(&quot;the&quot;, &quot;boy&quot;, &quot;you&quot;, &quot;him&quot;), POS = c(&quot;ART&quot;, &quot;N&quot;, &quot;PRO&quot;, &quot;PRO&quot;), FREQ = c(1104, 35, 104, 34)) ex_df You can subset a particular row of the data frame: ex_df[1, ] ex_df[c(1, 3), ] You can subset a particular column of the data frame: ex_df[, 1] [1] &quot;the&quot; &quot;boy&quot; &quot;you&quot; &quot;him&quot; ex_df[, c(1, 3)] ex_df[, c(&quot;WORD&quot;, &quot;FREQ&quot;)] Please compare the following two ways of accessing a column from the data frame. Can you tell the differences in the returned results? ex_df[, c(&quot;FREQ&quot;)] ex_df[, c(&quot;FREQ&quot;), drop = FALSE] Exercise 4.8 Create and store the following data frame as dframe in your R workspace. person should be a character vector sex should be a factor with levels F and M funny should be a factor with levels Low, Mid, and High Exercise 4.9 Stan and Francine are 41 years old, Steve is 15, Hayley is 21, and Klaus is 60. Roger is extremely old–1,600 years. Following Exercise 4.8, append these data as a new numeric column variable in dframe called age. Exercise 4.10 Following Exercise 4.9, write a single line of code that will extract frome dframe just the names and ages of any records where the individual is male and has a level of funniness equal to Low OR Mid. 4.5 Tibble tibble is a new data structure with lots of advantages. For the moment, we treat tibble and data.frame as the same, with the former being an augmented version of the latter. All functions that work of a data.frame should be compatible with a tibble. Now the tibble is the major structure that R users work with under the tidy framework. If you are new to tibbles, the best place to start is the tibbles chapter in R for data science. require(tibble) Loading required package: tibble ex_tb &lt;- tibble(WORD = c(&quot;the&quot;, &quot;boy&quot;, &quot;you&quot;, &quot;him&quot;), POS = c(&quot;ART&quot;, &quot;N&quot;, &quot;PRO&quot;, &quot;PRO&quot;), FREQ = c(1104, 35, 104, 34)) ex_tb There is another way to create a tibble. You can use tribble(), short for transposed tibble. tribble() is customised for data entry in code: column headings are defined by formulas (i.e. they start with ~), and entries are separated by commas. This makes it possible to lay out small amounts of data in easy to read form. ex_tb_2 &lt;- tribble( ~WORD, ~POS, ~FREQ, #----|--------|------ &quot;the&quot;, &quot;ART&quot;, 1104, &quot;boy&quot;, &quot;N&quot;, 35, &quot;you&quot;, &quot;PRO&quot;, 104, &quot;him&quot;, &quot;PRO&quot;, 34 ) ex_tb_2 You can subset a tibble in exactly the same ways as you work with a data.frame: ex_tb[1, ] ex_tb[, 1] ex_tb[, c(1:3)] Exercise 4.11 Please compare again the following codes and see if you can tell the major differences between tibble and data.frame? ex_tb[, c(&quot;FREQ&quot;)] # indexing tibble ex_df[, c(&quot;FREQ&quot;)] # indexing data.frame [1] 1104 35 104 34 There are three major advantages with tibble() when compared with data.frame(): tibble set strings to default to character vectors while data.frame converts all character vectors to factors by default When auto-printing the contents, tibble would only display the first ten rows, but data.frame would print out everything. This could be devestating! (Imagine that you have a table with hundreds of thousands rows.) The auto-printing of the tibble is a lot more informative, providing additional attributes of the tibble such as (a) row and column numbers and (b) data type of each column "],["data-visualization.html", "Chapter 5 Data Visualization 5.1 Why Visualization? 5.2 ggplot2 5.3 Variables and Data Type 5.4 One-variable Graph 5.5 Two-variable Graph 5.6 Adding Other Aesthetic Features 5.7 Saving Plots 5.8 Exercises on iris 5.9 Exercises on COVID-19", " Chapter 5 Data Visualization 5.1 Why Visualization? Data visualization is very important. I would like to illustrate this point with two interesting examples. 5.1.1 Datasaurus Dozen Dataset First, let us take a look at an interesting dataset—Datasaurus, which is available in demo_data/data-datasaurus.csv (source: Datasaurus data package. This dataset was first created by Alberto Cairo. Table 5.1: An Interesting Dataset group x y dino 95.38460 36.794900 dino 98.20510 33.718000 away 91.63996 79.406603 away 82.11056 1.210552 h_lines 98.28812 30.603919 h_lines 95.24923 30.459454 v_lines 89.50485 48.423408 v_lines 89.50162 45.815179 x_shape 84.84824 95.424804 x_shape 85.44619 83.078294 star 82.54024 56.541052 star 86.43590 59.792762 high_lines 92.24840 32.377154 high_lines 96.08052 28.053601 dots 77.92604 50.318660 dots 77.95444 50.475579 circle 85.66476 45.542753 circle 85.62249 45.024166 bullseye 91.72601 52.623353 bullseye 91.73554 48.970211 slant_up 92.54879 42.901908 slant_up 95.26053 46.008830 slant_down 95.44349 36.189702 slant_down 95.59342 33.234129 wide_lines 77.06711 51.486918 wide_lines 77.91587 45.926843 With a dataset like this, we can break the dataset into several subsets by group and for each subset we compute their respective mean scores and standard deviations of x and y. According to the summary statistics of each sub-group (cf. Table 5.2), they look indeed similar: Table 5.2: An Interesting Dataset - Summary group x_fn1 y_fn1 x_fn2 y_fn2 away 54.266 47.835 16.770 26.940 bullseye 54.269 47.831 16.769 26.936 circle 54.267 47.838 16.760 26.930 dino 54.263 47.832 16.765 26.935 dots 54.260 47.840 16.768 26.930 h_lines 54.261 47.830 16.766 26.940 high_lines 54.269 47.835 16.767 26.940 slant_down 54.268 47.836 16.767 26.936 slant_up 54.266 47.831 16.769 26.939 star 54.267 47.840 16.769 26.930 v_lines 54.270 47.837 16.770 26.938 wide_lines 54.267 47.832 16.770 26.938 x_shape 54.260 47.840 16.770 26.930 So it may be tempting for us to naively conclude that all groups show similar behaviors in x and y measures. But what if we plot all subjects by groups? See? When we visualize our data, sometimes the patterns reveal themselves. What you see in numbers may be even deceiving. 5.1.2 Simpson’s Paradox Another example is Simpson’s Paradox, which occurs when trends that appear when a dataset is separated into groups reverse when the data are aggregated. Based on the above graph, you would probably conclude that when x increases, y decreases. However, if you plot the scatter plots by groups, you may get the opposite conclusions. All correlations between x and y in each sub-group are now positive. 5.2 ggplot2 R is famous for its power in data visualization. In this chapter, I will introduce you a very powerful graphic library in R, ggplot2. For any data visualization, there are three basic elements: Data: The raw material of your visualization, i.e., a data frame. Aesthetics: The mapping of your data to aesthetic attributes, such as x, y, color, linetype, fill. Geometric Objects: The layers of geometric objects you would like to see on the plots, e.g., lines, points etc. I will demonstrate some basic functions of ggplot2, with the pre-loaded dataset mpg: library(tidyverse) mpg For data visualization, the first step is to know your dataset, i.e., the meanings of rows and columns. In the dataset mpg, each row refers to a car and the columns include: model: manufacturer model name displ: engine displacement, in litres (排氣量) hwy: highway miles per gallon cty: city miles per gallon cyl: number of cylinders (汽缸數目) class: car type drv: the type of drive train, where f = front-wheel drive (前輪驅動), r = rear wheel drive (後輪驅動), 4 = 4wd (四輪傳動) To begin with, I like to use one simple example to show you how we can create a plot using ggplot2. With the dataset mpg, we can look at the relationship between displ and hwy: whether the engine displacement has to do with the city miles per gallon. We can draw a scatter plot as shown below. ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() A ggplot object often includes lat least three important components: ggplot() initializes the basic frame of the graph, with data = mpg specifying the data frame on which the plot is built aes() further specifies the mapping of axises and the factors in the data frame. aes(x = displ, y = hwy) indicates that displ is mapped as the x axis and hwy as y axis + means that you want to add one layer of the graph to the template. geom_point() means that you want to add a layer of point graph. 5.3 Variables and Data Type When creating the graphs for your data, you need to know very well the data type of all your variables to be included in the graph. There are at least three important data types you need to know: Categorical variables: these variables usually have only limited set of discrete values, i.e., levels. They are usually coded as character vector or factor in R. Numeric variables: these variables are continuous numeric values. They are usually coded as numeric vector. Date-Time variables: these variables, although being numeric sometimes, refer to calendar dates or times. They are usually coded as Date-TimeClasses in R. The general principle in data visualization is that always pay attention to the data type for variables on the x-axis and y-axis. 5.4 One-variable Graph If your graph includes only one variable from the data, usually this would indicate that you are interested in the distribution of the variable values. Continuous variable Density plot Histogram ggplot(data = mpg, aes(hwy)) + geom_density(kernel = &quot;gaussian&quot;) ggplot(data = mpg, aes(hwy)) + geom_histogram() Categorical variable Bar plot ggplot(data = mpg, aes(x = class)) + geom_bar() Exercise 5.1 How can we create a bar plot as above but with the bars arranged according to the counts in a descending order from left to right? (see below) Hint: check reorder() 5.5 Two-variable Graph If your graph includes two variables, then very likely one variable would go to the x-axis and the other, y-axis. Depending on their data types (categorical or numeric), you may need to create different types of graphs. Continuous X, Continuous Y Scatter Plot ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() We can add a regression line to the scatter plot: ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x) Categorical X, Continuous Y Boxplot ggplot(data = mpg, aes(x = class, y = hwy)) + geom_boxplot() Error Plot ggplot(data = mpg, aes(x = class, y = hwy)) + stat_summary(fun.data = mean_cl_boot, geom = &quot;pointrange&quot;) If you run into problems plotting the error plot using stat_summary(), probably you did not have the necessary packages installed in your current R environment. Please make sure that you install the package tidyverse or ggplot2 properly without any error messages in the process of installation. Also, please note that you need to install the tidyverse from source. (For the other relevant packages, it is ok to install those packages in a normal way from CRAN). For more detail, please refer back to Chapter 2.9. Categorical X, Categorical Y Bubble Plot ggplot(data = mpg, aes(x = manufacturer, y = class)) + geom_count() + theme(axis.text.x = element_text(angle = -90)) 5.6 Adding Other Aesthetic Features 5.6.1 color Now I would like to demonstrate how we can add additional aesthetic mappings to your graphs. Earlier we create a scatter plot using the following code: ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() The above plot includes two variables into the graph, x = displ and y = hwy. The additional aesthetic features include things like colors, sizes, shapes, line-types, widths etc. The idea is that we can introduce a third variable into the plot by modifying the color of the points based on the value of that third variable. For example, you can add color = ... in the aes(x = ..., y = ..., color = ...) to create the graphs on the basis of another grouping factor. ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() In the above example, color is an aesthetic and the color of each point is now mapped to the variable drv. In this case, points belong to each group of drv would be of different colors—different drive train types have different colors in points. Note that the x-coordinates and y-coordinates are aesthetics too, and they got mapped to the displ and hwy variables, respectively. We further map the color to the third variable drv, which indicates whether a car is front wheel drive, rear wheel drive, or 4-wheel drive. 5.6.2 shape We can map a third variable to the graph using shape as well. ggplot(data = mpg, aes(x = displ, y = hwy, shape = drv)) + geom_point() And of course you can map both shape and color to the same third variable: ggplot(data = mpg, aes(x = displ, y = hwy, color = drv, shape = drv)) + geom_point() 5.6.3 Other geom_... Layers The ggplot object consists of layers of geometric objects. We can also add another geom object, such as a smooth line by using the +: ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth() Could you predict what kind of graph you would get with the following code? ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth() 5.6.4 Labels We can add self-defined labels of the x and y axes and main titles to the graphs using labs(). ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x) + labs(title = &quot;Correlation between Displacement and Highway Miles per Gallon&quot;, x = &quot;Displacement&quot;, y = &quot;Miles/Per Gallon&quot;) ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + labs(x = &quot;Engine Displacement (litres)&quot;, y = &quot;Highway Miles per Gallon&quot;, title = &quot;Scatter Plot -- DISPL by HWY&quot;) 5.6.5 fill For bar plots or histograms, we can fill the bars with different colors by adding fill = ... in the aes(). ggplot(data = mpg, aes(x = class, fill = class)) + geom_bar() ggplot(data = mpg, aes(x = class, y = hwy, fill = class)) + geom_boxplot() 5.6.6 Interim Summary Here are a list of common aesthetic parameters we often use in aes(): size = ... color = ... fill = ... alpha = ... 5.7 Saving Plots Saving a ggplot can be easily done by ggsave(). You can first save a ggplot object to a variable and then use ggsave() to output the ggplot object to an external file. It is recommended to use common image formats for publications, e.g., png, jpg. Also, please remember to set the width and height (in inches) of your graph. These settings will greatly affect the look of the graph. my_first_graph &lt;- ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + labs(x = &quot;Engine Displacement (litres)&quot;, y = &quot;Highway Miles per Gallon&quot;, title = &quot;Scatter Plot -- DISPL by HWY&quot;) class(my_first_graph) # check the class [1] &quot;gg&quot; &quot;ggplot&quot; my_first_graph # auto-print the ggplot ggsave(filename = &quot;my_first_plot.png&quot;, plot = my_first_graph, width = 6, height = 6) 5.8 Exercises on iris The following exercises will use the preloaded dataset iris in R. iris Exercise 5.2 Please create a scatter plot showing the relationship between Sepal.Length and Petal.Length for different iris Species. Also, please add the regression lines for each species. Your graph should look as close to the sample as possible. Exercise 5.3 Please create a boxplot showing the Petal.Width distributions of each iris Species. Exercise 5.4 Please create boxplots showing the distributions of Petal.Width and Sepal.Width of different iris Species on the same graph. Hint: You may need to transform your data into a longer format, as shown below, before you create the boxplot. See tidyr::pivot_longer(). Please work on this exercise when we finish Chapter 6. 5.9 Exercises on COVID-19 Please work on the exercises included this section after we finish the Chapter 6. (In other words, I may ask you to submit your assignments on these two chapters together.) The following exercises are based on a dataset downloaded from Kaggle. The dataset is also available in demo_data/data-covid19.csv. Exercise 5.5 Load the dataset in demo_data/data-covid19.csv into R as a data frame named covid19. Hint: Check readr::read_csv() Please note that in this dataset, the column Confirmed includes the cumulative numbers of confirmed cases on different days. These cumulative numbers allow us to keep track of the development of COVID-19. It is the same for the other columns as well (i.e., Deaths and Recovered). Also, for countries like Mainland China, the numbers are reported by Province/State. To get the total number of confirmed cases on a particular day for the entire country, you may need to sum up all the numbers in individual provinces first. Exercise 5.6 Use ggplot2 to create a line plot showing the number of confirmed cases by days for the following countries: Taiwan, Japan, South Korea, Iran, Italy, Mainland China. A sample graph is shown below. Exercise 5.7 Create a bar plot showing the top 10 countries ranked according to their number of confirmed cases of the COVID19. Exercise 5.8 Create a bar plot showing the top 10 countries ranked according to their death rates of the COVID19. (Death rates are defined as the number of deaths divided by the number of confirmed cases.) Exercise 5.9 (optional) Create a world map showing the current outbreak of covid19. Hint: Please check ggplot2::geom_polygon() and the package library(maps). This exercise is made to see if you know how to find resources online for more complex tasks like this. Please note that the country names may not match. "],["data-manipulation.html", "Chapter 6 Data Manipulation 6.1 Dataset 6.2 rename() 6.3 Pipe %&gt;% 6.4 mutate() 6.5 select() 6.6 filter() 6.7 arrange() 6.8 group_by() and summarize() 6.9 count() 6.10 Tidy Data 6.11 Exerceises", " Chapter 6 Data Manipulation In this chapter, we will be working with two powerful packages, dplyr and tidyr, which provide a consistent “grammar” for data manipulation and exploration by simplifying operations on data frames to a great deal. We first load the library: library(dplyr) library(tidyr) In the library dplyr, there are a list of key verbs: %&gt;%: the “pipe” operator is used to connect multiple verb actions together into a pipeline mutate(): add new variables/columns or transform existing variables select(): return a subset of the columns of a data frame, using a flexible notation filter(): extract a subset of rows from a data frame based on logical conditions summarise(): generate summary statistics of different variables in the data frame, possibly within strata group_by(): group the data frame into sub-tables according to a grouping factor arrange(): reorder rows of a data frame (according to a particular variable) rename(): rename variables in a data frame Several usueful functions for joining two data frames: inner_join() left_join() right_join() full_join() anti_join() Exercise 6.1 Please check the documentations of all the above functions of merging data frames and make sure you understand how two data frames are merged for each function. In the second library tidyr, we will focus on: pivot_longer(): to tidy the data from wide to long pivot_wider(): to tidy the data from long to wide 6.1 Dataset The dateaset we use in this chapter is a student performance dataset from kaggle. library(readr) student &lt;- read_csv(&quot;demo_data/data-students-performance.csv&quot;) student Usually we would start from an overview of the dataset, using summary(): summary(student) gender race/ethnicity parental level of education Length:1000 Length:1000 Length:1000 Class :character Class :character Class :character Mode :character Mode :character Mode :character lunch test preparation course math score reading score Length:1000 Length:1000 Min. : 0.00 Min. : 17.00 Class :character Class :character 1st Qu.: 57.00 1st Qu.: 59.00 Mode :character Mode :character Median : 66.00 Median : 70.00 Mean : 66.09 Mean : 69.17 3rd Qu.: 77.00 3rd Qu.: 79.00 Max. :100.00 Max. :100.00 writing score Min. : 10.00 1st Qu.: 57.75 Median : 69.00 Mean : 68.05 3rd Qu.: 79.00 Max. :100.00 6.2 rename() The column names in student are a mess. These names include spaces in them, which would be difficult to index these columns in R. This is however normal in the real world, where the dataset we get is often very messy. So, the first thing we can do with the dataset is the rename all the column names in a R-compatible way. rename(student, race = `race/ethnicity`, parent_edu = `parental level of education`, prep_course = `test preparation course`, math = `math score`, reading = `reading score`, writing = `writing score`) -&gt; student1 student1 Please note that in our earlier code, we save the output of rename() to a new object named student1. In other words, the object student1 should contain a new data frame with all column names fixed as above. 6.3 Pipe %&gt;% Now let’s look at a fantastic syntax in R, the pipe %&gt;%, which is definitely one of my favorite R idioms! To start with, the following two expressions are the same, giving you the same results: sum(c(1:10)) [1] 55 c(1:10) %&gt;% sum [1] 55 The meaning of %&gt;% is that the object on the left of the pipe is passed on to the right side of the pipe for further processing. By default, the object is passed onto to be the first argument of the function on the right-hand side. This pipe-based syntax would render the script more reader-friendly. For example, it is difficult to conceptualize the following code with several layers of embedding structures. sqrt(sum(abs(c(-10:10)))) [1] 10.48809 But the above code can be re-written with the %&gt;% as follows: c(-10:10) %&gt;% # create a vector abs %&gt;% # take each element&#39;s absolute value sum %&gt;% # sum all elements sqrt # take the square root of the sum [1] 10.48809 Now we understand the idiomatic expression of %&gt;%, our earlier rename() can be re-written as follows as well (cf. student1 and student1a): student %&gt;% rename(race = `race/ethnicity`, parent_edu = `parental level of education`, prep_course = `test preparation course`, math = `math score`, reading = `reading score`, writing = `writing score`) -&gt; student1a student1a From now on, we will use the pipe-based syntax more often. 6.4 mutate() Now imagine that you would like to create a new variable called final_grade, which is a weighted average of the student’s academic performance. Let us assume that you have the following weights in mind: math (50%), reading (25%), writing (25%). You can use mutate() to create a new column (i.e., variable) in your data frame: student1 %&gt;% mutate(final_grade = math * 0.5 + reading * 0.25 + writing * 0.25) We can create more than one new variables as well: student1 %&gt;% mutate(language = reading * 0.5 + writing * 0.5, final_grade = math + language) In the above practices of mutate(), we did not save the output of mutate() to a new object name. We only print the output directly to the console. In order words, the original data frame is still the same (i.e., student, student1); no new variables have been created with respect to these original data frames. 6.5 select() select() is to select particular columns of the data frame that you would like to focus on. You can select just one column student1 %&gt;% select(math) Or multiple columns: student1 %&gt;% select(math, reading, writing) student1 %&gt;% select(reading, writing, math) Or columns within a range: student1 %&gt;% select(math:writing) You can also omit variables using select() student1 %&gt;% select(-c(race:lunch)) 6.6 filter() While select() is for columns, filter() is for rows. You can extract subsets of rows from a data frame. Most importantly, you can extract rows according to self-defined conditions. one logical condition student1 %&gt;% filter(math &gt; 90) AND &amp; conditions: student1 %&gt;% filter(math &lt; 40 &amp; reading &lt; 40) OR | conditions: student1 %&gt;% filter(math &lt; 40 | reading &lt; 40) XOR xor conditions: student1 %&gt;% filter(xor(math &lt; 40, reading &lt; 40)) Exercise 6.2 Please check the row numbers of the above three filtered data frames. Any connection? Please check Chapter 4.1 Vector for more logical operations. 6.7 arrange() We can arrange the rows of the data frame according to a particular variable. student1 %&gt;% arrange(math) By default, R will arrange the rows in an ascending order. If you like to arrange your data in a descending order, put a desc() around your variable name: student1 %&gt;% arrange(desc(math)) 6.8 group_by() and summarize() The group_by() function is used to generate summary statistics from the data frame within strata defined by a grouping variable. For example, in this student1 dataset, you might want to know what the average math scores are for students of different genders. In conjunction with the group_by() function we often use the summarize() function to create the summarized statistics for each subgroup (i.e., male and female). Two important steps: Split the big data frame into smaller sub data frames according to a grouping factor/variable (group_by()) Summarize each sub data frame with respect to specific parameters (summarize()) student1 %&gt;% group_by(gender) %&gt;% summarize(math_average = mean(math), math_median = median(math), math_sd = sd(math)) 6.9 count() One of the most-often used feature when we have data frames is to tally the frequencies of the subjects according to some of the columns. The function count() is born for this. For example, we can create a frequency distribution of male and female students of different parental levels of education (i.e., parent_edu x gender contingency table): student1 %&gt;% count(parent_edu, gender) Exercise 6.3 Continuing the above example, how can you create another column, which includes the percentage of male and female students for those of the same parental level education (see below)? 6.10 Tidy Data Now I would like to talk about the idea of tidy dataset. Wickham and Grolemund (2017) suggests that a tidy dataset needs to satisfy the following three interrelated rules: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. However, in real life, we often encounter datasets that are NOT tidy at all. Instead of expecting others to do the tidying work for you (which is very unlikely), we might as well learn how to deal with messy dataset. Wickham and Grolemund (2017) suggests two common strategies data scientists often apply to the untidy dataset: One variable might be spread across multiple columns (from long to wide) One observation might be scattered across multiple rows (from wide to long) 6.10.1 An Long-to-Wide Example Here I would like to illustrate the idea of tidy dataset and also ways of tidying with a simple dataset provided in Wickham and Grolemund (2017), Chapter 12. people &lt;- tribble( ~name, ~profile, ~values, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) people The above dataset people is not tidy because the column profile contains more than one variable. An observation (e.g., Phillip Woods) is scattered across several rows. To tidy up people, we need strategy I: One variable might be spread across multiple columns The function tidyr::pivot_wider() is made for this. There are two important parameters in pivot_wider(): names_from = ...: The column to take variable names from. Here it’s profile. values_from = ...: The column to take values from. Here it’s values. Figure 6.1: From Long to Wide: pivot_wider() require(tidyr) people %&gt;% pivot_wider(names_from = profile, values_from = values) 6.10.2 A Wide-to-Long Example preg &lt;- tribble(~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12) preg The above dataset preg can be tidied up as follows: we can have a column sex we can have a column pregnant we can have a column count (representing the number of observations for the combinations of sex and pregnant) In other words, we need strategy II: One observation might be scattered across multiple rows (from wide to long) (Each level combination of pregnant and sex can be one observation.) And the function tidyr::pivot_longer() is made for this. There are three important parameters: cols = ...: The set of columns whose names are values, not variables. Here they are male and female. names_to = ...: The name of the variable to move the column names to. Here it is sex. values_to = ...: The name of the variable to move the values to. Here it is count. Figure 6.2: From Wide to Long: pivot_longer() preg %&gt;% pivot_longer(cols = c(&quot;male&quot;, &quot;female&quot;), names_to = &quot;sex&quot;, values_to = &quot;count&quot;) Exercise 6.4 Please get familiar with tidyr::separate() and tidyr::unite(), which are two important functions to manipulate the columns of the data frame. 6.11 Exerceises Exercise 6.5 In the dataset demo_data/data-students-performance.csv, please load the dataset in R and print out those students who are female and whose math scores are &lt; 40. In your output, please show the following columns only: gender, math. Exercise 6.6 With the same dataset, please compute the mean scores and standard deviations of math for different races. Also, please include the number of students for each race sub-group. Exercise 6.7 With the same dataset, please create a summary data frame, which includes the number of students, math mean scores, math standard deviations, for students of different genders and parental education levels. Exercise 6.8 In terms of Parental Education Levels (i.e., parent_edu), it would be better to be coded as an ordered factor. Can you tranform the variable parent_edu into a ordered factor and regenerate the outputs requested in Exercise 6.7. Let us assume that the factor parent_edu follows the following order: some high school &lt; high school &lt; some college &lt; associate's degree &lt; bachelor's degree &lt; master's degree Exercise 6.9 Have you any ideas how to generate the following graphs using ggplot2()? Your goal is to re-create graphs that look as similar to the following as possible. Exercise 6.10 In this exercise, please first download the dataset demo_data/data-word-freq.csv. You may use readr::read_csv() to load the dataset into R. This is a dataset including word frequencies in two different corpora. For example, the word the appears 346 times in perl corpus but 229 times in python corpus. In other words, each row in word_freq in fact represents the combination of (WORD, CORPUS) because the column FREQ contains the values for those variables. In addition, the same word appears twice in the dataset in the rows (e.g., the, a). Please transform word_freq into a wider format, where the word frequencies in each corpus can be independent columns (as shown in the second table). If the word appears in only one of the corpora, its frequency would be 0. require(readr) word_freq &lt;- read_csv(&quot;demo_data/data-word-freq.csv&quot;) word_freq References "],["string-manipulation.html", "Chapter 7 String Manipulation 7.1 What is Regular Expression? 7.2 String Basics 7.3 A Good Tool: RegExplain 7.4 Regular Expression Grammar 7.5 Pattern Matching 7.6 Advanced Pattern Matching: Look Ahead and Behind 7.7 More Practices 7.8 Case Study: Chinese Four-Character Idioms", " Chapter 7 String Manipulation In data analysis, most of the time we are dealing with texts/strings. In other words, when wrangling with the data, we need all kinds of techniques in string manipulations, such as finding/replacing a particular string, removing unnecessary strings, combining shorter strings into a longer one, or splitting a long string into smaller tokens. My experience tells me that a competent data scientist often needs a certain level of skills on string processing. Most importantly, an effective use of regular expressions plays the most important role. In this chapter, I would like to introduce some frequently-used techniques relating to string manipulation, with a special focus on regular expression. Also, I will use the package stringr, which is part of the tidyverse framework as well. library(tidyverse) library(stringr) 7.1 What is Regular Expression? In text processing, we often do “find-and-replace” in our documents. I am sure that you do this very often in MS-Word or MS-Excel. The routines reflect the fact that we often need to locate particular sets of strings and perform specific processing on these strings (e.g., replacing, removing, modifying them etc.). This is the exact niche where regular expression can really help a lot. Regular expression is a language, which allows us to create a schematic textual pattern and use this pattern to match other strings that may fall into this schematic pattern. This idea of one-pattern-for-multiple-matches is the beauty of regular expression. Several advantages of regular expressions are self-evident: Effective/Efficient pattern matching e-mail format checking, phone number checking reduplicated strings date format control Information extraction and text mining extract texts according to a particular format Proper names, e-mails, phone numbers, etc. Therefore, the nature of one-to-many mapping for regular expressions allows us to effectively retrieve strings of similar properties in a much simpler way. 7.2 String Basics Before we introduce regular expressions, let’s look at some of the basic string-related functions. There are three basic functions: str_length(): get the length of the string (i.e., number of characters) word_string &lt;- c(&quot;the&quot;, &quot;word&quot;, &quot;string&quot;) word_string %&gt;% str_length [1] 3 4 6 str_c(): combine strings into a longer one str_c(&quot;the&quot;, &quot;word&quot;, &quot;string&quot;) [1] &quot;thewordstring&quot; str_c(&quot;the&quot;, &quot;word&quot;, &quot;string&quot;, sep = &quot;_&quot;) [1] &quot;the_word_string&quot; From the above output, can you tell what is the default value for the argument str_c(..., sep = ...)? Please note that the following code generates a different result from the above. Can you tell the differences? How can you create exactly the same results by using str_c(word_string,…)? Please check ?str_c. str_c(word_string, sep = &quot;_&quot;) [1] &quot;the&quot; &quot;word&quot; &quot;string&quot; When you have several objects in the str_c(), please use the argument str_c(…, sep = …) to combine them into a long string; when you have only one object (but it is a multiple-element character vector), please use the argument str_c(…, collpase = …) to collapse a vector into a long string. str_sub(): substract part of the string by positions str_sub(string = &quot;international&quot;, start = 1, end = 5) [1] &quot;inter&quot; 7.3 A Good Tool: RegExplain There is a very useful tool for the use of regular expressions–RegExplain, which is an RStudio addin. It allows you to: interactively build your regexp and check the output of common string matching functions Figure 7.1: Taken from RegExplain Github Page use the included resources to learn regular expressions Figure 7.2: Taken from RegExplain Github Page consult the interactive help pages Figure 7.3: Taken from RegExplain Github Page This is very useful because we can prepare our regular expressions and use them in the code chunk after we have made sure that they work properly in the RegExplain. You can install the addin using devtools: ## Please install `devtools` if you haven&#39;t install.packages(&#39;devtools&#39;) ## library(devtools) install_github(&#39;gadenbuie/regexplain&#39;) 7.4 Regular Expression Grammar Now let’s look at the grammar of regular expressions in more detail. In this section, we will discuss the standard Perl-compatible regular expression syntax. This is by now the most widely used version of regular expressions in most programming languages. To start with, in stringr, there is a very useful function, str_view(STRING, PATTERN), which can show us the match of the pattern in the string in a visually intuitive way: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(string = x, pattern = &quot;an&quot;) For basic syntax of regular expressions, I will use this str_view() to show you how the regular pattern works in string-matching. You can also use RegExplain addin to test your regular expressions. 7.4.1 Metacharacters To implement the idea of one-to-many mapping, RegEx defines several metacharacters, which are of special use in regular expressions. Their meanings are NOT the same as their literal counterparts. In RegEx, . is a special character, referring to any character: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(string = x, pattern = &quot;.a.&quot;) But what if you really want to match a period . symbol literally in your string? x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;orange. And&quot;) str_view(string = x, pattern = &quot;.&quot;) str_view(string = x, pattern = &quot;\\\\.&quot;) This leads us to the notion of escaping character \\. In R, \\ is used if you want to tell R that the metacharacter after \\ should be treated literally, not metaphorically as a metacharacter. But why two slashes \\\\? It’s simple: because \\ itself is a metacharacter in R as well. We use it to escape quotes like \" and '. Therefore, the first backslash is needed in order to tell R engine that the following character (i.e., the second slash) should be taken literally; the second backslash is needed in order to tell RegEx engine that the following character (i.e., .) should be taken literally. 7.4.2 Anchors RegEx defines a few metacharacters, which serve as anchors in pattern matching. These anchoring metacharacters allow us to find a match in a particular position of the string (e.g., at the beginning/ending of the string). ^: The start of the string x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;^a&quot;) $: The end of the string x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;a$&quot;) x &lt;- c(&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;) str_view(x, &quot;^apple$&quot;) The anchors are evaluated according to the base unit you are matching. In our previous examples, the RegEx pattern is applied to find a match in each element of the vector. The vector includes words. Therefore, the ^ indicates a word-initial position; $ indicates a word-final position. If you have a vector of sentences, the ^ would indicate a sentence-initial position; $ would indicate a sentence-final position. (See below) x &lt;- c(&quot;apple is good&quot;, &quot;banana is better than apple&quot;, &quot;an apple a day keeps the doctor away&quot;) str_view(x, &quot;^apple&quot;) str_view(x, &quot;apple$&quot;) 7.4.3 Character Set RegEx also defines a few character sets because in our patten-matching, the characters included in a set often show similar behaviors. Common predefined character sets include: \\\\d: matches any digit. \\\\s: matches any whitespace (e.g. space, tab, newline). \\\\w: matches any alphanumeric characters x &lt;- c(&quot;apple&quot;, &quot;apple123&quot;, &quot;banana1&quot;) str_view(string = x, pattern = &quot;\\\\d&quot;) In pattern-matching, very often you will have cases where one base unit may have more than one match. In the previous example, str_view() identifies only the first match of each base unit from x. If you need to identify all the matches from each base unit, you need to use str_view_all(). x &lt;- c(&quot;aeiouAEIOU1234_ .\\\\$%-*()&quot;) str_view_all(string = x, pattern = &quot;\\\\w&quot;) # compare str_view_all(string = x, pattern = &quot;.&quot;) The difference of finding-the-first-match (str_view()) vs. finding-all-the-matches (str_view_all()) will turn out to be very important in practical tasks. Please bear this in mind. In the above comparison of the two meta-character sets, . and \\w, it is clear that the \\w includes alphanumetic characters plus the underscore _. It does NOT include the hyphen -, which however can be a word-internal component. 7.4.4 Alternatives Section 7.4.3 describes a few prefined character sets in RegEx. We can also define our own character set using the square brackets [ ]. And we can use [^...] to define a complementary character set. (Please note that ^ has a different RegEx meaning within [].) [abc]: matches a, b, or c. [^abc]: matches anything except a, b, or c. x &lt;- c(&quot;grey&quot;, &quot;gray&quot;) str_view(string = x, pattern = &quot;gr[ea]y&quot;) If you know very well which characters you are to match/find, use inclusive character sets, i.e., […]. If you know very well which characters you do NOT like to match/find, use exclusive character sets, i.e., [^...]. For example, what if we would like to find all non-vowel letters in the words? Instead of coming up with an inclusive character set [...], which include all possible consonant letters, it would be more efficient if you create an exclusive character set, [^aeiou], which suggests that you need any characters that do not belong to vowels. x &lt;- c(&quot;grey&quot;, &quot;gray&quot;) str_view_all(string = x, pattern = &quot;[^aeiou]&quot;) 7.4.5 Quantifiers We can use quantifiers to specify the number of occurrences of a particular pattern (e.g., the character preceding the quantifier): ?: 0 or 1 +: 1 or more *: 0 or more x &lt;- &quot;Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;CC?&quot;) str_view(x, &quot;X+&quot;) We can specify an exact range of number of ocurrences using the curly brackets { , }: {n}: exactly n occurrences {n,}: n or more occurrences {,m}: at most m occurrences {n,m}: between n and m occurrences x &lt;- &quot;Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;C{2}&quot;) str_view(x, &quot;C{2,}&quot;) str_view(x, &quot;C{2,3}&quot;) When we use the quantifiers, be very careful about the scope of the quantifier. By default, the quantifier takes only its preceding character as the scope. If you need to specify the number of occurrences for a group of characters or a specific pattern, you need to put the pattern in a group (…) and then put your quantifier right after the group. x &lt;- &quot;aaabbbababcdf&quot; str_view(x, &quot;ab{2}&quot;) # the scope of the quantifier is `b` str_view(x, &quot;(ab){2}&quot;) # the scope of the quantifier is `ab` 7.4.6 Greedy vs. Non-greedy match The above two examples show you that when the RegEx locates the pattern in the string, it prefers to find a longest match that satisfies the pattern. In the above examples, the substring CC should satisfy the RegEx C{2,} already but the RegEx returns CCC as the match. This is the idea of greedy match. In other words, by default, when we apply quantifiers in our regular expressions, the RegEx engine assumes a greedy match (i.e., to find a longest possible match). To cancel this default greedy match, we can add ? after the quantifiers. x &lt;- &quot;Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;CLX+&quot;) # find longest match str_view(x, &quot;CLX+?&quot;) # find minial match 7.4.7 Group and Back-reference # `fruit` is a preloaded vector from `stringr` x &lt;- fruit %&gt;% head(10) x [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; Now if we want to extract English fruit words which has two same letter repeating twice in a row (e.g, apple, bell pepper). But the problem is: we don’t know which letters are going to be repeated in the words. How should we define our regular expressions? str_view(x, &quot;.{2}&quot;) The results are not what you have expected. The above pattern would only give you the first two characters of each word. The trick is that when you use . to match any character, the quantifier does not help much as any character that repeats would fit the pattern. We need a new strategy to ask the RegEx engine to remember the previously matched character and quantify the number of occurrences of the remembered character: str_view(x, &quot;(.)\\\\1&quot;) The regular expression can be conceptualized as follows: .: matches any character (.): the parenthesis would label the matched as a group. Internally, the RegEx engine numbers all groups serially from left to right \\\\1: back-reference the first group. The same logic applies to the second group of the regular expression (i.e., \\\\2) So (.)\\\\1 means that when the engine matches a character (which can be any character), there has to be another same character following the former. Exercise 7.1 With the same set of fruit names in x, how do we match fruits with a abab pattern, such as “banana?” Exercise 7.2 With the same set of fruit names in x, how do we match fruits with a abba pattern, such as “pepper?” Exercise 7.3 With the same set of fruit names in x, please find fruit names which has at least one letter that is the same as their initial letters 7.5 Pattern Matching This section will show you examples of how we can make use of regular expresions to process strings. In stringr, there are a list of verbs that we use with regular expressions: 7.5.1 str_detect() str_detect(STRING, PATTERN): Determine which strings in STRING has a match of the PATTERN (binary) str_detect(x, &quot;e$&quot;) [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE 7.5.2 str_subset() str_subset(STRING, PATTERN): Subset the full strings in STRING that have either a full or partial match of the PATTERN (character) str_subset(x, &quot;e$&quot;) [1] &quot;apple&quot; &quot;blood orange&quot; 7.5.3 str_extract() str_extract(STRING, PATTERN): Extract the content of the matches of the strings in STRING (character) str_extract(x, &quot;e$&quot;) [1] &quot;e&quot; NA NA NA NA NA NA NA &quot;e&quot; NA Also, please note that str_extract() only extract the first match of the string. To extract all matches from the strings: str_extract(x, &quot;[aeiou]&quot;) # find only the first match [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;e&quot; &quot;i&quot; &quot;a&quot; &quot;a&quot; &quot;o&quot; &quot;u&quot; str_extract_all(x, &quot;[aeiou]&quot;) # find all matches in each string [[1]] [1] &quot;a&quot; &quot;e&quot; [[2]] [1] &quot;a&quot; &quot;i&quot; &quot;o&quot; [[3]] [1] &quot;a&quot; &quot;o&quot; &quot;a&quot; &quot;o&quot; [[4]] [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; [[5]] [1] &quot;e&quot; &quot;e&quot; &quot;e&quot; [[6]] [1] &quot;i&quot; &quot;e&quot; [[7]] [1] &quot;a&quot; &quot;e&quot; [[8]] [1] &quot;a&quot; &quot;u&quot; &quot;a&quot; [[9]] [1] &quot;o&quot; &quot;o&quot; &quot;o&quot; &quot;a&quot; &quot;e&quot; [[10]] [1] &quot;u&quot; &quot;e&quot; &quot;e&quot; 7.5.4 str_match() str_match(STRING, PATTERN): Extract the content of the matches of the strings in STRING as well as each capture groups (character) str_match(x, &quot;(bl)([aeiou]+)&quot;) [,1] [,2] [,3] [1,] NA NA NA [2,] NA NA NA [3,] NA NA NA [4,] NA NA NA [5,] NA NA NA [6,] NA NA NA [7,] &quot;bla&quot; &quot;bl&quot; &quot;a&quot; [8,] &quot;bla&quot; &quot;bl&quot; &quot;a&quot; [9,] &quot;bloo&quot; &quot;bl&quot; &quot;oo&quot; [10,] &quot;blue&quot; &quot;bl&quot; &quot;ue&quot; For each match, we will get not only the full match, but also the capture groups substrings specified in the regular expression. Each group (i.e., parenthesis) in your regular expression will have a partial match in the results. Exercise 7.4 How do you use str_match() to find out all the c+Vowel structures, and at the same time identify which vowels follow the letter c? [,1] [,2] [1,] NA NA [2,] &quot;co&quot; &quot;o&quot; [3,] &quot;ca&quot; &quot;a&quot; [4,] NA NA [5,] NA NA [6,] NA NA [7,] NA NA [8,] &quot;cu&quot; &quot;u&quot; [9,] NA NA [10,] NA NA 7.5.5 str_replace() str_replace(STRING, PATTERN, REPLACEMENT): Replace matches of the PATTERN with REPLACEMENT in STRING (character) str_replace(string = x, pattern = &quot;[aeiou]&quot;, replacement = &quot;V&quot;) [1] &quot;Vpple&quot; &quot;Vpricot&quot; &quot;Vvocado&quot; &quot;bVnana&quot; &quot;bVll pepper&quot; [6] &quot;bVlberry&quot; &quot;blVckberry&quot; &quot;blVckcurrant&quot; &quot;blVod orange&quot; &quot;blVeberry&quot; It should be noted that str_replace() only replaces the first match of each string. str_replace_all(string = x, pattern = &quot;[aeiou]&quot;, replacement = &quot;V&quot;) [1] &quot;VpplV&quot; &quot;VprVcVt&quot; &quot;VvVcVdV&quot; &quot;bVnVnV&quot; &quot;bVll pVppVr&quot; [6] &quot;bVlbVrry&quot; &quot;blVckbVrry&quot; &quot;blVckcVrrVnt&quot; &quot;blVVd VrVngV&quot; &quot;blVVbVrry&quot; 7.5.6 str_split() str_split(STRING, PATTERN): Split a string in STRING based on a PATTERN (character) x &lt;- sentences %&gt;% head(5) x [1] &quot;The birch canoe slid on the smooth planks.&quot; [2] &quot;Glue the sheet to the dark blue background.&quot; [3] &quot;It&#39;s easy to tell the depth of a well.&quot; [4] &quot;These days a chicken leg is a rare dish.&quot; [5] &quot;Rice is often served in round bowls.&quot; str_split(string = x, pattern = &quot;\\\\s&quot;) [[1]] [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; [8] &quot;planks.&quot; [[2]] [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; [6] &quot;dark&quot; &quot;blue&quot; &quot;background.&quot; [[3]] [1] &quot;It&#39;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; &quot;a&quot; &quot;well.&quot; [[4]] [1] &quot;These&quot; &quot;days&quot; &quot;a&quot; &quot;chicken&quot; &quot;leg&quot; &quot;is&quot; &quot;a&quot; [8] &quot;rare&quot; &quot;dish.&quot; [[5]] [1] &quot;Rice&quot; &quot;is&quot; &quot;often&quot; &quot;served&quot; &quot;in&quot; &quot;round&quot; &quot;bowls.&quot; Please note that the return of str_split() is a list. fields &lt;- c(&quot;Name: Hadley&quot;, &quot;Country: NZ&quot;, &quot;Age: 35&quot;) fields %&gt;% str_split(&quot;: &quot;) [[1]] [1] &quot;Name&quot; &quot;Hadley&quot; [[2]] [1] &quot;Country&quot; &quot;NZ&quot; [[3]] [1] &quot;Age&quot; &quot;35&quot; # To get a simpler structure in return: fields %&gt;% str_split(&quot;: &quot;, simplify = T) [,1] [,2] [1,] &quot;Name&quot; &quot;Hadley&quot; [2,] &quot;Country&quot; &quot;NZ&quot; [3,] &quot;Age&quot; &quot;35&quot; Exercise 7.5 Convert American dates American.dates to British dates using str_replace_all(). Please note that in your output, you need to preserve the original delimiters for each date. American.dates &lt;- c(&quot;7/31/1976&quot;, &quot;02.15.1970&quot;, &quot;11-31-1986&quot;, &quot;04/01.2020&quot;) [1] &quot;31/7/1976&quot; &quot;15.02.1970&quot; &quot;31-11-1986&quot; &quot;01/04.2020&quot; Exercise 7.6 Please use the default sentences vector as your input and find all patterns of “any BE verbs + words ending with ‘en’ or ‘ed’.” Please extract these matches from the sentences and your result should be a vector of these matches, as shown below. Hint: Check unlist() and nzchar() sentences[1:5] [1] &quot;The birch canoe slid on the smooth planks.&quot; [2] &quot;Glue the sheet to the dark blue background.&quot; [3] &quot;It&#39;s easy to tell the depth of a well.&quot; [4] &quot;These days a chicken leg is a rare dish.&quot; [5] &quot;Rice is often served in round bowls.&quot; length(sentences) [1] 720 [1] &quot;is often&quot; &quot;were fed&quot; &quot;is used&quot; &quot;was cooked&quot; [5] &quot;was seized&quot; &quot;is used&quot; &quot;was spattered&quot; &quot;is red&quot; [9] &quot;was fired&quot; &quot;is ten&quot; &quot;is used&quot; &quot;are pushed&quot; [13] &quot;are men&quot; &quot;are used&quot; &quot;were hired&quot; &quot;was covered&quot; [17] &quot;were lined&quot; &quot;was ten&quot; &quot;is used&quot; &quot;are paved&quot; [21] &quot;is carved&quot; &quot;were led&quot; &quot;is needed&quot; &quot;were painted&quot; [25] &quot;were mailed&quot; &quot;was pressed&quot; &quot;is seen&quot; &quot;was packed&quot; [29] &quot;was barred&quot; &quot;was crowded&quot; &quot;was carved&quot; &quot;was drilled&quot; [33] &quot;was hidden&quot; &quot;was seen&quot; &quot;were pierced&quot; &quot;are jangled&quot; [37] &quot;is tinged&quot; &quot;were stamped&quot; &quot;was jammed&quot; &quot;was robbed&quot; 7.6 Advanced Pattern Matching: Look Ahead and Behind The regular expressions we have introduced so far will consume the input strings when doing the pattern matching. I would like to illustrate this idea with the following simple example. If we want to find Windows, but only when it is followed by \"95, 98, NT, 2000\", how should we write our RegEx pattern? win &lt;- c(&quot;Windows2000&quot;, &quot;Windows&quot;, &quot;WindowsNT&quot;, &quot;Windows7&quot;, &quot;Windows10&quot;) str_view(win, &quot;Windows((95)|(98)|(NT)|(2000))&quot;) The pattern seems OK because we did find the Windows2000 that satisfies our RegEx. But what if you would like to replace the word “Windows” with “OldSystem” but only when “Windows” is followed by \"95, 98, NT, 2000\"? str_replace(win, pattern = &quot;Windows((95)|(98)|(NT)|(2000))&quot;, replacement = &quot;OldSystem&quot;) [1] &quot;OldSystem&quot; &quot;Windows&quot; &quot;OldSystem&quot; &quot;Windows7&quot; &quot;Windows10&quot; Now you see the problem? Not only “Windows” was replaced, but also the entire string. This is not what we expect to get. Instead, we would expect something like: [1] &quot;OldSystem2000&quot; &quot;Windows&quot; &quot;OldSystemNT&quot; &quot;Windows7&quot; [5] &quot;Windows10&quot; That is why we need lookahead function in RegEx. (?=...): positive lookahead—extract a match only when … is on the right (?!...): negative lookahead—extract a match only when … is NOT on the right str_view(win, &quot;Windows(?=95|98|NT|2000)&quot;) In the above regular expression, the () after Windows is a lookahead condition for matching. The strings of “2000” and “NT” in the condition are NOT consumed. The RegEx engine looks ahead the following few characters first to check the patterns but after it finds a match, the lookahead characters will still be the input of the next pattern-matching. We can make use of this lookahead to do the string replacement: str_replace(string = win, pattern = &quot;Windows(?=95|98|NT|2000)&quot;, replacement = &quot;OldSystem&quot;) [1] &quot;OldSystem2000&quot; &quot;Windows&quot; &quot;OldSystemNT&quot; &quot;Windows7&quot; [5] &quot;Windows10&quot; Moreover, we can create a negative lookahead as well: str_view(win, &quot;Windows(?!7|10)&quot;) str_replace(string = win, pattern = &quot;Windows(?!7|10)&quot;, replacement = &quot;NewSystem&quot;) [1] &quot;NewSystem2000&quot; &quot;NewSystem&quot; &quot;NewSystemNT&quot; &quot;Windows7&quot; [5] &quot;Windows10&quot; In addition to lookahead, we can also specify look-behind conditions: Look-behind conditions: (?&lt;=pattern): The target match has to be preceded by the pattern defined in the condition (?&lt;!pattern): The target match cannot be preceded by the pattern defined in the condition win &lt;- c(&quot;2000Windows&quot;, &quot;Windows&quot;, &quot;NTWindowsNT&quot;, &quot;7Windows&quot;, &quot;10Windows&quot;) str_view(win, &quot;(?&lt;=95|98|NT|2000)Windows&quot;) win &lt;- c(&quot;2000Windows&quot;, &quot;Windows&quot;, &quot;NTWindowsNT&quot;, &quot;7Windows&quot;, &quot;10Windows&quot;) str_view(win, &quot;(?&lt;!95|98|NT|2000)Windows&quot;) Exercise 7.7 Please use the first ten words in the fruit vector for this exercise. Based on the fruit vocabulary, can you identify all the a that is followed by STOP_SOUNDS and replace them with “V?” STOP_SOUNDS refer to p, t, k, b, d, g Hint: str_replace_all() dataset fruit[1:10] [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; target matches your result [1] &quot;Vpple&quot; &quot;Vpricot&quot; &quot;avocVdo&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; Exercise 7.8 Similar to the previous example, Exercise 7.7, also based on the first ten words in fruit, please identify all the vowels that are both followed and preceded by STOP_SOUNDS and replace them with “V.” Vowels are defined as letters including a, e, i, o,and u, dataset fruit[1:10] [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; target matches your result [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pVpper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; 7.7 More Practices This section will show you more examples of the RegEx applications. Most importantly, I will specifically point out the capacities and potentials of regular expressions in helping us manipulating the datasets. In Chapter 6, we have talked about important ways in which we can manipulate our data as a data.frame/tibble. Please refer to the Chapter 6 for a review of the important verbs in dplyr. Exercise 7.9 In the library tidyr, there are two very useful functions for data manipulation: separate(), extract() and unnest(). Please read the documentations of these two functions and run the examples provided in the documentation to make sure that you understand how they work. 7.7.1 Case 1 If we have a data frame like dt below, how do we extract only the numbers of the weeks from the y column, and add this information to a new column, z? dt &lt;- tibble(x = 1:4, y = c(&quot;wk 3&quot;, &quot;week-1&quot;, &quot;7&quot;, &quot;w#9&quot;)) dt Exercise 7.10 If we have a data frame like dt below, how do we extract all the vowels of the words in the WORD column and create two additional columns: Because a word may have several vowels, create a new column, which shows all the vowels in the word by combining them into a long string with the delimiter \"_\" Create another column for the number of vowels for each word Vowels are defined as [aeiou]. dt &lt;- tibble(WORD = fruit[1:10]) dt 7.7.2 Case 2 How to separate the English and Chinese strings in x column and create two new columns, EN, CH? tb &lt;- tibble(x = c(&quot;I我&quot;, &quot;love愛&quot;, &quot;you你&quot;)) tb Exercise 7.11 How to extract the numbers and split the numbers into START and END? df &lt;- tibble(x = c(&quot;1-12周&quot;, &quot;1-10周&quot;, &quot;5-12周&quot;)) df 7.7.3 Case 3 How to extract numbers of each token and compute the sum of the number and save the results in a new column SUM? df &lt;- tibble(x = c(&quot;1234&quot;, &quot;B246&quot;, &quot;217C&quot;, &quot;2357f&quot;, &quot;21WD4&quot;)) df Exercise 7.12 How to extract all the numbers that follow a upper-casing letter? For example, 34 after W; 217 after B? df &lt;- tibble(x = c(&quot;12W34&quot;, &quot;AB2C46&quot;, &quot;B217C&quot;, &quot;akTs6df&quot;, &quot;21WD4&quot;)) df Exercise 7.13 Based on Exercise 7.12, can you add another column to the resulting data frame, which records the upper-casing letter that the numbers follow for each row? Hint: Check str_match_all() and tidyr::unnest() 7.8 Case Study: Chinese Four-Character Idioms Many studies have shown that Chinese makes use of large proportion of four-character idioms in the discourse. Let’s have an exploratory analysis of four-character idioms in Chinese. 7.8.1 Dictionary Entries In our demo_data directory, there is a file demo_data/dict-ch-idiom.txt, which includes a list of four-character idioms in Chinese. These idioms are collected from 搜狗輸入法詞庫 and the original file formats (.scel) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., .txt. Let’s first load the idioms dataset in R. all_idioms &lt;- readLines(con = &quot;demo_data/dict-ch-idiom.txt&quot;, encoding = &quot;UTF-8&quot;) head(all_idioms) [1] &quot;阿保之功&quot; &quot;阿保之勞&quot; &quot;阿鼻地獄&quot; &quot;阿鼻叫喚&quot; &quot;阿斗太子&quot; &quot;阿芙蓉膏&quot; tail(all_idioms) [1] &quot;罪無可逭&quot; &quot;罪人不帑&quot; &quot;作纛旗兒&quot; &quot;坐纛旂兒&quot; &quot;作姦犯科&quot; &quot;作育英才&quot; length(all_idioms) [1] 56536 In order to make use of the tidy structure in R, we convert the data into a tibble: idiom &lt;- tibble(string = all_idioms) idiom %&gt;% head 7.8.2 Case Study: X來Y去 We can create a regular expression pattern to extract all idioms with the format of X來X去: idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) To analyze the meaning of this constructional schema, we may need to extract the X and Y in the schema: idiom_laiqu &lt;- idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) %&gt;% mutate(pattern = str_replace(string, &quot;(.)來(.)去&quot;, &quot;\\\\1_\\\\2&quot;)) %&gt;% separate(pattern, into = c(&quot;w1&quot;, &quot;w2&quot;), sep = &quot;_&quot;) idiom_laiqu # # version 2 require(tidyr) idiom %&gt;% filter(str_detect(string, &#39;.來.去&#39;)) %&gt;% # mutate(string2 = string) %&gt;% extract(col=&#39;string2&#39;, into=c(&#39;w1&#39;,&#39;w2&#39;), regex # = &#39;(.)來(.)去&#39;) One empirical question is how many of these idioms are of the pattern W1 = W2 (e.g., 想來想去, 直來直去) and how many are of the pattern W1 != W2 (e.g., 說來道去, 朝來暮去): # Create `structure` column idiom_laiqu_2 &lt;- idiom_laiqu %&gt;% mutate(structure = ifelse(w1 == w2, &quot;XX&quot;, &quot;XY&quot;)) idiom_laiqu_2 # Count `structure` frequecnies idiom_laiqu_count &lt;- idiom_laiqu_2 %&gt;% count(structure) idiom_laiqu_count # Create barplots idiom_laiqu_count %&gt;% ggplot(aes(structure, n, fill = structure)) + geom_col() ########################## Another alterantive### # idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &#39;XX&#39;,&#39;XY&#39;)) %&gt;% # count(structure) %&gt;% ggplot(aes(structure, n, fill = structure)) + geom_col() Exercise 7.14 Please use same dataset idiom (loaded from demo_data/dict-ch-idiom.txt) and extract all the idioms that fall into the schema of 一X一Y. idiom &lt;- tibble(string = readLines(&quot;demo_data/dict-ch-idiom.txt&quot;)) Exercise 7.15 Also with the idiom as our data source, now if we are interested in all idioms that have duplicated characters in them, with schemas like either _A_A or A_A_, where A is a fixed character. How can we extract all idioms of these two types from idiom? Also, please visualize the distribution of the two idiom types using a bar plot. Sample answers have been provided below. Idioms with duplicate characters in them Type Distribution Exercise 7.16 Following Exercise 7.15, for each type of the idioms (i.e., “A_A_” or \"_A_A\"), please provide their respective proportions of W1 = W2 vs. W1 != W2, where W1 and W2 refer to the words filled in the variable slots of the idiomatic templates. The following table is a random sample of aech idiom type (5 tokens for each type) (Not sure if you can get the exact sample results with set.seed(123)): "],["data-import.html", "Chapter 8 Data Import 8.1 Overview 8.2 Importing Data 8.3 Character Encoding 8.4 Character, Code Point, and Hexadecimal Mode 8.5 R Base Functions 8.6 readr Functions 8.7 Directory Operations", " Chapter 8 Data Import Most of the time, we need to work with our own data. In this chapter, we will learn the fundamental concepts and techniques in data I/O (input/output). In particular, we have two main objectives: Learn how to load our data in R from external files Learn how to save our data in R to external files 8.1 Overview There are two major file types that data analysts often work with: txt and csv files. Following the spirit of tidy structure, we will introduce the package, readr, which is also part of the tidyverse. This package provides several useful functions for R users to load their data efficiently from plain-text files. In particular, we will introduce the most effective functions: read_csv() and write_csv() for the loading of CSV files. 8.1.1 What is a CSV file? A CSV is a comma-separated values file, which allows data to be saved in a tabular format. CSVs look like a spreadsheet but with a .csv extension. A CSV file has a fairly simple structure. It’s a list of data separated by commas. For example, let’s say you have a few contacts in a contact manager, and you export them as a CSV file. You’d get a file containing texts like this: Name,Email,Phone Number,Address Bob Smith,bob@example.com,123-456-7890,123 Fake Street Mike Jones,mike@example.com,098-765-4321,321 Fake Avenue CSV files can be easily viewed with any of the text editors (e.g., Notepad++, TextEdit), or spreadsheet programs, such as Microsoft Excel or Google Spreadsheets. But you can only have one single sheet in a file and all data are kept as normal texts in the file. 8.1.2 Why are .CSV files used? There are several advantages of using CSV files for data exchange: CSV files are plain-text files, making them easier for the developer to create Since they’re plain text, they’re easier to import into a spreadsheet or another storage database, regardless of the specific software you’re using To better organize large amounts of data 8.1.3 How do I save CSV files? Saving CSV files is relatively easy. You just need to know where to change the file type. With a normal spreadsheet application (e.g., MS Excel), under the “File” section in the “Save As” tab, you can select it and change default file extension to \"CSV (Comma delimited) (*.csv)\". Although people mostly use the comma character to separate (or delimit) data, sometimes people use other characters like the tab, i.e., \\t. Therefore, sometimes people use the file extension TSV to indicate that the tabular data in the file are delimited by tab (indicating the column boundaries). Please also note that all these tabular files can be named with the extension .txt as well. However, a more intuitive file extension is definitely better for data management. 8.1.4 What about other types of data? In data analysis, we sometimes may need to deal with some other types of data files in addition to the plain-texts. For the import/export of other types of data, R also has specific packages designed for them: haven - SPSS, Stata, and SAS files readxl - excel files (.xls and .xlsx) DBI - databases jsonlite - json xml2 - XML httr - Web APIs rvest - HTML (Web Scraping) readtext - large text data (corpus) 8.2 Importing Data Figure 8.1: Data I/O 8.3 Character Encoding Before we move on to the functions for data I/O, I would like to talk about the issues of character encoding. For all characters in languages, computers use numbers like 1 and 0 to encode all these characters. The more characters a language has, the more numbers are needed in the encoding. For example, for an English text, it usualy consists of only Latin alphabets, and limited sets of essential numbers and punctuation marks. A 7-bit encoding scheme is enough to cover all the possible characters in English. The 7-bit encoding gives us the maximum of 128 (27 = 128) possible different characters to encode: each coding number corresponds to one unique character. The ASCII (American Standard Corde for Information Interchange) encoding, a 7-bit character encoding developed from telegraph code, is a well-known encoding for English texts. 8.3.1 Problems with ASCII Many languages have more characters German umlauts (über) Accents in Indo-European Languages (déjà) Asciification/Romanization ASCII equivalents were defined for foreign characters that have not been included in the original character set Indo-European languages developed extended verions of ASCII 8.3.2 From 7-bit to One-Byte Encoding Single byte (8-bit) encoding Reserve the first 128 characters for ASCII characters 8-bit allows at most 256 possibilities ISO-8859-1 Encoding 8.3.3 Problems with Single-Byte Encoding Inconsistency A large number of overlapping character sets for encoding characters in different languages Often more than one symbol maps to the same code point/number Compatibility Still can’t deal with writing systems with large character sets (CKJ languages) Two-byte character sets Represent up to 65,536 distinct characters (216 = 65536) Multiple byte encoding Big-5 encoding for traditional Chinese GB encoding for simplified Chinese 8.3.4 Problems with Multi-byte Encoding Different languages still use different encodings Some are single-byte, while some are multiple-byte Digital texts usually have many writing systems single-byte texts, letters, spaces, punctuations, Arabic numerals Interspersed with 2-byte Chinese characters Problems Inconsistency in the byte-encoding number Compatibility across different languages 8.3.5 Unicode and UTF-8 Objective Seek to eliminate this character set ambiguity by specifying a Universal Character Set Including over 100,000 distinct coded characters Used in all the common writing systems today UTF-8 Each character is coded by a one to four byte encoding ASCII characters require 1 byte in UTF-8 (0-127) ISO-8859 characters require 2 bytes in UTF-8 (128-2,047) CKJ characters require 3 bytes in UTF-8 (2,048-65,535) Rare characters require 4 bytes in UTF-8 (65,536-1,112,064) Strengths Encodes text in any language Encodes characters in variable-length character encoding Encodes characters with no overlap or confusion between conflicting bytes ranges THE standard encoding now Web page, XML, JSON for data transmission 8.3.6 Suggestions Always use the UTF-8 when you create your dataset Always check the encoding of the dataset When loading the dataset, always specify the encoding of the file Be very careful of the default encoding used in the applications from which the dataset is created/collected/edited (e.g., MS-Word, MS-Excel, Praat, SPSS etc.) In Mac/Linux, the default encoding for files is UTF-8; in Windows, it is NOT. Be very careful if you are a Windows user. 8.4 Character, Code Point, and Hexadecimal Mode Character: A minimal unit of text that has semantic value in the language (cf. morpheme vs. grapheme) Code Point: Any legal numeric value in the character encoding set Hexadecimal: A positional system that represents numbers using a base of 16. In R, there are a few base functions that work with these concepts: Encoding(): Read or set the declared encodings for a character vector iconv(): Convert a character vector between encodings utf8ToInt(): Convert a UTF-8 encoded character to integers (code number in decimals) as.hexamode(): Convert numbers into Hexadecimals x1 &lt;- &quot;Q&quot; Encoding(x1) [1] &quot;unknown&quot; ## Convert the encoding to UTF-8 x2 &lt;- iconv(x1, to = &quot;UTF-8&quot;) Encoding(x2) # ASCII chars are never marked with a declared encoding [1] &quot;unknown&quot; x1_decimal &lt;- utf8ToInt(x1) x1_decimal # code point of `Q` [1] 81 x1_hex &lt;- as.hexmode(x1_decimal) x1_hex # code point in hexadecimal mode of `Q` [1] &quot;51&quot; We can represent characters in UTF-8 code point (hexadecimals) by using the escape \"\\u....\": print(&quot;Q&quot;) [1] &quot;Q&quot; The following is an example of a Chinese character. y1 &lt;- &quot;臺&quot; Encoding(y1) # Non-ASCII char encoding in R. What&#39;s the output in Windows? [1] &quot;UTF-8&quot; y1_decimal &lt;- utf8ToInt(y1) y1_decimal [1] 33274 y1_hex &lt;- as.hexmode(y1_decimal) y1_hex [1] &quot;81fa&quot; print(&quot;臺&quot;) [1] &quot;臺&quot; Exercise 8.1 What is the character of the Unicode code point (hexadecimal) U+20AC? How do you find out the character in R? Exercise 8.2 What is the Unicode code point (hexadecimal) for the character 我 and 你? Which character is larger in terms of the code points? Hexadecimal numerals are widely used by computer system designers and programmers, as they provide a human-friendly representation of binary-coded values. One single byte can encode 256 different characters, whose values may range from 00000000 to 11111111 in binary form. These binary forms can be represented as 00 to FF in hexadecimal. Exercise 8.3 In our demo_data directory, there are three text files in different encodings: demo_data/chinese_gb2312.txt demo_data/chinese_big5.txt demo_data/chinese_utf8.txt Please figure out ways to load these files into R as vectors properly. All three files include the same texts: [1] &quot;這是中文字串。&quot; &quot;文本也有English Characters。&quot; The simplified Chinese version (i.e., chinese_gb2312.txt) [1] &quot;这是中文字串。&quot; &quot;文本也有English Characters。&quot; 8.5 R Base Functions 8.5.1 readLines() As we are often dealing with text data, we may need to import text files in R for further data processsing. In R, there is a base function readLines(), which reads a txt file with the line breaks as the delimiter and returns the content of the file as a (character) vector. That is, each line in the text will be one element in the vector. alice &lt;- readLines(con = &quot;demo_data/corp-alice.txt&quot;) alice[1:10] [1] &quot;[Alice&#39;s Adventures in Wonderland by Lewis Carroll 1865]&quot; [2] &quot;&quot; [3] &quot;CHAPTER I. Down the Rabbit-Hole&quot; [4] &quot;&quot; [5] &quot;Alice was beginning to get very tired of sitting by her sister on the&quot; [6] &quot;bank, and of having nothing to do: once or twice she had peeped into the&quot; [7] &quot;book her sister was reading, but it had no pictures or conversations in&quot; [8] &quot;it, &#39;and what is the use of a book,&#39; thought Alice &#39;without pictures or&quot; [9] &quot;conversation?&#39;&quot; [10] &quot;&quot; class(alice) [1] &quot;character&quot; length(alice) [1] 3331 Depending on how you arrange the contents in the text file, you may sometimes get a vector of different types: If each paragraph in the text file is a word, you get a word-based vector. If each paragraph in the text file is a sentence, you get a sentence-based vector. If each paragraph in the text file is a paragraph, you get a paragraph-based vector. Exercise 8.4 Based on the inspection of the vector alice created above, what is the content of each line in the original txt file (demo_data/corp-alice.txt)? 8.5.2 writeLines() After processing the data in R, we often need to save our data in an external file for future reference. For text data, we can use the base function writeLines() to save a character vector. By default, each element will be delimited by a line break after it is exported to the file. output &lt;- sample(alice[nzchar(alice)], 10) writeLines(output, con = &quot;corp-alice-2.txt&quot;) Exercise 8.5 Please describe the meaning of the data processing in the code chunk above: sample(alice[nzchar(alice)],10). What did it do with the vector alice? Exercise 8.6 The above code is repeated below. How do you re-write the code using the %&gt;% pipe-based syntax with a structure provided below? Your output should be exactly the same as the output produced by the original codes. library(tidyverse) alice %&gt;% ... %&gt;% ... ... %&gt;% ... %&gt;% writeLiens(con=&quot;corp-alice-2.txt&quot;) 8.6 readr Functions 8.6.1 readr::read_csv() Another common source of data is a spreadsheet-like file, which corresponds to the data.frame in R. Usually we save these tabular data in a csv file, i.e., a comma-separated file. Although R has its own base functions for csv-files reading (e.g., read.table(), read.csv() etc.), here we will use the more powerful version read_csv() provided in the library of readr: library(readr) nobel &lt;- read_csv(&quot;demo_data/data-nobel-laureates.csv&quot;) nobel The csv file is in fact a normal plain-text file. Each line consists of a row data, with the columns separated by commas. Sometimes we may receive a dataset with other self-defined characters as the delimiter. Another often-seen case is to use the tab as the delimiter. Files with tab as the delimiter are often with the extension tsv. In readr, we can use read_tsv() to read tsv files. gender_freq &lt;- read_tsv(file = &quot;demo_data/data-stats-f1-freq.tsv&quot;) gender_freq 8.6.2 readr::write_csv() In readr, we can also export our data frames to external files, using write_csv() or write_tsv(). Exercise 8.7 Load the plain-text csv file demo_data/data-bnc-bigram.csv into a data frame and print the top 20 bigrams in the R console arranged by their frequencies (i.e., bi.freq column). Exercise 8.8 Following Exercise 8.7, please export the data frame of the top 20 bigrams to an external file, named data-bnc-bigram-10.csv, and save it under your current working directory. 8.7 Directory Operations When we work with files, we often need to deal with directories as well. It should now be clear that we need to know very well both the paths and filenames of the external data in order to properly load/save the data in R. There are a few important directory operations that we often need when working with files (import/export): getwd(): check the working directory of the current R session setwd(): set the working directory for the current R session getwd() setwd() By default, R looks for the filename or the path under the working directory unless the absolute/relative path to the files/directories is particularly specified. dir(path = &quot;demo_data&quot;, full.names = FALSE, recursive = FALSE) file.exists(&quot;demo_data/data-bnc-bigram.csv&quot;) dir(path = &quot;../../&quot;) Exercise 8.9 Please make yourself familar with the following commands: file.create(), dir.create(), unlink(), basename(),file.info(), save(), and load(). Exercise 8.10 Please create a sub-directory in your working directory, named temp. Load the dataset demo_data/data-bnc-bigram.csv and subset bigrams whose bigram frequencies (bi.freq column) are larger than 200. Order the sub data frame according to the bigram frequencies in a descending order and save the sub data frame into a csv file named data-bnc-bigram-freq200.csv in the temp directory. bnc_bigram_freq200 &lt;- read_csv(&quot;temp/data-bnc-bigram-freq200.csv&quot;) bnc_bigram_freq200 "],["conditions-and-loops.html", "Chapter 9 Conditions and Loops 9.1 if Statements 9.2 for 9.3 while loop 9.4 Toy Example", " Chapter 9 Conditions and Loops When you start to write more sophisticated programs with R, you will very often need to control the flow and order the execution in your code. You will usually run into two types of scenarios: Make the execution of a particular code chunk dependent on a condition Repeat a particular code chunk a certain number of times, which is often referred to as loops. In this Chapter, we will explore these core programming techniques using: if-else statements for and while loops However, in Chapter 10, we will talk about loops more and point you to the idiomatic ways of dealing with loops in R. 9.1 if Statements The main purpose of if is to control precisely which operations are carried out in a given code chunk. A if statement runs a code chunk only if a certain condition is true. This conditional expression allows R to respond differently depending on whether the condition is TRUE or FALSE. The basic template of if is as follows: if(CONDITION IS TRUE){ DO THIS CODE CHUNK 1 } else { DO THIS CODE CHUNK 2 } The condition is palced in the parenthesis after if. The condition must be an expression that returns only a single logical value (TRUE or FALSE). If it is TRUE, the code chunk 1 in the braces will be executed; if the condition is not satisfied, the code chunk 2 in the braces after else will be executed. Let’s create a simple password checker. Imagine that your system password is stored on the server. You can only get into the system if you enter the correct password. For every password you enter, the system gatekeeper will check if your input password matches the one stored on the server. If they don’t match, you will be banned from entry. input &lt;- 113 # assuming that you have the input 113 if (input == 987) { writeLines(&quot;Congratulations! Now you may get in!&quot;) } else { writeLines(&quot;Sorry! Wrong password.&quot;) } Sorry! Wrong password. If they match, you will be allowed to get through. input &lt;- 987 # assuming that you have the input 113 if (input == 987) { writeLines(&quot;Congratulations! Now you may get in!&quot;) } else { writeLines(&quot;Sorry! Wrong password.&quot;) } Congratulations! Now you may get in! Now we can ask R to read the input directly from the user’s input in the R console: input &lt;- readline(prompt = &quot;Please enter your password:&quot;) if (input == 987) { writeLines(&quot;Congratulations! Now you may get in!&quot;) } else { writeLines(&quot;Sorry! Wrong password.&quot;) } 9.2 for The for loop statment is to repeat a code chunk, often while incrementing an index or counter. The most frequent scenario is to repeat a code chunk through a vector/list, element by element, or through a data frame row by row (or column by column). The basic for loop template is as follows: for(LOOP_INDEX in LOOP_VECTOR){ DO THIS CODE CHUNK } The LOOP_INDEX is a placeholder that represents an element in the LOOP_VECTOR. When the loop begins, the LOOP_INDEX starts off as the first element in the vector. When the loop reaches the end of the brace, the LOOP_INDEX is incremented, taking on the next element in the vector. This process continues until the loop reaches the final element of the LOOP_VECTOR. At this point, the code chunk is executed for the last time, and the loop exits. For example, if we have a character vector with a few words in it. We can use a for loop to get the number of characters for each element in the vector. word_vec &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;watermelon&quot;, &quot;papaya&quot;) for (w in word_vec) { word_nchar &lt;- nchar(w) writeLines(as.character(word_nchar)) } 5 6 10 6 For the above example, there is another way to write the for loop: for (i in 1:length(word_vec)) { word_nchar &lt;- nchar(word_vec[i]) writeLines(as.character(word_nchar)) } 5 6 10 6 In our first example, the LOOP_INDEX directly represent elements in the LOOP_VECTOR. In our second example, the LOOP_INDEX represents indexes of the vector. 9.3 while loop There is another type of loop. Unlike the for loop, which repeats a code chunk by going through every element in a vector/list/data frame, the while loop repeats a code chunk UNTIL a specific condition evalutes to FALSE. The basic template is as follows: while(LOOP_CONDITION){ DO THIS CODE CHUNK (UNTIL THE LOOP_CONDITION BECOMES FALSE) } Upon the start of a while loop, the LOOP_CONDITION is evaluated. If the condition is TRUE, the braced code chunk is executed line by line till the end of the chunk. At this point, the LOOP_CONDITION will be checked again. The loop terminates immediately when the condition is evaluated to be FALSE. Based on the template above, you should infer that the code chunk executed must somehow cause the loop to exit, and very often will change the value of certain objects, which would eventaully lead to the change of the LOOP_CONDITION. If nothing ever changes the LOOP_CONDITION, R will crash due to the infinite loops. Let’s come back to our password checker. This time let’s create a dumb checker. When you give a wrong password which is smaller than your true answer, it will automatically approach the right asnwer for you (and of course no real-world application would do that!) ans &lt;- 90 guess &lt;- 83 while (guess != ans) { writeLines(&quot;Your `guess` is too small! The system will take care for you!&quot;) guess &lt;- guess + 1 cat(&quot;Now the system is adjusting your `guess` to &quot;, guess, &quot;\\n&quot;) } Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 84 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 85 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 86 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 87 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 88 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 89 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 90 Exercise 9.1 In the above example, if the guess is smaller than ans, our script works fine. However, if the guess is larger than ans, then our script will crash. How to fix it? In other words, you script should produce the following results when guess (83) &lt; ans (90): Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 84 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 85 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 86 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 87 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 88 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 89 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 90 And your script should produce the following results when guess (100) &gt; ans (90): Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 99 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 98 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 97 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 96 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 95 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 94 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 93 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 92 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 91 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 90 9.4 Toy Example Now we are playing the Guess Game. The game is as follows: I pick a number from 1 to 100. You have to guess which number I picked. Every time you guess wrong, I’ll tell you whether the number is higher or lower. We first pack the game as an R function object: guessMyNumber &lt;- function() { ans &lt;- 18 guess &lt;- readline(prompt = &quot;Please guess my number(0~100):&quot;) guess &lt;- as.numeric(guess) while (guess != ans) { if (guess &lt; ans) { writeLines(&quot;The asnwer is HIGHER.&quot;) guess &lt;- readline(prompt = &quot;Please guess my number(0~100):&quot;) guess &lt;- as.numeric(guess) } else { writeLines(&quot;The asnwer is LOWER&quot;) guess &lt;- readline(prompt = &quot;Please guess my number(0~100):&quot;) guess &lt;- as.numeric(guess) } } writeLines(&quot;Correct!&quot;) } # endfunc You can play the game by running the function guessMyNumber(): guessMyNumber() Exercise 9.2 (optional) The above guessMyNumber() can be improved. Sometimes naughty users would not input numbers as requested. Instead, they may accidentally (or on purpose) input characters that are NOT numbers in their guesses. How can you adjust the guessMyNumber() so that when users input non-numeric characters, your function would send out a warning message? (See below.) &gt; guessMyNumber_v2() Please guess my number(0~100):12 The asnwer is HIGHER. Please guess my number(0~100):80 The asnwer is LOWER Please guess my number(0~100):1000 The asnwer is LOWER Please guess my number(0~100):a ****Please behave. Input an INTEGER!!!**** Please guess my number(0~100):f ****Please behave. Input an INTEGER!!!**** Please guess my number(0~100):grwf ****Please behave. Input an INTEGER!!!**** Please guess my number(0~100): "],["iteration.html", "Chapter 10 Iteration 10.1 Code Duplication 10.2 vector vs. list in R 10.3 Iteration 10.4 purr 10.5 purr + dplyr 10.6 Writing Own Functions", " Chapter 10 Iteration 10.1 Code Duplication Code duplication is tedious and hard to maintain and debug. Plus, there are obvious advantages for reducing code duplications. According to Wickham and Grolemund (2017) Chapter 21 Iteration, there are three main advantages of doing so: It’s easier to see the intent of your code, because your eyes are drawn to what’s different, not what stays the same. It’s easier to respond to changes in requirements. As your needs change, you only need to make changes in one place, rather than remembering to change every place that you copied-and-pasted the code. You’re likely to have fewer bugs because each line of code is used in more places. There are in general two major ways to reduce duplication in coding: wrap the duplicate procedures into a function use iteration In this chapter, we talk about code efficiency. In particular we will work with the library purr. 10.2 vector vs. list in R Most of the R-internal functions are vectorized. By default, if we apply a function to a multi-element vector, R will automatically apply the same procedure to each element of the vector, and return the results of the same length. a.vec &lt;- c(1:10) sqrt(a.vec) [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 [9] 3.000000 3.162278 round(a.vec, 2) [1] 1 2 3 4 5 6 7 8 9 10 But this is NOT something we can do with a list: a.list &lt;- list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) sqrt(a.list) Error in sqrt(a.list): non-numeric argument to mathematical function 10.3 Iteration As we work with list and data.frame (tibble) very often, it would be great if we can have an easy way to apply the same procedure to: each element in the list each row in the data.frame each column in the data.frame These three scenarios are the most-often used contexts for iteration. Let’s start with a scenario. We first create a pseudo data set, i.e., a list with students’ grades from five different classes. exams.list &lt;- list( class1 = round(runif(30, 0, 100)), # 30 tokens of random numbers in the range &lt;0, 100&gt; class2 = round(runif(30, 0, 100)), class3 = round(runif(30, 0, 100)), class4 = round(runif(30, 0, 100)), class5 = round(runif(30, 0, 100)) ) exams.list $class1 [1] 61 35 11 24 67 42 79 10 43 98 89 89 18 13 65 34 66 32 19 78 9 47 51 60 33 [26] 49 95 48 89 91 $class2 [1] 61 41 15 94 30 6 95 72 14 55 95 59 40 65 32 31 22 37 98 15 9 14 69 62 89 [26] 67 74 52 66 82 $class3 [1] 79 98 44 31 41 1 18 84 23 24 8 25 73 85 50 39 25 11 39 57 22 44 22 50 35 [26] 65 37 36 53 74 $class4 [1] 22 41 27 63 18 86 75 67 62 37 53 87 58 84 31 71 27 59 48 27 56 91 90 27 32 [26] 99 62 94 47 41 $class5 [1] 66 15 57 24 96 60 52 40 88 36 29 17 17 48 25 22 67 5 70 35 41 82 92 28 96 [26] 73 69 5 40 48 If we like to compute the mean scores of each cluster, you probably want to use mean(): mean(exams.list) [1] NA It should be clear now that mean() expects a numeric vector, on which the mean score is computed. So you may think that why not do it in a dumb way? We can compute the mean scores for each class and save all the five scores in a list: set.seed(123) # Make sure we get the same results exams.list.means &lt;- list(class1mean = mean(exams.list$class1), class2mean = mean(exams.list$class2), class3mean = mean(exams.list$class3), class4mean = mean(exams.list$class4), class5mean = mean(exams.list$class5)) exams.list.means $class1mean [1] 51.5 $class2mean [1] 52.03333 $class3mean [1] 43.1 $class4mean [1] 56.06667 $class5mean [1] 48.1 The disadvantage is obvious: (a) what if you have 10 classes? 100 classes? (b) what if now you decide to compute standard deviation? The rule-of-thumb is that the more you find code reduplication in your script, the more you need to restructure your codes with iterations. 10.4 purr library(tidyverse) Now let’s take a look at how iteration structures can help us with repeated procedures. exams.list %&gt;% map(mean) $class1 [1] 51.5 $class2 [1] 52.03333 $class3 [1] 43.1 $class4 [1] 56.06667 $class5 [1] 48.1 With only one-line code, you have achieved your goal. map() is a very powerful function to do iteration. Its usage is as follows: To conceptualize this code map(exams.list, mean): For each element in the exams.list, apply the function mean Do the first element, and save the result in the first elemenet of the new list Do the second element, and save the result in the second elemvent of the new list … After finishing all elements in the exams.list, return the new list result In purrr, by default map() returns results in a list format. You can specify a particular data structure you like by using other variants of the mapping function: exams.list %&gt;% map_df(mean) exams.list %&gt;% map_dbl(mean) class1 class2 class3 class4 class5 51.50000 52.03333 43.10000 56.06667 48.10000 Exercise 10.1 Use the same dataset, exam.list, and compute the median and standard deviation for each class. Median class1 class2 class3 class4 class5 48.5 57.0 39.0 57.0 44.5 Standard Deviation class1 class2 class3 class4 class5 28.26933 28.86052 24.75027 24.41725 27.23885 10.5 purr + dplyr The map() function can be very powerful and efficient in data frame manipulation when used in combination with the mutate(). Let’s first load the dataset of four-word idioms from the previous chapter. idiom &lt;- tibble(string = readLines(&quot;demo_data/dict-ch-idiom.txt&quot;, encoding = &quot;utf-8&quot;)) idiom Now if we would like to find out whether each idiom has duplicate characters in it, we can make use of regular expressions: x &lt;- idiom$string[1:10] str_detect(x, &quot;.*(.).*\\\\1.*&quot;) [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE What if we would like to include this information in an independent column of idiom? Two important things should come up in your mind: We need mutate() to help us create a new column We need to apply the above procedure, str_detect(), to each element in the column idiom$string. So would you give it a try? (The following example shows only rows whose duplicate column is TRUE) Exercise 10.2 This exercise will use the subset of idiom, which include only four-word idioms with at least one duplicate character in them. Please create a new column, showing how many types of duplications there are in each idiom? For example, in 阿狗阿貓, there is only one duplicate character 阿; but in 矮矮胖胖, there are two duplicate characters, 矮 and 胖. Exercise 10.3 Continuing the previous exercise, please create another new column, showing all the duplicate characters in each idiom. For example, in 阿狗阿貓, the duplicate character is 阿; but in 矮矮胖胖, the duplicate character is矮_胖. That is, if the idiom has more than one duplicate character, please use the _ as the delimiter and concatenate all duplicate characters into a long string. Exercise 10.4 Based on the previous exercise, please analyze the distribution of all the duplicate characters in the four-character idioms included in the dictionary (i.e., the duplicate_char column from the previous exercise) and identify the top 20 duplicate characters. Please visualize your results in a bar plot with both the top 20 duplicate characters as well as the number of the character’s duplications in the idiom. 10.6 Writing Own Functions With the power and flexibility of purrr::map(), we can basically do everything iteratively. More attractively, we can apply a self-defined function as well! A function object is defined using the following template: FUNCTION_NAME &lt;- function(ARG1, ARG2) { THINGS TO BE DONE WITHIN THE FUNCTION return(...) } A function object usually include: Self-defined name Arguments Return Let’s consider a simple example. First we create own self-defined function my_center(): This function takes a vector object x Substract each element of x by the mean score of x return the resulting vector as the output of the function my_center &lt;- function(x) { x - mean(x) } Now we can apply our my_center function to each class in exams.list: exams.list %&gt;% map_df(my_center) ## or alternatively map_df(exams.list, my_center) Exercise 10.5 Use the built-in the mtcars dataset (?mtcars for more detail). How do you get the class type of each column in the mtcars by using map()? Exercise 10.6 Create a self-defined function to convert each number of a numeric vector to a “z” score. y &lt;- c(1, 4, 6, 10, 20) my_z(y) [1] -0.9779865 -0.5704921 -0.2988292 0.2444966 1.6028112 Exercise 10.7 Use the earlier dataset exams.list. For each element in exams.list, please convert the student’s score to a z-score by applying your self-defined function in an iterative structure (e.g., map). Please present the result as a data frame. References "],["data-scientist-first-step.html", "Chapter 11 Data Scientist First Step 11.1 Loading Nobel Laureates Dataset 11.2 Column Names 11.3 Data Completeness NA 11.4 Data Preprocessing 11.5 Exploratory Analysis 11.6 Exercises", " Chapter 11 Data Scientist First Step Finally this chapter will demonstrate how you can make use of what you have learned from the previous chapters to perform an exploratory data analysis on the dataset you are interested in. Here we will look at a dataset of Nobel Laureates. Before you start, always remember to load necessary R libraries first. library(tidyverse) 11.1 Loading Nobel Laureates Dataset df &lt;- read_csv(&quot;demo_data/data-nobel-laureates.csv&quot;) df 11.2 Column Names Before we start, it is clear to see that the column names are full of spaces, which we would like to get rid off. So first, we remove all the spaces in the columns and replace them with _: # original column names names(df) [1] &quot;Year&quot; &quot;Category&quot; &quot;Prize&quot; [4] &quot;Motivation&quot; &quot;Prize Share&quot; &quot;Laureate ID&quot; [7] &quot;Laureate Type&quot; &quot;Full Name&quot; &quot;Birth Date&quot; [10] &quot;Birth City&quot; &quot;Birth Country&quot; &quot;Sex&quot; [13] &quot;Organization Name&quot; &quot;Organization City&quot; &quot;Organization Country&quot; [16] &quot;Death Date&quot; &quot;Death City&quot; &quot;Death Country&quot; # overwrite the original with new ones names(df) &lt;- str_replace_all(names(df), &quot;\\\\s&quot;, &quot;_&quot;) # autoprint updated df df 11.3 Data Completeness NA It is always important to check if the information of each row is complete. We first define a function check_num_NA(), which checks the number of NA values of the input x object We then map() this self-defined function to each column of the df # define a function check_num_NA &lt;- function(x) { x %&gt;% is.na %&gt;% sum } # map the function to the data frame df %&gt;% map_df(check_num_NA) ## Alternatively, you can write this way: map_df(df, check_num_NA) From the above results, we see that Sex column has 26 cases of NA values. We can check rows with NA’s by filtering out these cases: df %&gt;% filter(is.na(Sex)) 11.4 Data Preprocessing df %&gt;% filter(Birth_Country == &quot;China&quot;) There are a few important/necessary preprocessing steps here: We lower all character vectors (i.e., normalizing the casing of the letters) We identify duplicate tokens in our dataset We create two new columns: Decade Prize_Age nobel_winners &lt;- df %&gt;% mutate_if(is.character, tolower) %&gt;% # lower all character vectors distinct_at(vars(Full_Name, Year, Category), .keep_all = TRUE) %&gt;% mutate(Decade = 10 * (Year %/% 10), Prize_Age = Year - lubridate::year(Birth_Date)) We can check if our new Prize_Age has any NA values? nobel_winners %&gt;% filter(is.na(Prize_Age)) Check Chinese laureates again: nobel_winners %&gt;% filter(Birth_Country == &quot;china&quot;) %&gt;% select(Full_Name, Year, Category) 11.5 Exploratory Analysis In this section, we will see how we can create relevant statistics/graphs for the research questions we are interested in. 11.5.1 Discipline Distribution RQ: How many laureates were there in different disciplines? # statistics nobel_winners %&gt;% count(Category) %&gt;% mutate(percent = round(n/sum(n), 2)) # barplots nobel_winners %&gt;% count(Category) %&gt;% ggplot(aes(x = Category, y = n, fill = Category)) + geom_col() + geom_text(aes(label = n), vjust = -0.25) + labs(title = &quot;No. of Laureates in Different Disciplines&quot;, x = &quot;Category&quot;, y = &quot;N&quot;) + theme(legend.position = &quot;none&quot;) # barplots (ordered) nobel_winners %&gt;% count(Category) %&gt;% ggplot(aes(x = fct_reorder(Category, -n), y = n, fill = Category)) + geom_col() + geom_text(aes(label = n), vjust = -0.25) + labs(title = &quot;No. of Laureates in Different Disciplines&quot;, x = &quot;Category&quot;, y = &quot;N&quot;) + theme(legend.position = &quot;none&quot;) An even more dynamic graph: # barplot (dynamic) library(gganimate) #install.packages(&#39;gganimate&#39;, dependencies = T) df %&gt;% count(Category) %&gt;% mutate(Category = fct_reorder(Category, -n)) %&gt;% ggplot(aes(x = Category, y = n, fill = Category)) + geom_text(aes(label = n), vjust = -0.25) + geom_col() + labs(title = &quot;No. of Laureates in Different Disciplines&quot;, x = &quot;Category&quot;, y = &quot;N&quot;) + theme(legend.position = &quot;none&quot;) + transition_states(Category) + shadow_mark(past = TRUE) 11.5.2 Age Distribution RQ: At what age do poeple receive the Nobel Prize? # statistics summary(nobel_winners$Prize_Age) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 17.00 50.00 60.00 59.45 69.00 90.00 30 psych::describe(nobel_winners$Prize_Age) %&gt;% t X1 vars 1.00000000 n 881.00000000 mean 59.45175936 sd 12.41297890 median 60.00000000 trimmed 59.50496454 mad 13.34340000 min 17.00000000 max 90.00000000 range 73.00000000 skew -0.04907038 kurtosis -0.44875709 se 0.41820389 # histogram nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Prize_Age)) + geom_histogram(color = &quot;white&quot;) # boxplot nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(y = Prize_Age)) + geom_boxplot() 11.5.3 Category x Age Interaction RQ: Does the age of prize-winning vary for different prize categories? # Statistics nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% group_by(Category) %&gt;% summarize(mean_age = mean(Prize_Age), sd_age = sd(Prize_Age)) %&gt;% ungroup %&gt;% arrange(mean_age) # Histogram nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Prize_Age, fill = Category, color = Category)) + geom_histogram(color = &quot;white&quot;) + facet_wrap(~Category) + theme(legend.position = &quot;none&quot;) # Density graphs nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Prize_Age, fill = Category, color = Category)) + geom_density() + facet_wrap(~Category) + theme(legend.position = &quot;none&quot;) # boxplot nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Category, y = Prize_Age, fill = Category)) + geom_boxplot(notch = T) # mean and CI plots nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(Category, Prize_Age, fill = Category)) + stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;white&quot;, color = &quot;black&quot;) + stat_summary(fun.data = function(x) mean_se(x, mult = 1.96), geom = &quot;errorbar&quot;, width = 0.1, color = &quot;grey40&quot;) 11.5.4 Gender Distribution RQ: What is the gender distribution of Nobel winners? # statistics nobel_winners %&gt;% filter(!is.na(Sex)) %&gt;% count(Sex) %&gt;% mutate(percent = round(n/sum(n), 2)) # graphs nobel_winners %&gt;% filter(!is.na(Sex)) %&gt;% ggplot(aes(Sex)) + geom_bar(fill = &quot;white&quot;, color = &quot;black&quot;) 11.5.5 Age x Gender Interaction Did the ages differ greatly for male and female winners? # statistics nobel_winners %&gt;% filter(!is.na(Sex) &amp; !is.na(Prize_Age)) %&gt;% group_by(Sex) %&gt;% summarize(mean_prize_age = mean(Prize_Age), sd_prize_age = sd(Prize_Age), min_prize_age = min(Prize_Age), max_prize_age = max(Prize_Age), N = n()) -&gt; sum_sex_age sum_sex_age As for visualization, we can create boxplots for male and female winners. nobel_winners %&gt;% filter(!is.na(Sex) &amp; !is.na(Prize_Age)) %&gt;% ggplot(aes(Sex, Prize_Age, fill = Sex)) + geom_boxplot(notch = T, color = &quot;grey30&quot;) + scale_fill_manual(values = c(&quot;lightpink&quot;, &quot;lightblue&quot;)) Or alternatively, we can create the mean and confidence interval plots for each sex. # require(Hmisc) # you have to have this for `stat_summary()` nobel_winners %&gt;% filter(!is.na(Sex) &amp; !is.na(Prize_Age)) %&gt;% ggplot(aes(Sex, Prize_Age, color = Sex)) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 2) + stat_summary(fun.data = function(x) mean_se(x, mult = 1.96), geom = &quot;errorbar&quot;, width = 0.1) We can create an informative graph showing not only the mean ages of male and female winners, but also their respective minimum and maxinum ages. sum_sex_age %&gt;% pivot_longer(cols = c(&quot;mean_prize_age&quot;, &quot;min_prize_age&quot;, &quot;max_prize_age&quot;), names_to = &quot;Prize_Age&quot;, values_to = &quot;Age&quot;) %&gt;% mutate(Prize_Age = str_replace_all(Prize_Age, &quot;_prize_age$&quot;, &quot;&quot;)) %&gt;% ggplot(aes(Sex, Age, fill = Prize_Age)) + geom_bar(stat = &quot;identity&quot;, width = 0.8, color = &quot;white&quot;, position = position_dodge(0.8)) 11.6 Exercises Exercise 11.1 Create a subset of nobel_winners, which includes only winners who won the prizes more than once and in more than one category. Exercise 11.2 Please create a data frame of summary statistics, which shows us the distributions of male and female winners in different categories as shown below. Also, please show the number of males and females as well as the proportions for each prize category. (i.e., frequencies and normalizaed frequencies) Exercise 11.3 Please show us the states distribution of the US Nobel Winners. This would give us an idea which state in the United States has the most Nobel winners. For US winners, you can extract their birth states from their birth cities (using regular expressions of course): nobel_winners %&gt;% filter(Birth_Country == &quot;united states of america&quot;) %&gt;% select(Year, Category, Full_Name, Birth_City) Exercise 11.4 Following Exercise 11.3, can you include the full names of states in the above table by adding another column? In the demo-data/US-states-csv, you can find a csv with the mapping between states abbreviations and their full names. The US-states.csv dataset US_states &lt;- read_csv(&quot;demo_data/US-states.csv&quot;) US_states "],["text-analytics-a-start.html", "Chapter 12 Text Analytics: A Start 12.1 Installing quanteda 12.2 Building a corpus from character vector 12.3 Keyword-in-Context (KWIC) 12.4 KWIC with Regular Expressions 12.5 Lexical Density Plot 12.6 Document-Feature Matrix 12.7 Feature Selection 12.8 Top Features 12.9 Wordclouds 12.10 Keyness Analysis 12.11 Flowchart 12.12 Exercises", " Chapter 12 Text Analytics: A Start In this chapter, I will present a quick overview of computational text analytics with R. The most important package for exploratory text analysis is quanteda. As computational text analytics itself is an interesting topic, I would recommend other more advanced courses for those who are interested in this field. This chapter is just to give you a taste of it. 12.1 Installing quanteda There are many packages that are made for computational text analytics in R. You may consult the CRAN Task View: Natural Language Processing for a lot more alternatives. To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. library(quanteda) library(readtext) library(tidytext) library(dplyr) packageVersion(&quot;quanteda&quot;) [1] &#39;3.0.0&#39; 12.2 Building a corpus from character vector To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. data_corpus_inaugural Corpus consisting of 59 documents and 4 docvars. 1789-Washington : &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot; 1793-Washington : &quot;Fellow citizens, I am again called upon by the voice of my c...&quot; 1797-Adams : &quot;When it was first perceived, in early times, that no middle ...&quot; 1801-Jefferson : &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot; 1805-Jefferson : &quot;Proceeding, fellow citizens, to that qualification which the...&quot; 1809-Madison : &quot;Unwilling to depart from examples of the most revered author...&quot; [ reached max_ndoc ... 53 more documents ] class(data_corpus_inaugural) [1] &quot;corpus&quot; &quot;character&quot; We create a corpus() object with the pre-loaded corpus in quanteda– data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) # save the `corpus` to a short obj name summary(corp_us) After the corpus is loaded, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() Exercise 12.1 Could you reproduce the above line plot and add information of President to the plot as labels of the dots? Hints: Please check ggplot2::geom_text() or more advanced one, ggrepel::geom_text_repel() 12.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or concordances, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examining how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. Please note that kwic(), when taking a corpus object as the argument, will automatically tokenize the corpus data and do the keyword-in-context search on a word basis. In other words, the pattern you look for cannot be a linguistic pattern across several words. quanteda can take care of Chinese word segmentation but with a limited capacity. 12.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. corp_us_tokens &lt;- tokens(corp_us) kwic(corp_us_tokens, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us_tokens, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. Exercise 12.2 Please create a bar plot, showing the number of uses of the word country in each president’s address. Please include different variants of the word, e.g., countries, Countries, Country, in your kwic() search. 12.5 Lexical Density Plot Plotting a kwic object produces a lexical dispersion plot. which allows us to visualize the occurrences of particular terms throughout the text. require(quanteda.textplots) corp_us_tokens %&gt;% tokens_subset(Year &gt; 1949) %&gt;% kwic(pattern = &quot;american&quot;) %&gt;% textplot_xray() corp_us_subset &lt;- corp_us_tokens %&gt;% tokens_subset(Year &gt; 1949) textplot_xray(kwic(corp_us_subset, pattern = &quot;american&quot;), kwic(corp_us_subset, pattern = &quot;people&quot;)) 12.6 Document-Feature Matrix Another important object class is defined in quanteda: the dfm. It stands for Document-Feature-Matrix. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterizing the documents. The cells in the matrix often refer to the co-occurrence statistics between each document and the feature. We can use dfm() to create the dfm of a corpus. corp_us_dfm &lt;- corp_us %&gt;% tokens %&gt;% dfm corp_us_dfm[1:10, 1:10] Document-feature matrix of: 10 documents, 10 features (43.00% sparse) and 4 docvars. features docs fellow-citizens of the senate and house representatives : 1789-Washington 1 71 116 1 48 2 2 1 1793-Washington 0 11 13 0 2 0 0 1 1797-Adams 3 140 163 1 130 0 2 0 1801-Jefferson 2 104 130 0 81 0 0 1 1805-Jefferson 0 101 143 0 93 0 0 0 1809-Madison 1 69 104 0 43 0 0 0 features docs among vicissitudes 1789-Washington 1 1 1793-Washington 0 0 1797-Adams 4 0 1801-Jefferson 1 0 1805-Jefferson 7 0 1809-Madison 0 0 [ reached max_ndoc ... 4 more documents ] We can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and 12.7 Feature Selection A dfm may not be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered with respect to the features of the dfm: The granularity of the features The informativeness of the features The distributional properties of the features Not only does dfm() provide many arguments for users to specify conditions for features selection; in quanteda, we can also apply dfm_trim() to select important features for later analysis. corp_dfm_trimmed &lt;- corp_us %&gt;% tokens(remove_punct = T, remove_numbers = T, remove_symbols = T) %&gt;% dfm %&gt;% dfm_remove(stopwords(&quot;en&quot;)) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us) - 1, docfreq_type = &quot;count&quot;) 12.8 Top Features With a dfm, we can check important features from the corpus. topfeatures(corp_dfm_trimmed, 10) people government us can must upon great 584 564 505 487 376 371 344 may states world 343 334 319 12.9 Wordclouds With a dfm, we can visualize important words in the corpus with a Word Cloud. It is a novel but intuitive visual representation of text data. It allows us to quickly perceive the most prominent words from a large collection of texts. corp_dfm_trimmed %&gt;% textplot_wordcloud(min_count = 50, random_order = FALSE, rotation = 0.25, color = RColorBrewer::brewer.pal(8, &quot;Dark2&quot;)) We can also compare word clouds for different subsets of the corpus: corpus_subset(corp_us, President %in% c(&quot;Obama&quot;, &quot;Trump&quot;, &quot;Clinton&quot;)) %&gt;% tokens(remove_punct = T, remove_numbers = T, remove_symbols = T) %&gt;% tokens_group(groups = President) %&gt;% dfm() %&gt;% dfm_remove(stopwords(&quot;en&quot;)) %&gt;% dfm_trim(min_termfreq = 5, termfreq_type = &quot;count&quot;) %&gt;% textplot_wordcloud(comparison = TRUE) 12.10 Keyness Analysis 12.11 Flowchart Finally, Figure 12.1 below provides a summary flowchart for computatutional text analytics in R. Figure 12.1: Computational Text Processing Flowchart in R 12.12 Exercises In the following exercise, please use the dataset demo_data/TW_President.tar.gz, which is a text collection of the inaugural speeches of Taiwan Presidents (till 2016). You may load the entire text collection as a corpus object using the following code: require(readtext) require(quanteda) corp_tw &lt;- readtext(&quot;demo_data/TW_President.tar.gz&quot;) %&gt;% corpus Exercise 12.3 Please create a data frame, which includes the metadata information for each text. You may start with a data frame you get from summary(corp_tw) and then create two additional columns—President and Year, which can be extracted from the text filenames in the Text column. Hint: tidyr::extract() summary(corp_tw) %&gt;% as_tibble After you create the metadata DF, please assign it to the docvars(corp_tw) for later analysis. Exercise 12.4 Please create a lexical density plot for the use of “台灣” in all presidents’ texts. Exercise 12.5 Please create a word cloud of the entire corpus corp_tw. In the word cloud, please remove punctuations, numbers, and symbols. The word cloud has to only include words whose frequency &gt;= 20. Exercise 12.6 Create word clouds showing the comparison of President Tsai (CAYANGWEN), Ma (MAYANGJIU), and Shuibian Chen (CHENSHUIBIAN). Exercise 12.7 Please create a keyness plot, showing the preferred words used by President Tsai (CAYANGWEN) vs. President Ma (MAYANGJIU). "],["python-fundamentals.html", "Chapter 13 Python Fundamentals 13.1 Set up Environment 13.2 Conda Environment 13.3 Data Type 13.4 Data Structure 13.5 String 13.6 Control Structure 13.7 Function 13.8 List Comprehension 13.9 Python Scripts 13.10 Modules 13.11 Input/Output", " Chapter 13 Python Fundamentals This unit covers Python fundamentals. All the codes here are Python codes. 13.1 Set up Environment Install Anaconda Install Jupyter Notebook/Lab (See Jupyter Notebook installation documentation) Run the codes in notebooks 13.2 Conda Environment We can create a conda environment using: $ conda create --name XXX Please specify the conda environment name XXX on your own. Similarly, when you add the self-defined conda environment to the notebook kernel list: $ python -m ipykernel install --user --name=XXX You need to specify the conda environment name XXX. There are several important things here: You need to install the relevant modules AFTER you activate the conda environment in the terminal. You need to add the kernel name with python -m ipykernel install --user --name=XXX within the conda enviroment as well. In other words, you need to install the module ipykernel in the target conda environment as well. After a few trial-and-errors, I think the best environment setting is that you only add the kernel name (conda environment) to ipykernel within the conda environment. Do not add the conda environment again in your base python environment. What’s even better is to install jupyter in your conda environment (python-notes) and run your notebook from this python-notes as well. 13.3 Data Type String Numbers (Integers and Floats) Data Type Conversion Input a = &#39;cats&#39; b = &#39;dogs&#39; print(a + b) catsdogs a = 12 b = 13 print(a+b) 25 13.4 Data Structure List Tuple Dictionary vocab = [&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;] word_pos = ((&#39;cat&#39;,&#39;n&#39;),(&#39;dog&#39;,&#39;n&#39;),(&#39;bark&#39;,&#39;v&#39;)) word_freq = {&#39;cat&#39;: 3, &#39;dog&#39;: 1, &#39;bird&#39;: 5} print(vocab) [&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;] print(type(vocab)) &lt;class &#39;list&#39;&gt; print(type(word_pos)) &lt;class &#39;tuple&#39;&gt; print(type(word_freq)) &lt;class &#39;dict&#39;&gt; List and Tuple look similar but they differ in one important aspect: List is mutable while Tuple is Immutable. That is, when a List is created, particular elements of it can be reassigned. Along with this, the entire list can be reassigned. Elements and slices of elements can be deleted from the list. But these changes will not be possible for a Tuple. 13.5 String vocab1 = [&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;] vocab2 = (&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;) vocab1[0] = &#39;human&#39; print(vocab1) [&#39;human&#39;, &#39;dog&#39;, &#39;bird&#39;] vocab2[0] = &#39;human&#39; Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: &#39;tuple&#39; object does not support item assignment Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; w = &#39;wonderful&#39; type(w) &lt;class &#39;str&#39;&gt; w[:3] &#39;won&#39; In Python, a String functions as a List: &#39;o&#39; in w True w2 = &#39;book&#39; w + w2 &#39;wonderfulbook&#39; &#39; &#39;.join(w) &#39;w o n d e r f u l&#39; Useful functions for String: sent = &#39; This is a sentence example with leading/trailing spaces. &#39; sent.capitalize() &#39; this is a sentence example with leading/trailing spaces. &#39; sent.title() &#39; This Is A Sentence Example With Leading/Trailing Spaces. &#39; sent.upper() &#39; THIS IS A SENTENCE EXAMPLE WITH LEADING/TRAILING SPACES. &#39; sent.lower() &#39; this is a sentence example with leading/trailing spaces. &#39; sent.rstrip() &#39; This is a sentence example with leading/trailing spaces.&#39; sent.lstrip() &#39;This is a sentence example with leading/trailing spaces. &#39; sent.strip() &#39;This is a sentence example with leading/trailing spaces.&#39; sent.find(&#39;is&#39;) 5 sent.replace(&#39;is&#39;,&#39;was&#39;) &#39; Thwas was a sentence example with leading/trailing spaces. &#39; String formatting nwords = 50 textid = &#39;diary&#39; &#39;%s has %d words&#39; % (textid.upper(), nwords) &#39;DIARY has 50 words&#39; 13.6 Control Structure Iteration (for-loop) for w in vocab: print(&quot;_&quot; + w + &quot;_&quot;) _cat_ _dog_ _bird_ Condition (if-else) for w in vocab: if(w[0]==&quot;c&quot;): print(w) cat 13.7 Function def greet(name): print(&#39;Hello, &#39; + name + &#39;, how are you doing!&#39;) greet(name=&#39;Alvin&#39;) Hello, Alvin, how are you doing! greet(name=&#39;Charlie&#39;) Hello, Charlie, how are you doing! 13.8 List Comprehension [len(w) for w in vocab] [3, 3, 4] 13.9 Python Scripts Depending on the editor you use, you may have two types of Python script files: *.py: A simple python script file *.ipynb: A Jupyter Notebook file which needs to be run in Jupyter Lab/Notebook 13.10 Modules $ pip install PACKAGE_NAME Import packages/libraries import re import os 13.11 Input/Output with open(&#39;temp.txt&#39;, &#39;w&#39;) as f: f.write(&#39;hello world!\\n&#39;+&#39;This is a sentence.&#39;) 32 with open(&#39;temp.txt&#39;, &#39;r&#39;) as f: texts = [l for l in f.readlines()] print(texts) [&#39;hello world!\\n&#39;, &#39;This is a sentence.&#39;] rm temp.txt File and Directory Operation import os os.getcwdb() b&#39;/Users/Alvin/Dropbox/NTNU/Programming_Linguistics/Programming_Linguistics_bookdown&#39; os.unlink() os.rename() os.chdir() os.listdir() os.getwd() os.mkdir() os.rmdir os.path.exists() os.path.isfile() os.path.isdir() "],["regular-expression.html", "Chapter 14 Regular Expression 14.1 Comparison of R and Python 14.2 Structure of Regular Expression Usage 14.3 Special Falgs/Settings for Regular Expressions 14.4 Regular Expression in Python 14.5 Text Munging 14.6 References", " Chapter 14 Regular Expression 14.1 Comparison of R and Python R Python str_extract() re.search() str_extract_all() re.findall() str_match_all() re.finditer() str_replace_all() re.sub() str_split() re.split() ? re.subn() ? re.match() str_detect() ? str_subset() ? The above table shows the similarities and differences in terms of the regular expression functions in Python and R. They are more or less similar. These mappings can be helpful for R users to understand the re in Python. 14.2 Structure of Regular Expression Usage 14.2.1 re.search() Import the regex module with import re Create a Regex object by compiling a regular expression pattern (re.compile()). Remember to use a raw string. Use the pattern for search (re.search()) by passing the string you want to search into the Regex object’s search method. This returns a Match object. Call the Match object’s group() method to return a string of the actual matched text. import re text_to_search = &#39;&#39;&#39; abcdefghijklmnopqurtuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ 1234567890 Ha HaHa MetaCharacters (Need to be escaped): . ^ $ * + ? { } [ ] \\ | ( ) coreyms.com 321-555-4321 123.555.1234 123*555*1234 800-555-1234 900-555-1234 Mr. Schafer Mr Smith Ms Davis Mrs. Robinson Mr. T &#39;&#39;&#39; sentence = &#39;Start a sentence and then bring it to an end&#39; pattern = re.compile(r&#39;\\d{3}-\\d{3}-\\d{4}&#39;, re.I) 14.2.2 re.findall() While search() will return a Match object of the first matched text in the searched string, the findall() method will return the strings of every match in the searched string. (re.findall() will not return a Match object but a list of strings–*as long as there are no groups in the regular expression.) ## perform a search matches= pattern.findall(text_to_search) matches [&#39;321-555-4321&#39;, &#39;800-555-1234&#39;, &#39;900-555-1234&#39;] If there are groups in the regular expressions, then re.findall() will return a list of tuples. pattern2 = re.compile(r&#39;(\\d{3})-(\\d{3})-(\\d{4})&#39;, re.I) pattern2.findall(text_to_search) [(&#39;321&#39;, &#39;555&#39;, &#39;4321&#39;), (&#39;800&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;900&#39;, &#39;555&#39;, &#39;1234&#39;)] 14.2.3 re.finditer() ## find all matches matches = pattern.finditer(text_to_search) if matches: for m in matches: print(&quot;%02d-%02d: %s&quot; % (m.start(), m.end(), m.group())) 151-163: 321-555-4321 190-202: 800-555-1234 203-215: 900-555-1234 14.3 Special Falgs/Settings for Regular Expressions re.IGNORECASE: case-insensitive for pattern matching re.DOTALL: to allow the wildcard * to match linebreaks re.VERBOSE: to create complex regular expressions with multilines and comments (#) pattern3 = re.compile(r&#39;&#39;&#39; (\\d{3}) # area code - # delimiter (\\d{3}) # first 3 digits - # delimiter (\\d{4}) # last 4 digits &#39;&#39;&#39;, re.VERBOSE) pattern3.findall(text_to_search) [(&#39;321&#39;, &#39;555&#39;, &#39;4321&#39;), (&#39;800&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;900&#39;, &#39;555&#39;, &#39;1234&#39;)] Exercise 14.1 With the text_to_search, how to create a more complete regular expression to extract all the phone numbers, including those numbers that have other delimiters (e.g., *) [(&#39;321&#39;, &#39;555&#39;, &#39;4321&#39;), (&#39;123&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;123&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;800&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;900&#39;, &#39;555&#39;, &#39;1234&#39;)] 14.4 Regular Expression in Python 14.4.1 Raw String Notation Raw string notation (r'text') keeps regular expressions sane. Without it, every backslash ('') in a regular expression would have to be prefixed with another one to escape it. For example, the two following lines of code are functionally identical: 14.4.2 Find all matches re.findall(): matches all occurrences of a pattern, not just the first one as re.search() does. re.finditer(): If one wants more information about all matches of a pattern than the matched text, re.finditer() is useful as it provides match objects instead of strings. 14.4.3 group() vs. groups() group(): by default, returns the whole match of the pattern groups(): by default, returns all capturing groups m = re.match(&quot;a(.)(.)&quot;,&quot;abcedf&quot;) print(m.group(0)) # return the whole match abc print(m.group()) # return the whole match, same as above abc print(m.groups()) # return each capturing group match (&#39;b&#39;, &#39;c&#39;) print(m.group(1)) # return first capturing gorup match b 14.4.4 string format validation valid = re.compile(r&quot;^[a-z]+@[a-z]+\\.[a-z]{3}$&quot;) print(valid.match(&#39;alvin@ntnu.edu&#39;)) &lt;re.Match object; span=(0, 14), match=&#39;alvin@ntnu.edu&#39;&gt; print(valid.match(&#39;alvin123@ntnu.edu&#39;)) None print(valid.match(&#39;alvin@ntnu.homeschool&#39;)) None 14.4.5 re.match() vs. re.search() Python offers two different primitive operations based on regular expressions: re.match() checks for a match only at the beginning of the string re.search() checks for a match anywhere in the string (this is what Perl does by default). print(re.match(&quot;c&quot;, &quot;abcdef&quot;)) # No match None print(re.search(&quot;^c&quot;, &quot;abcdef&quot;)) # No match, same as above None print(re.search(&quot;c&quot;, &quot;abcdef&quot;)) # Match &lt;re.Match object; span=(2, 3), match=&#39;c&#39;&gt; re.match always matches at the beginning of the input string even if it is in the MULTILINE mode. re.search however, when in MULTILINE mode, is able to search at the beginning of every line if used in combination with ^. print(re.match(&#39;X&#39;, &#39;A\\nB\\nX&#39;, re.MULTILINE)) # No match None print(re.search(&#39;^X&#39;, &#39;A\\nB\\nX&#39;, re.MULTILINE)) # Match &lt;re.Match object; span=(4, 5), match=&#39;X&#39;&gt; print(re.search(&#39;^X&#39;, &#39;A\\nB\\nX&#39;)) # No match None 14.4.6 re.split() text = &quot;&quot;&quot;Ross McFluff: 834.345.1254 155 Elm Street Ronald Heathmore: 892.345.3428 436 Finley Avenue Frank Burger: 925.541.7625 662 South Dogwood Way Heather Albrecht: 548.326.4584 919 Park Place&quot;&quot;&quot; # split text into lines re.split(r&#39;\\n&#39;,text) [&#39;Ross McFluff: 834.345.1254 155 Elm Street&#39;, &#39;&#39;, &#39;Ronald Heathmore: 892.345.3428 436 Finley Avenue&#39;, &#39;Frank Burger: 925.541.7625 662 South Dogwood Way&#39;, &#39;&#39;, &#39;&#39;, &#39;Heather Albrecht: 548.326.4584 919 Park Place&#39;] re.split(r&#39;\\n+&#39;, text) [&#39;Ross McFluff: 834.345.1254 155 Elm Street&#39;, &#39;Ronald Heathmore: 892.345.3428 436 Finley Avenue&#39;, &#39;Frank Burger: 925.541.7625 662 South Dogwood Way&#39;, &#39;Heather Albrecht: 548.326.4584 919 Park Place&#39;] entries = re.split(r&#39;\\n+&#39;, text) [re.split(r&#39;\\s&#39;, entry) for entry in entries] [[&#39;Ross&#39;, &#39;McFluff:&#39;, &#39;834.345.1254&#39;, &#39;155&#39;, &#39;Elm&#39;, &#39;Street&#39;], [&#39;Ronald&#39;, &#39;Heathmore:&#39;, &#39;892.345.3428&#39;, &#39;436&#39;, &#39;Finley&#39;, &#39;Avenue&#39;], [&#39;Frank&#39;, &#39;Burger:&#39;, &#39;925.541.7625&#39;, &#39;662&#39;, &#39;South&#39;, &#39;Dogwood&#39;, &#39;Way&#39;], [&#39;Heather&#39;, &#39;Albrecht:&#39;, &#39;548.326.4584&#39;, &#39;919&#39;, &#39;Park&#39;, &#39;Place&#39;]] [re.split(r&#39;:?\\s&#39;, entry, maxsplit=3) for entry in entries] [[&#39;Ross&#39;, &#39;McFluff&#39;, &#39;834.345.1254&#39;, &#39;155 Elm Street&#39;], [&#39;Ronald&#39;, &#39;Heathmore&#39;, &#39;892.345.3428&#39;, &#39;436 Finley Avenue&#39;], [&#39;Frank&#39;, &#39;Burger&#39;, &#39;925.541.7625&#39;, &#39;662 South Dogwood Way&#39;], [&#39;Heather&#39;, &#39;Albrecht&#39;, &#39;548.326.4584&#39;, &#39;919 Park Place&#39;]] 14.5 Text Munging re.sub() text = &#39;&#39;&#39;Peter Piper picked a peck of pickled peppers A peck of pickled peppers Peter Piper picked If Peter Piper picked a peck of pickled peppers Where’s the peck of pickled peppers Peter Piper picked?&#39;&#39;&#39; print(re.sub(r&#39;[aeiou]&#39;,&#39;_&#39;, text)) P_t_r P_p_r p_ck_d _ p_ck _f p_ckl_d p_pp_rs A p_ck _f p_ckl_d p_pp_rs P_t_r P_p_r p_ck_d If P_t_r P_p_r p_ck_d _ p_ck _f p_ckl_d p_pp_rs Wh_r_’s th_ p_ck _f p_ckl_d p_pp_rs P_t_r P_p_r p_ck_d? print(re.sub(r&#39;([aeiou])&#39;,r&#39;[\\1]&#39;, text)) P[e]t[e]r P[i]p[e]r p[i]ck[e]d [a] p[e]ck [o]f p[i]ckl[e]d p[e]pp[e]rs A p[e]ck [o]f p[i]ckl[e]d p[e]pp[e]rs P[e]t[e]r P[i]p[e]r p[i]ck[e]d If P[e]t[e]r P[i]p[e]r p[i]ck[e]d [a] p[e]ck [o]f p[i]ckl[e]d p[e]pp[e]rs Wh[e]r[e]’s th[e] p[e]ck [o]f p[i]ckl[e]d p[e]pp[e]rs P[e]t[e]r P[i]p[e]r p[i]ck[e]d? American_dates = [&quot;7/31/1976&quot;, &quot;02.15.1970&quot;, &quot;11-31-1986&quot;, &quot;04/01.2020&quot;] print(American_dates) [&#39;7/31/1976&#39;, &#39;02.15.1970&#39;, &#39;11-31-1986&#39;, &#39;04/01.2020&#39;] print([re.sub(r&#39;(\\d+)(\\D)(\\d+)(\\D)(\\d+)&#39;, r&#39;\\3\\2\\1\\4\\5&#39;, date) for date in American_dates]) [&#39;31/7/1976&#39;, &#39;15.02.1970&#39;, &#39;31-11-1986&#39;, &#39;01/04.2020&#39;] In re.sub(repl, string), the repl argument can be a function. If repl is a function, it is called for every non-overlapping occurrence of pattern. The function takes a single match object argument, and returns the replacement string. s = &quot;This is a simple sentence.&quot; pat_vowels = re.compile(r&#39;[aeiou]&#39;) def replaceVowels(m): c = m.group(0) c2 = &quot;&quot; if c in &quot;ie&quot;: c2 = &quot;F&quot; else: c2 = &quot;B&quot; return c2 pat_vowels.sub(replaceVowels, s) &#39;ThFs Fs B sFmplF sFntFncF.&#39; Exercise 14.2 Create a small program to extract both emails and phone numbers from the texts on this faculty page: Department o English, NTNU. All Phone Numbers: [&#39;02-7749-1801&#39;, &#39;02-7749-1772&#39;, &#39;02-7749-1775&#39;, &#39;02-7749-1783&#39;, &#39;02-7749-1767&#39;, &#39;02-7749-1757&#39;, &#39;02-7749-1781&#39;, &#39;02-7749-1777&#39;, &#39;02-7749-1773&#39;, &#39;02-7749-1759&#39;, &#39;02-7749-1768&#39;, &#39;02-7749-1822&#39;, &#39;02-7749-1760&#39;, &#39;02-7749-1764&#39;, &#39;02-7749-1769&#39;, &#39;02-7749-1817&#39;, &#39;02-7749-1756&#39;, &#39;02-7749-1788&#39;, &#39;02-7749-1758&#39;, &#39;02-7749-1754&#39;, &#39;02-7749-1821&#39;, &#39;02-7749-1787&#39;, &#39;02-7749-1776&#39;, &#39;02-7749-1770&#39;, &#39;02-7749-1786&#39;, &#39;02-7749-1789&#39;, &#39;02-7749-1816&#39;, &#39;02-7749-1774&#39;, &#39;02-7749-1765&#39;, &#39;02-7749-1819&#39;, &#39;02-7749-1778&#39;, &#39;02-7749-1761&#39;, &#39;02-7749-1541&#39;, &#39;02-7749-1761&#39;, &#39;02-7749-1823&#39;, &#39;02-7749-1540&#39;, &#39;02-7749-1763&#39;, &#39;02-7749-1762&#39;, &#39;02-7749-1785&#39;, &#39;02-7749-1820&#39;, &#39;02-7749-1766&#39;, &#39;02-7749-1782&#39;, &#39;02-7749-1811&#39;, &#39;02-7749-1821&#39;, &#39;02-7749-1779&#39;, &#39;02-7749-1762&#39;, &#39;02-7749-1784&#39;, &#39;02-7749-1755&#39;, &#39;02-7749-1800&#39;, &#39;02-2363-4793&#39;] All Emails: [&#39;chunyin@gapps.ntnu.edu.tw&#39;, &#39;mhchang@ntnu.edu.tw&#39;, &#39;clchern@ntnu.edu.tw&#39;, &#39;t22035@ntnu.edu.tw&#39;, &#39;joanchang@ntnu.edu.tw&#39;, &#39;hjchen@ntnu.edu.tw&#39;, &#39;tcsu@ntnu.edu.tw&#39;, &#39;t22028@ntnu.edu.tw&#39;, &#39;lip@ntnu.edu.tw&#39;, &#39;ting@ntnu.edu.tw&#39;, &#39;hclee@ntnu.edu.tw&#39;, &#39;hslin@ntnu.edu.tw&#39;, &#39;chyhuang@ntnu.edu.tw&#39;, &#39;profgood@ntnu.edu.tw&#39;, &#39;cclin@ntnu.edu.tw&#39;, &#39;iriswu@ntnu.edu.tw&#39;, &#39;yeutingliu@gapps.ntnu.edu.tw&#39;, &#39;ioana.luca@ntnu.edu.tw&#39;, &#39;lindsey@ntnu.edu.tw&#39;, &#39;jprystash@ntnu.edu.tw&#39;, &#39;brianyeh@ntnu.edu.tw&#39;, &#39;lijeni@ntnu.edu.tw&#39;, &#39;ykhsu@ntnu.edu.tw&#39;, &#39;ycshao@ntnu.edu.tw&#39;, &#39;t22045@ntnu.edu.tw&#39;, &#39;jjwu@ntnu.edu.tw&#39;, &#39;mlhsieh@ntnu.edu.tw&#39;, &#39;jungsu@ntnu.edu.tw&#39;, &#39;hsysu@ntnu.edu.tw&#39;, &#39;t22001@ntnu.edu.tw&#39;, &#39;shiaohui@ntnu.edu.tw&#39;, &#39;jjtseng@gapps.ntnu.edu.tw&#39;, &#39;peichinchang@ntnu.edu.tw&#39;, &#39;aaron.c.deveson@ntnu.edu.tw&#39;, &#39;t22050@ntnu.edu.tw&#39;, &#39;lihsin@ntnu.edu.tw&#39;, &#39;wanghc@gapps.ntnu.edu.tw&#39;, &#39;hllin@ntnu.edu.tw&#39;, &#39;jiaqiwu8@ntnu.edu.tw&#39;, &#39;angelawu@ntnu.edu.tw&#39;, &#39;yichien@ntnu.edu.tw&#39;, &#39;gfsayang@ntnu.edu.tw&#39;, &#39;alvinchen@ntnu.edu.tw&#39;, &#39;yuchentai@gapps.ntnu.edu.tw&#39;, &#39;fwkung@ntnu.edu.tw&#39;, &#39;t22040@ntnu.edu.tw&#39;, &#39;curran@ntnu.edu.tw&#39;, &#39;english@ntnu.edu.tw&#39;] 14.6 References Python regular expression cheatsheet Python official regular expression documentation Friedl, Jeffrey. Mastering Regular Expressions. 3rd ed., O’Reilly Media, 2009. A good graphic interface to try out regular expressions: pythex.org "],["pandas.html", "Chapter 15 Pandas 15.1 Libraries 15.2 Importing/Exporting Data 15.3 Inspecting Data Frame 15.4 Basic Functions 15.5 Subsetting Data Frame 15.6 Exploration 15.7 Join/Combine Data Frames 15.8 Statistics 15.9 Generic Functions 15.10 References", " Chapter 15 Pandas Methods to deal with tabular data These methods are to replicate what dplyr in R is capable of To handle tabular data like data frames, I would still recommend using R instead of Python for beginners. pandas can be intimidating for a lot of beginners. The statsmodels can download R datasets from https://vincentarelbundock.github.io/Rdatasets/datasets.html 15.1 Libraries import pandas as pd import numpy as np import statsmodels.api as sm import matplotlib 15.2 Importing/Exporting Data Importing: pd.read_csv(filename): From a CSV file pd.read_table(filename): From a delimited text file (like TSV) pd.read_excel(filename): From an Excel file pd.read_sql(query, connection_object): Read from a SQL table/database pd.read_json(json_string): Read from a JSON formatted string, URL or file. pd.read_html(url): Parses an html URL, string or file and extracts tables to a list of dataframes pd.read_clipboard(): Takes the contents of your clipboard and passes it to read_table() pd.DataFrame(dict): From a dict, keys for columns names, values for data as lists pd.DataFrame(list of tuples): From a list, which includes the records of each row Exporting: df.to_csv(filename) df.to_excel(filename) df.to_sql(table_name, connection_object) df.to_json(filename) DEMO_DATA_DIR = &#39;/Users/Alvin/GoogleDrive/_MySyncDrive/RepositoryData/data/titanic/&#39; iris = sm.datasets.get_rdataset(&#39;iris&#39;).data titanic = pd.read_csv(DEMO_DATA_DIR+&#39;train.csv&#39;) iris.head() Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa titanic.head() PassengerId Survived Pclass ... Fare Cabin Embarked 0 1 0 3 ... 7.2500 NaN S 1 2 1 1 ... 71.2833 C85 C 2 3 1 3 ... 7.9250 NaN S 3 4 1 1 ... 53.1000 C123 S 4 5 0 3 ... 8.0500 NaN S [5 rows x 12 columns] x= [(1,2,3,4), (5,6,7,8), (9,10,11,12)] pd.DataFrame(x,columns=[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;]) A B C D 0 1 2 3 4 1 5 6 7 8 2 9 10 11 12 x = {&quot;A&quot;:[1,2,3,4], &quot;B&quot;:[5,6,7,8], &quot;C&quot;:[9,10,11,12]} pd.DataFrame(x) A B C 0 1 5 9 1 2 6 10 2 3 7 11 3 4 8 12 When you have data of the columns, use dict; when you have the data of the rows, use list as the source data structures of a data frame. 15.3 Inspecting Data Frame df.head(n): First n rows of the DataFrame df.tail(n): Last n rows of the DataFrame df.shape: Number of rows and columns df.info(): Index, Datatype and Memory information df.describe(): Summary statistics for numerical columns s.value_counts(dropna=False): View unique values and counts df.apply(pd.Series.value_counts): Unique values and counts for all columns df.columns df.index df.dtypes df.set_index('column_name'): Set a column as the index iris.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Sepal.Length 150 non-null float64 1 Sepal.Width 150 non-null float64 2 Petal.Length 150 non-null float64 3 Petal.Width 150 non-null float64 4 Species 150 non-null object dtypes: float64(4), object(1) memory usage: 6.0+ KB iris.describe() Sepal.Length Sepal.Width Petal.Length Petal.Width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 print(iris.shape) (150, 5) iris.head(3) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa iris.tail(3) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica iris[&#39;Species&#39;].value_counts() setosa 50 virginica 50 versicolor 50 Name: Species, dtype: int64 print(iris.columns) Index([&#39;Sepal.Length&#39;, &#39;Sepal.Width&#39;, &#39;Petal.Length&#39;, &#39;Petal.Width&#39;, &#39;Species&#39;], dtype=&#39;object&#39;) print(iris.index) RangeIndex(start=0, stop=150, step=1) print(iris.dtypes) Sepal.Length float64 Sepal.Width float64 Petal.Length float64 Petal.Width float64 Species object dtype: object 15.4 Basic Functions ## DataFrame attributes iris.shape (150, 5) iris.columns Index([&#39;Sepal.Length&#39;, &#39;Sepal.Width&#39;, &#39;Petal.Length&#39;, &#39;Petal.Width&#39;, &#39;Species&#39;], dtype=&#39;object&#39;) iris.index RangeIndex(start=0, stop=150, step=1) iris.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Sepal.Length 150 non-null float64 1 Sepal.Width 150 non-null float64 2 Petal.Length 150 non-null float64 3 Petal.Width 150 non-null float64 4 Species 150 non-null object dtypes: float64(4), object(1) memory usage: 6.0+ KB iris.describe() Sepal.Length Sepal.Width Petal.Length Petal.Width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 iris.dtypes # check column data types Sepal.Length float64 Sepal.Width float64 Petal.Length float64 Petal.Width float64 Species object dtype: object 15.5 Subsetting Data Frame df[col]: Returns column with label col as Series df[[col1, col2]]: Returns columns as a new DataFrame s.iloc[0]: Selection by position s.loc['index_one']: Selection by index df.iloc[0,:]: First row df.iloc[0,0]: First element of first column iris.loc[:5, &#39;Species&#39;] # first six rows of &#39;Species&#39; column 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa 5 setosa Name: Species, dtype: object iris.iloc[:5, 4] # same as above 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name: Species, dtype: object 15.6 Exploration How to perform the key functions provided in R dplyr? dplyr Key Verbs filter() select() mutate() arrange() summarize() group_by() 15.6.1 NA Values Functions to take care of NA values: df.isnull() df.notnull() df.dropna(): Drop rows with null values df.dropna(axis=1): Drop columns with null values df.dropna(axis=1, thresh=n): Drop all columns have less than n non-values df.fillna(x): Replaces all null values with x s.fillna(s.mean()): Replace the null values of a Series with its mean score Quick check of the null values in each column titanic.isnull().sum() PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 titanic.dropna(axis=1, thresh=600) PassengerId Survived Pclass ... Ticket Fare Embarked 0 1 0 3 ... A/5 21171 7.2500 S 1 2 1 1 ... PC 17599 71.2833 C 2 3 1 3 ... STON/O2. 3101282 7.9250 S 3 4 1 1 ... 113803 53.1000 S 4 5 0 3 ... 373450 8.0500 S .. ... ... ... ... ... ... ... 886 887 0 2 ... 211536 13.0000 S 887 888 1 1 ... 112053 30.0000 S 888 889 0 3 ... W./C. 6607 23.4500 S 889 890 1 1 ... 111369 30.0000 C 890 891 0 3 ... 370376 7.7500 Q [891 rows x 11 columns] titanic.notnull().sum() PassengerId 891 Survived 891 Pclass 891 Name 891 Sex 891 Age 714 SibSp 891 Parch 891 Ticket 891 Fare 891 Cabin 204 Embarked 889 dtype: int64 15.6.2 Converting Data Types s.astype(float): Convert a Series into a float type iris.dtypes Sepal.Length float64 Sepal.Width float64 Petal.Length float64 Petal.Width float64 Species object dtype: object iris[&#39;Species&#39;]=iris[&#39;Species&#39;].astype(&#39;category&#39;) iris.dtypes #iris.value_counts(iris[&#39;Species&#39;]).plot.bar() Sepal.Length float64 Sepal.Width float64 Petal.Length float64 Petal.Width float64 Species category dtype: object 15.6.3 Pandas-supported Data Types pandas-dtypes (source) 15.6.4 Transformation s.replace(X, Y) titanic.head() PassengerId Survived Pclass ... Fare Cabin Embarked 0 1 0 3 ... 7.2500 NaN S 1 2 1 1 ... 71.2833 C85 C 2 3 1 3 ... 7.9250 NaN S 3 4 1 1 ... 53.1000 C123 S 4 5 0 3 ... 8.0500 NaN S [5 rows x 12 columns] titanic.value_counts(titanic[&#39;Survived&#39;]).plot.bar() &lt;AxesSubplot:xlabel=&#39;Survived&#39;&gt; titanic.columns Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Name&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;], dtype=&#39;object&#39;) titanic.groupby([&#39;Sex&#39;,&#39;Pclass&#39;]).mean() PassengerId Survived Age SibSp Parch Fare Sex Pclass female 1 469.212766 0.968085 34.611765 0.553191 0.457447 106.125798 2 443.105263 0.921053 28.722973 0.486842 0.605263 21.970121 3 399.729167 0.500000 21.750000 0.895833 0.798611 16.118810 male 1 455.729508 0.368852 41.281386 0.311475 0.278689 67.226127 2 447.962963 0.157407 30.740707 0.342593 0.222222 19.741782 3 455.515850 0.135447 26.507589 0.498559 0.224784 12.661633 titanic[titanic[&#39;Age&#39;]&lt;18].groupby([&#39;Sex&#39;,&#39;Pclass&#39;]).mean() PassengerId Survived Age SibSp Parch Fare Sex Pclass female 1 525.375000 0.875000 14.125000 0.500000 0.875000 104.083337 2 369.250000 1.000000 8.333333 0.583333 1.083333 26.241667 3 374.942857 0.542857 8.428571 1.571429 1.057143 18.727977 male 1 526.500000 1.000000 8.230000 0.500000 2.000000 116.072900 2 527.818182 0.818182 4.757273 0.727273 1.000000 25.659473 3 437.953488 0.232558 9.963256 2.069767 1.000000 22.752523 15.6.5 filter() ## filter iris[iris[&#39;Sepal.Length&#39;]&gt;5] Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 5 5.4 3.9 1.7 0.4 setosa 10 5.4 3.7 1.5 0.2 setosa 14 5.8 4.0 1.2 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica [118 rows x 5 columns] When there are more than one filtering condition, put the conditions in parentheses. iris[(iris[&#39;Sepal.Length&#39;]&gt;4) &amp; (iris[&#39;Sepal.Width&#39;]&gt;5)] Empty DataFrame Columns: [Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species] Index: [] iris.query(&#39;`Sepal.Length`&gt;5&#39;) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 5 5.4 3.9 1.7 0.4 setosa 10 5.4 3.7 1.5 0.2 setosa 14 5.8 4.0 1.2 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica [118 rows x 5 columns] iris[(iris[&#39;Sepal.Length&#39;]&gt;5) &amp; (iris[&#39;Sepal.Width&#39;]&gt;4)] Sepal.Length Sepal.Width Petal.Length Petal.Width Species 15 5.7 4.4 1.5 0.4 setosa 32 5.2 4.1 1.5 0.1 setosa 33 5.5 4.2 1.4 0.2 setosa 15.6.6 arrange() iris.sort_values([&#39;Species&#39;,&#39;Sepal.Length&#39;], ascending=[False,True]) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 106 4.9 2.5 4.5 1.7 virginica 121 5.6 2.8 4.9 2.0 virginica 113 5.7 2.5 5.0 2.0 virginica 101 5.8 2.7 5.1 1.9 virginica 114 5.8 2.8 5.1 2.4 virginica .. ... ... ... ... ... 33 5.5 4.2 1.4 0.2 setosa 36 5.5 3.5 1.3 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa 18 5.7 3.8 1.7 0.3 setosa 14 5.8 4.0 1.2 0.2 setosa [150 rows x 5 columns] 15.6.7 select() ## select iris[[&#39;Sepal.Length&#39;, &#39;Species&#39;]] Sepal.Length Species 0 5.1 setosa 1 4.9 setosa 2 4.7 setosa 3 4.6 setosa 4 5.0 setosa .. ... ... 145 6.7 virginica 146 6.3 virginica 147 6.5 virginica 148 6.2 virginica 149 5.9 virginica [150 rows x 2 columns] ## deselect columns iris.drop([&#39;Sepal.Length&#39;], axis=1).head() Sepal.Width Petal.Length Petal.Width Species 0 3.5 1.4 0.2 setosa 1 3.0 1.4 0.2 setosa 2 3.2 1.3 0.2 setosa 3 3.1 1.5 0.2 setosa 4 3.6 1.4 0.2 setosa iris.filter([&#39;Species&#39;,&#39;Sepal.Length&#39;]) Species Sepal.Length 0 setosa 5.1 1 setosa 4.9 2 setosa 4.7 3 setosa 4.6 4 setosa 5.0 .. ... ... 145 virginica 6.7 146 virginica 6.3 147 virginica 6.5 148 virginica 6.2 149 virginica 5.9 [150 rows x 2 columns] iris[[&#39;Species&#39;,&#39;Sepal.Length&#39;]] Species Sepal.Length 0 setosa 5.1 1 setosa 4.9 2 setosa 4.7 3 setosa 4.6 4 setosa 5.0 .. ... ... 145 virginica 6.7 146 virginica 6.3 147 virginica 6.5 148 virginica 6.2 149 virginica 5.9 [150 rows x 2 columns] ## extract one particular column sepal_length = iris[&#39;Sepal.Length&#39;] type(sepal_length) &lt;class &#39;pandas.core.series.Series&#39;&gt; 15.6.8 mutate() ## mutate iris[&#39;Species_new&#39;] = iris[&#39;Species&#39;].apply(lambda x: len(x)) iris[&#39;Species_initial&#39;] = iris[&#39;Species&#39;].apply(lambda x: x[:2].upper()) iris Sepal.Length Sepal.Width ... Species_new Species_initial 0 5.1 3.5 ... 6 SE 1 4.9 3.0 ... 6 SE 2 4.7 3.2 ... 6 SE 3 4.6 3.1 ... 6 SE 4 5.0 3.6 ... 6 SE .. ... ... ... ... ... 145 6.7 3.0 ... 9 VI 146 6.3 2.5 ... 9 VI 147 6.5 3.0 ... 9 VI 148 6.2 3.4 ... 9 VI 149 5.9 3.0 ... 9 VI [150 rows x 7 columns] ## mutate alternative 2 iris.assign(Specias_initial2 = iris[&#39;Species&#39;].apply(lambda x: x.upper())) Sepal.Length Sepal.Width ... Species_initial Specias_initial2 0 5.1 3.5 ... SE SETOSA 1 4.9 3.0 ... SE SETOSA 2 4.7 3.2 ... SE SETOSA 3 4.6 3.1 ... SE SETOSA 4 5.0 3.6 ... SE SETOSA .. ... ... ... ... ... 145 6.7 3.0 ... VI VIRGINICA 146 6.3 2.5 ... VI VIRGINICA 147 6.5 3.0 ... VI VIRGINICA 148 6.2 3.4 ... VI VIRGINICA 149 5.9 3.0 ... VI VIRGINICA [150 rows x 8 columns] 15.6.9 apply(), mutate_if() df.apply(np.mean): Apply a function to all columns df.apply(np.max,axis=1): Apply a function to each row When apply() functions to the data frame, the axis=1 refers to row mutation and axis=0 refers to column mutation. This is very counter-intuitive for R users. iris.head(10) Sepal.Length Sepal.Width Petal.Length ... Species Species_new Species_initial 0 5.1 3.5 1.4 ... setosa 6 SE 1 4.9 3.0 1.4 ... setosa 6 SE 2 4.7 3.2 1.3 ... setosa 6 SE 3 4.6 3.1 1.5 ... setosa 6 SE 4 5.0 3.6 1.4 ... setosa 6 SE 5 5.4 3.9 1.7 ... setosa 6 SE 6 4.6 3.4 1.4 ... setosa 6 SE 7 5.0 3.4 1.5 ... setosa 6 SE 8 4.4 2.9 1.4 ... setosa 6 SE 9 4.9 3.1 1.5 ... setosa 6 SE [10 rows x 7 columns] iris[[&#39;Sepal.Width&#39;,&#39;Petal.Width&#39;]].apply(np.sum, axis=1).head(10) 0 3.7 1 3.2 2 3.4 3 3.3 4 3.8 5 4.3 6 3.7 7 3.6 8 3.1 9 3.2 dtype: float64 15.6.10 group_by() and summarize() iris.groupby(by=&#39;Species&#39;).mean() Sepal.Length Sepal.Width Petal.Length Petal.Width Species setosa 5.006 3.428 1.462 0.246 versicolor 5.936 2.770 4.260 1.326 virginica 6.588 2.974 5.552 2.026 iris.filter([&#39;Species&#39;,&#39;Sepal.Length&#39;]).groupby(&#39;Species&#39;).agg({&#39;Sepal.Length&#39;:[&#39;mean&#39;,&#39;count&#39;,&#39;std&#39;]}) Sepal.Length mean count std Species setosa 5.006 50 0.352490 versicolor 5.936 50 0.516171 virginica 6.588 50 0.635880 titanic.head() PassengerId Survived Pclass ... Fare Cabin Embarked 0 1 0 3 ... 7.2500 NaN S 1 2 1 1 ... 71.2833 C85 C 2 3 1 3 ... 7.9250 NaN S 3 4 1 1 ... 53.1000 C123 S 4 5 0 3 ... 8.0500 NaN S [5 rows x 12 columns] titanic.groupby([&#39;Pclass&#39;,&#39;Sex&#39;]).agg(np.sum) PassengerId Survived Age SibSp Parch Fare Pclass Sex 1 female 44106 91 2942.00 52 43 9975.8250 male 55599 45 4169.42 38 34 8201.5875 2 female 33676 70 2125.50 37 46 1669.7292 male 48380 17 3043.33 37 24 2132.1125 3 female 57561 72 2218.50 129 115 2321.1086 male 158064 47 6706.42 173 78 4393.5865 titanic.pivot_table(index=[&#39;Pclass&#39;,&#39;Sex&#39;], values=[&#39;Survived&#39;], aggfunc=np.sum) Survived Pclass Sex 1 female 91 male 45 2 female 70 male 17 3 female 72 male 47 15.6.11 rename() iris Sepal.Length Sepal.Width ... Species_new Species_initial 0 5.1 3.5 ... 6 SE 1 4.9 3.0 ... 6 SE 2 4.7 3.2 ... 6 SE 3 4.6 3.1 ... 6 SE 4 5.0 3.6 ... 6 SE .. ... ... ... ... ... 145 6.7 3.0 ... 9 VI 146 6.3 2.5 ... 9 VI 147 6.5 3.0 ... 9 VI 148 6.2 3.4 ... 9 VI 149 5.9 3.0 ... 9 VI [150 rows x 7 columns] iris.columns Index([&#39;Sepal.Length&#39;, &#39;Sepal.Width&#39;, &#39;Petal.Length&#39;, &#39;Petal.Width&#39;, &#39;Species&#39;, &#39;Species_new&#39;, &#39;Species_initial&#39;], dtype=&#39;object&#39;) Selective renaming column names iris = iris.rename(columns={&#39;Sepal.Length&#39;:&#39;SLen&#39;}) iris SLen Sepal.Width Petal.Length ... Species Species_new Species_initial 0 5.1 3.5 1.4 ... setosa 6 SE 1 4.9 3.0 1.4 ... setosa 6 SE 2 4.7 3.2 1.3 ... setosa 6 SE 3 4.6 3.1 1.5 ... setosa 6 SE 4 5.0 3.6 1.4 ... setosa 6 SE .. ... ... ... ... ... ... ... 145 6.7 3.0 5.2 ... virginica 9 VI 146 6.3 2.5 5.0 ... virginica 9 VI 147 6.5 3.0 5.2 ... virginica 9 VI 148 6.2 3.4 5.4 ... virginica 9 VI 149 5.9 3.0 5.1 ... virginica 9 VI [150 rows x 7 columns] Massive renaming column names iris.rename(columns=lambda x: &#39;XX&#39;+x) XXSLen XXSepal.Width ... XXSpecies_new XXSpecies_initial 0 5.1 3.5 ... 6 SE 1 4.9 3.0 ... 6 SE 2 4.7 3.2 ... 6 SE 3 4.6 3.1 ... 6 SE 4 5.0 3.6 ... 6 SE .. ... ... ... ... ... 145 6.7 3.0 ... 9 VI 146 6.3 2.5 ... 9 VI 147 6.5 3.0 ... 9 VI 148 6.2 3.4 ... 9 VI 149 5.9 3.0 ... 9 VI [150 rows x 7 columns] titanic.head(10) PassengerId Survived Pclass ... Fare Cabin Embarked 0 1 0 3 ... 7.2500 NaN S 1 2 1 1 ... 71.2833 C85 C 2 3 1 3 ... 7.9250 NaN S 3 4 1 1 ... 53.1000 C123 S 4 5 0 3 ... 8.0500 NaN S 5 6 0 3 ... 8.4583 NaN Q 6 7 0 1 ... 51.8625 E46 S 7 8 0 3 ... 21.0750 NaN S 8 9 1 3 ... 11.1333 NaN S 9 10 1 2 ... 30.0708 NaN C [10 rows x 12 columns] titanic.set_index(&#39;Name&#39;).rename(index=lambda x:x.replace(&#39; &#39;,&quot;_&quot;).upper()) PassengerId ... Embarked Name ... BRAUND,_MR._OWEN_HARRIS 1 ... S CUMINGS,_MRS._JOHN_BRADLEY_(FLORENCE_BRIGGS_THA... 2 ... C HEIKKINEN,_MISS._LAINA 3 ... S FUTRELLE,_MRS._JACQUES_HEATH_(LILY_MAY_PEEL) 4 ... S ALLEN,_MR._WILLIAM_HENRY 5 ... S ... ... ... ... MONTVILA,_REV._JUOZAS 887 ... S GRAHAM,_MISS._MARGARET_EDITH 888 ... S JOHNSTON,_MISS._CATHERINE_HELEN_&quot;CARRIE&quot; 889 ... S BEHR,_MR._KARL_HOWELL 890 ... C DOOLEY,_MR._PATRICK 891 ... Q [891 rows x 11 columns] 15.7 Join/Combine Data Frames df1.append(df2): Add the rows in df1 to the end of df2 (columns should be identical) (rbind() in R) pd.concat([df1, df2],axis=1): Add the columns in df1 to the end of df2 (rows should be identical) (cbind() in R) df1.join(df2,on=col1,how='inner'): SQL-style join the columns in df1 with the columns on df2 where the rows for col have identical values. ‘how’ can be one of ‘left,’ ‘right,’ ‘outer,’ ‘inner’ 15.8 Statistics df.describe(): Summary statistics for numerical columns df.mean(): Returns the mean of all columns df.corr(): Returns the correlation between columns in a DataFrame df.count(): Returns the number of non-null values in each DataFrame column df.max(): Returns the highest value in each column df.min(): Returns the lowest value in each column df.median(): Returns the median of each column df.std(): Returns the standard deviation of each column titanic.count() PassengerId 891 Survived 891 Pclass 891 Name 891 Sex 891 Age 714 SibSp 891 Parch 891 Ticket 891 Fare 891 Cabin 204 Embarked 889 dtype: int64 titanic.median() PassengerId 446.0000 Survived 0.0000 Pclass 3.0000 Age 28.0000 SibSp 0.0000 Parch 0.0000 Fare 14.4542 dtype: float64 15.9 Generic Functions pandas.pivot_table() pandas.crosstab() pandas.cut() pandas.qcut() pandas.merge() pandas.get_dummies() 15.10 References Python for Data Analysis Python for Data Analysis GitHub How to get sample datasets in Python "],["nltk.html", "Chapter 16 NLTK 16.1 Installation 16.2 Corpora Data 16.3 WordNet 16.4 Discovering Word Collocations 16.5 Tokenization 16.6 Chinese Word Segmentation", " Chapter 16 NLTK The almighty nltk package! 16.1 Installation Install package in the terminal !pip install nltk Download nltk data in python import nltk nltk.download(&#39;all&#39;, halt_on_error=False) import nltk # nltk.download() The complete collection of the nltk.corpus is huge. You probably don’t need all of the corpora data. You can use nltk.download() to initialize a User Window for installation of specific datasets. 16.2 Corpora Data The package includes a lot of pre-loaded corpora datasets The default nltk_data directory is in /Users/YOUT_NAME/nltk_data/ Selective Examples Brown Corpus Reuters Corpus WordNet from nltk.corpus import gutenberg, brown, reuters # brown corpus ## Categories (topics?) print(&#39;Brown Corpus Total Categories: &#39;, len(brown.categories())) Brown Corpus Total Categories: 15 print(&#39;Categories List: &#39;, brown.categories()) Categories List: [&#39;adventure&#39;, &#39;belles_lettres&#39;, &#39;editorial&#39;, &#39;fiction&#39;, &#39;government&#39;, &#39;hobbies&#39;, &#39;humor&#39;, &#39;learned&#39;, &#39;lore&#39;, &#39;mystery&#39;, &#39;news&#39;, &#39;religion&#39;, &#39;reviews&#39;, &#39;romance&#39;, &#39;science_fiction&#39;] # Sentences print(brown.sents()[0]) ## first sentence [&#39;The&#39;, &#39;Fulton&#39;, &#39;County&#39;, &#39;Grand&#39;, &#39;Jury&#39;, &#39;said&#39;, &#39;Friday&#39;, &#39;an&#39;, &#39;investigation&#39;, &#39;of&#39;, &quot;Atlanta&#39;s&quot;, &#39;recent&#39;, &#39;primary&#39;, &#39;election&#39;, &#39;produced&#39;, &#39;``&#39;, &#39;no&#39;, &#39;evidence&#39;, &quot;&#39;&#39;&quot;, &#39;that&#39;, &#39;any&#39;, &#39;irregularities&#39;, &#39;took&#39;, &#39;place&#39;, &#39;.&#39;] print(brown.sents(categories=&#39;fiction&#39;)) ## first sentence for fiction texts [[&#39;Thirty-three&#39;], [&#39;Scotty&#39;, &#39;did&#39;, &#39;not&#39;, &#39;go&#39;, &#39;back&#39;, &#39;to&#39;, &#39;school&#39;, &#39;.&#39;], ...] ## Tagged Sentences print(brown.tagged_sents()[0]) [(&#39;The&#39;, &#39;AT&#39;), (&#39;Fulton&#39;, &#39;NP-TL&#39;), (&#39;County&#39;, &#39;NN-TL&#39;), (&#39;Grand&#39;, &#39;JJ-TL&#39;), (&#39;Jury&#39;, &#39;NN-TL&#39;), (&#39;said&#39;, &#39;VBD&#39;), (&#39;Friday&#39;, &#39;NR&#39;), (&#39;an&#39;, &#39;AT&#39;), (&#39;investigation&#39;, &#39;NN&#39;), (&#39;of&#39;, &#39;IN&#39;), (&quot;Atlanta&#39;s&quot;, &#39;NP$&#39;), (&#39;recent&#39;, &#39;JJ&#39;), (&#39;primary&#39;, &#39;NN&#39;), (&#39;election&#39;, &#39;NN&#39;), (&#39;produced&#39;, &#39;VBD&#39;), (&#39;``&#39;, &#39;``&#39;), (&#39;no&#39;, &#39;AT&#39;), (&#39;evidence&#39;, &#39;NN&#39;), (&quot;&#39;&#39;&quot;, &quot;&#39;&#39;&quot;), (&#39;that&#39;, &#39;CS&#39;), (&#39;any&#39;, &#39;DTI&#39;), (&#39;irregularities&#39;, &#39;NNS&#39;), (&#39;took&#39;, &#39;VBD&#39;), (&#39;place&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)] ## Sentence in natural forms sents = brown.sents(categories=&#39;fiction&#39;) [&#39; &#39;.join(sent) for sent in sents[1:5]] [&#39;Scotty did not go back to school .&#39;, &#39;His parents talked seriously and lengthily to their own doctor and to a specialist at the University Hospital -- Mr. McKinley was entitled to a discount for members of his family -- and it was decided it would be best for him to take the remainder of the term off , spend a lot of time in bed and , for the rest , do pretty much as he chose -- provided , of course , he chose to do nothing too exciting or too debilitating .&#39;, &#39;His teacher and his school principal were conferred with and everyone agreed that , if he kept up with a certain amount of work at home , there was little danger of his losing a term .&#39;, &#39;Scotty accepted the decision with indifference and did not enter the arguments .&#39;] ## Get tagged words tagged_words = brown.tagged_words(categories=&#39;fiction&#39;) #print(tagged_words[1]) ## a tuple ## Get all nouns nouns = [(word, tag) for word, tag in tagged_words if any (noun_tag in tag for noun_tag in [&#39;NP&#39;,&#39;NN&#39;])] ## Check first ten nouns nouns[:10] [(&#39;Scotty&#39;, &#39;NP&#39;), (&#39;school&#39;, &#39;NN&#39;), (&#39;parents&#39;, &#39;NNS&#39;), (&#39;doctor&#39;, &#39;NN&#39;), (&#39;specialist&#39;, &#39;NN&#39;), (&#39;University&#39;, &#39;NN-TL&#39;), (&#39;Hospital&#39;, &#39;NN-TL&#39;), (&#39;Mr.&#39;, &#39;NP&#39;), (&#39;McKinley&#39;, &#39;NP&#39;), (&#39;discount&#39;, &#39;NN&#39;)] ## Creating Freq list nouns_freq = nltk.FreqDist([w for w, t in nouns]) sorted(nouns_freq.items(),key=lambda x:x[1], reverse=True)[:20] [(&#39;man&#39;, 111), (&#39;time&#39;, 99), (&#39;men&#39;, 72), (&#39;room&#39;, 63), (&#39;way&#39;, 62), (&#39;eyes&#39;, 60), (&#39;face&#39;, 55), (&#39;house&#39;, 54), (&#39;head&#39;, 54), (&#39;night&#39;, 53), (&#39;day&#39;, 52), (&#39;hand&#39;, 50), (&#39;door&#39;, 47), (&#39;life&#39;, 44), (&#39;years&#39;, 44), (&#39;Mrs.&#39;, 41), (&#39;God&#39;, 41), (&#39;Kate&#39;, 40), (&#39;Mr.&#39;, 39), (&#39;people&#39;, 39)] sorted(nouns_freq.items(),key=lambda x:x[0], reverse=True)[:20] [(&#39;zoo&#39;, 2), (&#39;zlotys&#39;, 1), (&#39;zenith&#39;, 1), (&#39;youth&#39;, 5), (&#39;yelling&#39;, 1), (&#39;years&#39;, 44), (&#39;yearning&#39;, 1), (&quot;year&#39;s&quot;, 1), (&#39;year&#39;, 9), (&#39;yards&#39;, 4), (&#39;yard&#39;, 7), (&#39;yachts&#39;, 1), (&#39;writing&#39;, 2), (&#39;writers&#39;, 1), (&#39;writer&#39;, 4), (&#39;wrists&#39;, 1), (&#39;wrist&#39;, 2), (&#39;wrinkles&#39;, 1), (&#39;wrinkle&#39;, 1), (&#39;wretch&#39;, 1)] nouns_freq.most_common(10) [(&#39;man&#39;, 111), (&#39;time&#39;, 99), (&#39;men&#39;, 72), (&#39;room&#39;, 63), (&#39;way&#39;, 62), (&#39;eyes&#39;, 60), (&#39;face&#39;, 55), (&#39;house&#39;, 54), (&#39;head&#39;, 54), (&#39;night&#39;, 53)] ## Accsess data via fileid brown.fileids(categories=&#39;fiction&#39;)[0] &#39;ck01&#39; brown.sents(fileids=&#39;ck01&#39;) [[&#39;Thirty-three&#39;], [&#39;Scotty&#39;, &#39;did&#39;, &#39;not&#39;, &#39;go&#39;, &#39;back&#39;, &#39;to&#39;, &#39;school&#39;, &#39;.&#39;], ...] 16.3 WordNet 16.3.1 A Dictionary Resource from nltk.corpus import wordnet as wn word = &#39;walk&#39; # get synsets word_synsets = wn.synsets(word, pos=&#39;v&#39;) word_synsets [Synset(&#39;walk.v.01&#39;), Synset(&#39;walk.v.02&#39;), Synset(&#39;walk.v.03&#39;), Synset(&#39;walk.v.04&#39;), Synset(&#39;walk.v.05&#39;), Synset(&#39;walk.v.06&#39;), Synset(&#39;walk.v.07&#39;), Synset(&#39;walk.v.08&#39;), Synset(&#39;walk.v.09&#39;), Synset(&#39;walk.v.10&#39;)] Word sense is closely connected to its parts-of-speech. Therefore, in WordNet it is crucial to specify the POS tag of the word to obtain the correct synset of the word. There are four common part-of-speech tags in WordNet, as shown below: Part of Speech Tag Noun n Adjective a Adverb r Verb v Check the definition of a synset (i.e., a specific sense of the word): word_synsets[0].definition() &quot;use one&#39;s feet to advance; advance by steps&quot; Check the examples of a synset: word_synsets[0].examples() [&quot;Walk, don&#39;t run!&quot;, &#39;We walked instead of driving&#39;, &#39;She walks with a slight limp&#39;, &#39;The patient cannot walk yet&#39;, &#39;Walk over to the cabinet&#39;] ## Get details of each synset for s in word_synsets: if str(s.name()).startswith(&#39;walk.v&#39;): print( &#39;Syset ID: %s \\n&#39; &#39;POS Tag: %s \\n&#39; &#39;Definition: %s \\n&#39; &#39;Examples: %s \\n&#39; % (s.name(), s.pos(), s.definition(),s.examples()) ) Syset ID: walk.v.01 POS Tag: v Definition: use one&#39;s feet to advance; advance by steps Examples: [&quot;Walk, don&#39;t run!&quot;, &#39;We walked instead of driving&#39;, &#39;She walks with a slight limp&#39;, &#39;The patient cannot walk yet&#39;, &#39;Walk over to the cabinet&#39;] Syset ID: walk.v.02 POS Tag: v Definition: accompany or escort Examples: [&quot;I&#39;ll walk you to your car&quot;] Syset ID: walk.v.03 POS Tag: v Definition: obtain a base on balls Examples: [] Syset ID: walk.v.04 POS Tag: v Definition: traverse or cover by walking Examples: [&#39;Walk the tightrope&#39;, &#39;Paul walked the streets of Damascus&#39;, &#39;She walks 3 miles every day&#39;] Syset ID: walk.v.05 POS Tag: v Definition: give a base on balls to Examples: [] Syset ID: walk.v.06 POS Tag: v Definition: live or behave in a specified manner Examples: [&#39;walk in sadness&#39;] Syset ID: walk.v.07 POS Tag: v Definition: be or act in association with Examples: [&#39;We must walk with our dispossessed brothers and sisters&#39;, &#39;Walk with God&#39;] Syset ID: walk.v.08 POS Tag: v Definition: walk at a pace Examples: [&#39;The horses walked across the meadow&#39;] Syset ID: walk.v.09 POS Tag: v Definition: make walk Examples: [&#39;He walks the horse up the mountain&#39;, &#39;Walk the dog twice a day&#39;] Syset ID: walk.v.10 POS Tag: v Definition: take a walk; go for a walk; walk for pleasure Examples: [&#39;The lovers held hands while walking&#39;, &#39;We like to walk every Sunday&#39;] 16.3.2 Lexical Relations Extract words that maintain some lexical relations with the synset: word_synsets[0].hypernyms() # hypernym [Synset(&#39;travel.v.01&#39;)] word_synsets[0].hypernyms()[0].hyponyms() # similar words [Synset(&#39;accompany.v.02&#39;), Synset(&#39;advance.v.01&#39;), Synset(&#39;angle.v.01&#39;), Synset(&#39;ascend.v.01&#39;), Synset(&#39;automobile.v.01&#39;), Synset(&#39;back.v.02&#39;), Synset(&#39;bang.v.04&#39;), Synset(&#39;beetle.v.02&#39;), Synset(&#39;betake_oneself.v.01&#39;), Synset(&#39;billow.v.02&#39;), Synset(&#39;bounce.v.03&#39;), Synset(&#39;breeze.v.02&#39;), Synset(&#39;caravan.v.01&#39;), Synset(&#39;career.v.01&#39;), Synset(&#39;carry.v.36&#39;), Synset(&#39;circle.v.01&#39;), Synset(&#39;circle.v.02&#39;), Synset(&#39;circuit.v.01&#39;), Synset(&#39;circulate.v.07&#39;), Synset(&#39;come.v.01&#39;), Synset(&#39;come.v.11&#39;), Synset(&#39;crawl.v.01&#39;), Synset(&#39;cruise.v.02&#39;), Synset(&#39;derail.v.02&#39;), Synset(&#39;descend.v.01&#39;), Synset(&#39;do.v.13&#39;), Synset(&#39;drag.v.04&#39;), Synset(&#39;draw.v.12&#39;), Synset(&#39;drive.v.02&#39;), Synset(&#39;drive.v.14&#39;), Synset(&#39;ease.v.01&#39;), Synset(&#39;fall.v.01&#39;), Synset(&#39;fall.v.15&#39;), Synset(&#39;ferry.v.03&#39;), Synset(&#39;float.v.01&#39;), Synset(&#39;float.v.02&#39;), Synset(&#39;float.v.05&#39;), Synset(&#39;flock.v.01&#39;), Synset(&#39;fly.v.01&#39;), Synset(&#39;fly.v.06&#39;), Synset(&#39;follow.v.01&#39;), Synset(&#39;follow.v.04&#39;), Synset(&#39;forge.v.05&#39;), Synset(&#39;get_around.v.04&#39;), Synset(&#39;ghost.v.01&#39;), Synset(&#39;glide.v.01&#39;), Synset(&#39;go_around.v.02&#39;), Synset(&#39;hiss.v.02&#39;), Synset(&#39;hurtle.v.01&#39;), Synset(&#39;island_hop.v.01&#39;), Synset(&#39;lance.v.01&#39;), Synset(&#39;lurch.v.03&#39;), Synset(&#39;outflank.v.01&#39;), Synset(&#39;pace.v.02&#39;), Synset(&#39;pan.v.01&#39;), Synset(&#39;pass.v.01&#39;), Synset(&#39;pass_over.v.04&#39;), Synset(&#39;play.v.09&#39;), Synset(&#39;plow.v.03&#39;), Synset(&#39;prance.v.02&#39;), Synset(&#39;precede.v.04&#39;), Synset(&#39;precess.v.01&#39;), Synset(&#39;proceed.v.02&#39;), Synset(&#39;propagate.v.02&#39;), Synset(&#39;pursue.v.02&#39;), Synset(&#39;push.v.09&#39;), Synset(&#39;raft.v.02&#39;), Synset(&#39;repair.v.03&#39;), Synset(&#39;retreat.v.02&#39;), Synset(&#39;retrograde.v.02&#39;), Synset(&#39;return.v.01&#39;), Synset(&#39;ride.v.01&#39;), Synset(&#39;ride.v.04&#39;), Synset(&#39;ride.v.10&#39;), Synset(&#39;rise.v.01&#39;), Synset(&#39;roll.v.12&#39;), Synset(&#39;round.v.01&#39;), Synset(&#39;run.v.11&#39;), Synset(&#39;run.v.34&#39;), Synset(&#39;rush.v.01&#39;), Synset(&#39;scramble.v.01&#39;), Synset(&#39;seek.v.04&#39;), Synset(&#39;shuttle.v.01&#39;), Synset(&#39;sift.v.01&#39;), Synset(&#39;ski.v.01&#39;), Synset(&#39;slice_into.v.01&#39;), Synset(&#39;slither.v.01&#39;), Synset(&#39;snowshoe.v.01&#39;), Synset(&#39;speed.v.04&#39;), Synset(&#39;steamer.v.01&#39;), Synset(&#39;step.v.01&#39;), Synset(&#39;step.v.02&#39;), Synset(&#39;step.v.06&#39;), Synset(&#39;stray.v.02&#39;), Synset(&#39;swap.v.02&#39;), Synset(&#39;swash.v.01&#39;), Synset(&#39;swim.v.01&#39;), Synset(&#39;swim.v.05&#39;), Synset(&#39;swing.v.03&#39;), Synset(&#39;taxi.v.01&#39;), Synset(&#39;trail.v.03&#39;), Synset(&#39;tram.v.01&#39;), Synset(&#39;transfer.v.06&#39;), Synset(&#39;travel.v.04&#39;), Synset(&#39;travel.v.05&#39;), Synset(&#39;travel.v.06&#39;), Synset(&#39;travel_by.v.01&#39;), Synset(&#39;travel_purposefully.v.01&#39;), Synset(&#39;travel_rapidly.v.01&#39;), Synset(&#39;trundle.v.01&#39;), Synset(&#39;turn.v.06&#39;), Synset(&#39;walk.v.01&#39;), Synset(&#39;walk.v.10&#39;), Synset(&#39;weave.v.04&#39;), Synset(&#39;wend.v.01&#39;), Synset(&#39;wheel.v.03&#39;), Synset(&#39;whine.v.01&#39;), Synset(&#39;whish.v.02&#39;), Synset(&#39;whisk.v.02&#39;), Synset(&#39;whistle.v.02&#39;), Synset(&#39;withdraw.v.01&#39;), Synset(&#39;zigzag.v.01&#39;), Synset(&#39;zoom.v.02&#39;)] word_synsets[0].root_hypernyms() # root [Synset(&#39;travel.v.01&#39;)] word_synsets[0].hypernym_paths() # from root to this synset in WordNet [[Synset(&#39;travel.v.01&#39;), Synset(&#39;walk.v.01&#39;)]] Synonyms # Collect synonyms of all synsets synonyms = [] for syn in wn.synsets(&#39;book&#39;, pos=&#39;v&#39;): for lemma in syn.lemmas(): synonyms.append(lemma.name()) len(synonyms) 6 len(set(synonyms)) 3 print(set(synonyms)) {&#39;reserve&#39;, &#39;hold&#39;, &#39;book&#39;} Antonyms # First Synset Lemma String word_synsets[0].lemmas()[0].name() # Antonyms of the First Synset &#39;walk&#39; word_synsets[0].lemmas()[0].antonyms()[0] Lemma(&#39;ride.v.02.ride&#39;) While previous taxonomic relations (e.g., hypernymy and hyponymy) are in-between synsets, the synonymy and antonymy relations are in-between lemmas. We need to be very clear about the use of the three variants in WordNet: Word Form Lemma Synset 16.3.3 Semantic Similarity Computation Synsets are organized in a hypernym tree, which can be used for reasoning about the semantic similarity of two Synsets. The closer the two Synsets are in the tree, the more similar they are. # syn1 = wn.synsets(&#39;walk&#39;, pos=&#39;v&#39;)[0] syn1 = wn.synset(&#39;walk.v.01&#39;) syn2 = wn.synset(&#39;toddle.v.01&#39;) syn3 = wn.synset(&#39;think.v.01&#39;) syn1.wup_similarity(syn2) 0.8 syn1.wup_similarity(syn3) 0.2857142857142857 syn1.path_similarity(syn2) 0.5 syn1.path_similarity(syn3) 0.16666666666666666 ref = syn1.hypernyms()[0] syn1.shortest_path_distance(ref) 1 syn2.shortest_path_distance(ref) 2 syn1.shortest_path_distance(syn2) 1 print(ref.definition()) change location; move, travel, or proceed, also metaphorically print([l.name() for l in ref.lemmas()]) [&#39;travel&#39;, &#39;go&#39;, &#39;move&#39;, &#39;locomote&#39;] syn1.hypernym_paths() [[Synset(&#39;travel.v.01&#39;), Synset(&#39;walk.v.01&#39;)]] syn2.hypernym_paths() [[Synset(&#39;travel.v.01&#39;), Synset(&#39;walk.v.01&#39;), Synset(&#39;toddle.v.01&#39;)]] syn3.hypernym_paths() [[Synset(&#39;think.v.03&#39;), Synset(&#39;evaluate.v.02&#39;), Synset(&#39;think.v.01&#39;)]] The wup_similarity method is short for Wu-Pamler Similarity. It is a scoring method for how similar the word senses are based on where the Synsets occur relative to each other in the hypernym tree. For more information or other scoring methods, please check NLTK WordNet documentation. 16.4 Discovering Word Collocations from nltk.corpus import brown from nltk.collocations import BigramCollocationFinder from nltk.metrics import BigramAssocMeasures words = [w.lower() for w in brown.words()] bcf = BigramCollocationFinder.from_words(words) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4) [(&#39;;&#39;, &#39;;&#39;), (&#39;?&#39;, &#39;?&#39;), (&#39;of&#39;, &#39;the&#39;), (&#39;.&#39;, &#39;``&#39;)] # deal with stopwords from nltk.corpus import stopwords stopset = set(stopwords.words(&#39;english&#39;)) ## Fitler critera: ## remove words whose length &lt; 3 or which are on the stop word list filter_stops = lambda w: len(w) &lt; 3 or w in stopset bcf.apply_word_filter(filter_stops) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10) [(&#39;united&#39;, &#39;states&#39;), (&#39;new&#39;, &#39;york&#39;), (&#39;per&#39;, &#39;cent&#39;), (&#39;years&#39;, &#39;ago&#39;), (&#39;rhode&#39;, &#39;island&#39;), (&#39;los&#39;, &#39;angeles&#39;), (&#39;peace&#39;, &#39;corps&#39;), (&#39;san&#39;, &#39;francisco&#39;), (&#39;high&#39;, &#39;school&#39;), (&#39;fiscal&#39;, &#39;year&#39;)] ## apply freq-based filter bcf.apply_freq_filter(3) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10) [(&#39;united&#39;, &#39;states&#39;), (&#39;new&#39;, &#39;york&#39;), (&#39;per&#39;, &#39;cent&#39;), (&#39;years&#39;, &#39;ago&#39;), (&#39;rhode&#39;, &#39;island&#39;), (&#39;los&#39;, &#39;angeles&#39;), (&#39;peace&#39;, &#39;corps&#39;), (&#39;san&#39;, &#39;francisco&#39;), (&#39;high&#39;, &#39;school&#39;), (&#39;fiscal&#39;, &#39;year&#39;)] Exercise 16.1 Try to identify bigram collocations in the corpus, Alice in the Wonderland. The texts are available in nltk.corpus.gutenburg. Exercise 16.2 Following the same strategy of bigram collocation extraction, please try to extract trigrams from the brown corpus. Remove stop words and short words as we did in the lecture. Include only trigrams whose frequency &gt; 5. 16.5 Tokenization import nltk from nltk.tokenize import word_tokenize, sent_tokenize text = &quot;&quot;&quot;Three blind mice! See how they run! They all ran after the farmer&#39;s wife, Who cut off their tails with a carving knife. Did you ever see such a thing in your life As three blind mice? &quot;&quot;&quot; text_sent = sent_tokenize(text) text_word = word_tokenize(text) text_pos = nltk.pos_tag(text_word) With words, we can create a frequency list: import pprint as pp text_fd= nltk.FreqDist([w.lower() for w in text_word]) pp.pprint(text_fd.most_common(10)) [(&#39;three&#39;, 2), (&#39;blind&#39;, 2), (&#39;mice&#39;, 2), (&#39;!&#39;, 2), (&#39;see&#39;, 2), (&#39;they&#39;, 2), (&#39;a&#39;, 2), (&#39;how&#39;, 1), (&#39;run&#39;, 1), (&#39;all&#39;, 1)] Exercise 16.3 Provide the word frequency list of the top 30 nouns in Alice in the Wonderland. The raw texts of the novel is available in NTLK (see below). alice = nltk.corpus.gutenberg.raw(fileids=&#39;carroll-alice.txt&#39;) [(&#39;Alice&#39;, 392), (&#39;Queen&#39;, 71), (&#39;time&#39;, 65), (&#39;King&#39;, 60), (&#39;Turtle&#39;, 58), (&#39;Mock&#39;, 56), (&#39;Hatter&#39;, 55), (&#39;*&#39;, 54), (&#39;Gryphon&#39;, 54), (&#39;way&#39;, 53), (&#39;head&#39;, 50), (&#39;thing&#39;, 49), (&#39;voice&#39;, 47), (&#39;Rabbit&#39;, 44), (&#39;Duchess&#39;, 42), (&#39;tone&#39;, 40), (&#39;Dormouse&#39;, 40), (&#39;March&#39;, 34), (&#39;moment&#39;, 31), (&#39;Hare&#39;, 31), (&#39;nothing&#39;, 30), (&#39;things&#39;, 30), (&#39;door&#39;, 30), (&#39;Mouse&#39;, 29), (&#39;eyes&#39;, 28), (&#39;Caterpillar&#39;, 27), (&#39;day&#39;, 25), (&#39;course&#39;, 25), (&#39;Cat&#39;, 25), (&#39;round&#39;, 23)] 16.6 Chinese Word Segmentation import jieba text = &quot;&quot;&quot; 高速公路局說，目前在國道3號北向水上系統至中埔路段車多壅塞，已回堵約3公里。另外，國道1號北向仁德至永康路段路段，已回堵約有7公里。建議駕駛人提前避開壅塞路段改道行駛，行經車多路段請保持行車安全距離，小心行駛。 國道車多壅塞路段還有國1內湖-五堵北向路段、楊梅-新竹南向路段；國3三鶯-關西服務區南向路段、快官-霧峰南向路段、水上系統-中埔北向路段；國6霧峰系統-東草屯東向路段、國10燕巢-燕巢系統東向路段。 &quot;&quot;&quot; text_jb = jieba.lcut(text) Building prefix dict from the default dictionary ... Loading model from cache /var/folders/n7/ltpzwx813c599nfxfb94s_640000gn/T/jieba.cache Loading model cost 0.746 seconds. Prefix dict has been built successfully. print(&#39; | &#39;.join(text_jb)) | 高速公路 | 局說 | ， | 目前 | 在 | 國道 | 3 | 號 | 北向 | 水上 | 系統 | 至 | 中埔 | 路段 | 車多 | 壅塞 | ， | 已回 | 堵約 | 3 | 公里 | 。 | 另外 | ， | 國道 | 1 | 號 | 北向 | 仁德 | 至 | 永康 | 路段 | 路段 | ， | 已回 | 堵 | 約 | 有 | 7 | 公里 | 。 | 建議 | 駕駛人 | 提前 | 避開 | 壅塞 | 路段 | 改道 | 行駛 | ， | 行經車 | 多 | 路段 | 請 | 保持 | 行車 | 安全 | 距離 | ， | 小心 | 行駛 | 。 | | 國道 | 車多 | 壅塞 | 路段 | 還有國 | 1 | 內湖 | - | 五堵 | 北向 | 路段 | 、 | 楊梅 | - | 新竹 | 南向 | 路段 | ； | 國 | 3 | 三鶯 | - | 關 | 西服 | 務區 | 南向 | 路段 | 、 | 快官 | - | 霧峰 | 南向 | 路段 | 、 | 水上 | 系統 | - | 中埔 | 北向 | 路段 | ； | 國 | 6 | 霧峰 | 系統 | - | 東 | 草屯 | 東向 | 路段 | 、 | 國 | 10 | 燕巢 | - | 燕巢 | 系統 | 東向 | 路段 | 。 | "],["web-scraping.html", "Chapter 17 Web Scraping 17.1 webbrowswer module 17.2 requests Module 17.3 bs4 Module (Beautiful Soup) 17.4 Reference", " Chapter 17 Web Scraping Web scraping is the term for using a program to download and process content from the web. webbrowser: A default Python module to open a browser to specific page. requests: A module to download files and web pages from the Internet. bs4: A modile to parse HTML, i.e., the format that web pages are written in. selenium: A module to launch and control a web browser (e.g., filling in forms, simulating mouse clicks.) 17.1 webbrowswer module Create a python script with the following codes, named py-checkword.py #! python3 import webbrowser, sys, pyperclip if len(sys.argv) &gt; 1: # Get input from the command line target = &#39; &#39;.join(sys.argv[1:]) else: # Get input from the clipboard target = pyperclip.paste() webbrowser.open(&#39;https://www.dictionary.com/browse/&#39;+ target) Run the python script in the terminal python py-checkword.py beauty Exercise 17.1 How to modify the py-checkword.py so that the user can attach a list of words separated by spaces for checking? For example, the modified script will be able to open three web browsers for beauty, internet, and national. python py-checkword2.py beauty internet national 17.2 requests Module The requests modules allow us to easily download files from the web without having to worry about complicated issues such as network errors, connection problems, and data compression. import requests res = requests.get(&#39;https://www.gutenberg.org/files/2591/2591-0.txt&#39;) type(res) ## Check status code to see if the download is successful &lt;class &#39;requests.models.Response&#39;&gt; res.status_code == requests.codes.ok ## `requests.codes.ok` == 200 True len(res.text) 559879 print(res.text[:250]) ï»¿The Project Gutenberg eBook of Grimmsâ Fairy Tales, by The Brothers Grimm This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may cop Prepare potential errors during the file download import requests res = requests.get(&#39;https://www.gutenberg.org/file-that-does-not-exist.txt&#39;) ## Check status code to see if the download is successful res.status_code == requests.codes.ok ## `requests.codes.ok` == 200 False len(res.text) 6396 print(res.text[:250]) &lt;!DOCTYPE html&gt; &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;/&gt; &lt;title&gt;404 | Project Gutenberg&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;/gutenberg/style.css?v=1.1&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;/gutenberg/collapsible.css res.raise_for_status() Error in py_call_impl(callable, dots$args, dots$keywords): HTTPError: 404 Client Error: Not Found for url: https://www.gutenberg.org/file-that-does-not-exist.txt Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;/Users/Alvin/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/requests/models.py&quot;, line 943, in raise_for_status raise HTTPError(http_error_msg, response=self) A better way to modify the codes is to make sure that the program stops as soon as some unexpected error happens. Always call raise_for_status() after calling requests.get() because we need to make sure the file has been successfully downloaded before the program continues. import requests res = requests.get(&#39;https://www.gutenberg.org/file-that-does-not-exist.txt&#39;) try: res.raise_for_status() except Exception as exc: print(&#39;There was a problem with the link: %s&#39; % (exc)) There was a problem with the link: 404 Client Error: Not Found for url: https://www.gutenberg.org/file-that-does-not-exist.txt Usually we may want to scrape the texts from the web and save them on the Hard Drive. import requests res = requests.get(&#39;https://www.gutenberg.org/files/2591/2591-0.txt&#39;) try: res.raise_for_status() except Exception as exc: print(&#39;There was a problem with the link: %s&#39; % (exc)) with open(&#39;grimms.txt&#39;, &#39;w&#39;) as f: f.write(res.text) 559879 17.3 bs4 Module (Beautiful Soup) Beautiful Soup is a module for extracting information from an HTML page. The package name is pip install -U beatifulsoup4 but in use, it is import bs4. Each Word is a dictionary: “headword”: head word string “pronunciation”: IPA “senses”: a list of tuples, (sense_definition, sense_example) import requests, bs4 target=&#39;produce&#39; res = requests.get(&#39;https://www.dictionary.com/browse/&#39; + target) res.raise_for_status() soup = bs4.BeautifulSoup(res.text, &#39;lxml&#39;) entries = soup.select(&#39;.css-1avshm7&#39;) # entries ## Define the word structure (dict) cur_word = {} # for each entry for i, entry in enumerate(entries): ## Include only the main entry of the page if len(entry.select(&#39;h1&#39;)) &gt; 0: #print(&#39;Entry Number: &#39;, i) ## headword and pronunciations cur_headword = entry.select(&#39;h1&#39;)[0].getText() cur_spell = entry.select(&#39;.pron-spell-content&#39;)[0].getText() cur_ipa = entry.select(&#39;.pron-ipa-content&#39;)[0].getText() #print(&#39;Headword: &#39;, cur_headword) #print(&#39;Pronunciation: &#39;, cur_ipa) cur_word[&#39;headword&#39;] = cur_headword cur_word[&#39;pronunciation&#39;] = cur_ipa # for each POS type in the current entry for pos in entry.select(&#39;.css-pnw38j&#39;): cur_pos = pos.select(&#39;.luna-pos&#39;)[0].getText() #print(&#39;=&#39;*10) #print(&#39;POS: &#39;, cur_pos.upper()) cur_definitions = pos.select(&#39;div[value] &gt; span.one-click-content&#39;) cur_sense_list =[] # for each definition in the current POS for sense in cur_definitions: #print(&#39;DEF: &#39; + sense.find(text=True, recursive=True)) ## check if there&#39;s any example ex = sense.find(attrs={&#39;class&#39;:&#39;luna-example&#39;}) if ex is not None: cur_ex = ex.getText() _ = ex.extract() else: cur_ex = &#39;&#39; cur_def = sense.getText() #print(&#39;-&#39;*10) #print(&#39;Definition: &#39; + cur_def) #print(&#39;Example: &#39;+ cur_ex) cur_sense = {&#39;definition&#39;: cur_def, &#39;example&#39;: cur_ex} cur_sense_list.append(cur_sense) cur_word[cur_pos] = cur_sense_list import json with open(target+&#39;.json&#39;, &#39;w&#39;) as f: json.dump(cur_word, f) import json with open(&#39;produce.json&#39;,&#39;r&#39;) as f: cur_word = json.load(f) cur_word.keys() dict_keys([&#39;headword&#39;, &#39;pronunciation&#39;, &#39;verb (used with object),&#39;, &#39;verb (used without object),&#39;, &#39;noun&#39;]) print(json.dumps(cur_word, sort_keys=False, indent=4)) { &quot;headword&quot;: &quot;produce&quot;, &quot;pronunciation&quot;: &quot;/ verb pr\\u0259\\u02c8dus, -\\u02c8dyus; noun \\u02c8pr\\u0252d us, -yus, \\u02c8pro\\u028a dus, -dyus /&quot;, &quot;verb (used with object),&quot;: [ { &quot;definition&quot;: &quot;to bring into existence; give rise to; cause: &quot;, &quot;example&quot;: &quot;to produce steam.&quot; }, { &quot;definition&quot;: &quot;to bring into existence by intellectual or creative ability: &quot;, &quot;example&quot;: &quot;to produce a great painting.&quot; }, { &quot;definition&quot;: &quot;to make or manufacture: &quot;, &quot;example&quot;: &quot;to produce automobiles for export.&quot; }, { &quot;definition&quot;: &quot;to bring forth; give birth to; bear: &quot;, &quot;example&quot;: &quot;to produce a litter of puppies.&quot; }, { &quot;definition&quot;: &quot;to provide, furnish, or supply; yield: &quot;, &quot;example&quot;: &quot;a mine producing silver.&quot; }, { &quot;definition&quot;: &quot;Finance. &quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;to cause to accrue: &quot;, &quot;example&quot;: &quot;stocks producing unexpected dividends.&quot; }, { &quot;definition&quot;: &quot;to bring forward; present to view or notice; exhibit: &quot;, &quot;example&quot;: &quot;to produce one&#39;s credentials.&quot; }, { &quot;definition&quot;: &quot;to bring (a play, movie, opera, etc.) before the public.&quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;to extend or prolong, as a line.&quot;, &quot;example&quot;: &quot;&quot; } ], &quot;verb (used without object),&quot;: [ { &quot;definition&quot;: &quot;to create, bring forth, or yield offspring, products, etc.: &quot;, &quot;example&quot;: &quot;Their mines are closed because they no longer produce.&quot; }, { &quot;definition&quot;: &quot;Economics. &quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;to create economic value; bring crops, goods, etc., to a point at which they will command a price.&quot;, &quot;example&quot;: &quot;&quot; } ], &quot;noun&quot;: [ { &quot;definition&quot;: &quot;something that is produced; yield; product. &quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;agricultural products collectively, especially vegetables and fruits.&quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;offspring, especially of a female animal: &quot;, &quot;example&quot;: &quot;the produce of a mare.&quot; } ] } Exercise 17.2 Now how to extend this short script to allow the users to perform searches of multiple words at one time and scrape all definitions and examples from the website of Dictionary.com. ? 17.4 Reference Beautiful Soup Documentation "],["the-shell.html", "A The Shell A.1 Why do you need to know shell commands? A.2 Shebang Line A.3 Basic Shell Commands A.4 Text-Analytic Related Commands A.5 References", " A The Shell A.1 Why do you need to know shell commands? The command line has many great advantages that can make you a more efficient and productive data scientist. Janssens (2014) has nicely summarized the strengths of command lines in five points: The command line is agile The command line is augmenting The command line is scalable The command line is extensible The command line is ubiquitouos A.2 Shebang Line Like in R console, you can interact with the R console line by line or you can package all your R scripts in a file. For command line, you can also combine several shell commands into a script file. For a shell script file, you need a shebang line, which is the character sequence consisting of the characters number sign and exclamation mark (#!) at the beginning of a script. This line would indicate to the shell engine which interpreter (language environment) is needed to parse the script file. A shell script often takes a shebang line as below. #!/bin/sh #!/bin/bash A.3 Basic Shell Commands Linux Man Pages The most basic commands are listed below: pwd (print working directory). Shows directory or “folder” you are currently operating in. This is not necessarily the same as the R working directory you get from getwd(). ls (list files). Shows the files in the current working directory. This is equivalent to looking at the files in your Finder/Explorer/File Manager. Use ls -a to also list hidden files, such as .Rhistory and .git. cd (change directory). Allows you to navigate through your directories by changing the shell’s working directory. You can navigate like so: go to subdirectory foo of current working directory: cd foo go to parent of current working directory: cd .. go to your “home” directory: cd ~ or simply cd go to directory using absolute path, works regardless of your current working directory: cd /home/my_username/Desktop. Windows uses a slightly different syntax with the slashes between the folder names reversed, \\, e.g. cd C:\\Users\\MY_USERNAME\\Desktop. Pro tip 1: Dragging and dropping a file or folder into the terminal window will paste the absolute path into the window. Pro tip 2: Use the tab key to autocomplete unambiguous directory and file names. Hit tab twice to see all ambiguous options. Use arrow-up and arrow-down to repeat previous commands. Or search for previous commands with CTRL + r. which Show the full path of a shell commands which python: Check which version of python your system uses which r: Check which version of R your system uses cp Copy files and directories rm Remove files and directories mv Move files and directories mkdir Make directories A.4 Text-Analytic Related Commands gzip [-cd#] FILENAME. Zips/Unzips a file. The zipped file created by gzip is often with the extension *.gz. -c: write output on standard output -d: decompress -#: Regulate the speed of compression using the specified digit #, where -1 or –fast indicates the fastest compression method (less compression) and -9 or –best indicates the slowest compression method (best compression). The default compression level is -6. tar [-j|-z] [cv] [-f FILENAME] filename.... Archive many files into one single file -j: Use bzip2 compression -z: Use gzip compression -c: Create a new archive -v: Print all files processed verbosely tar [-j|-z] [xv] [-f FILENAME] [-C PATH. Restore the original files from the achive file -j: Use bzip2 compression -z: Use gzip compression -x: Extract files from archive -v: Print all files processed verbosely C: Extract files to a particular path So when you see a file with the extention of *.tar.gz, this indicates that this file is a zipped file, which compresses a list of multiple files into one tar archive. Usually people pass their collection of multiple text files (i.e., a corpus) in this way because it is often easier to share one file instead of tons of files at a time. A.5 References If you are interested in more functions and potentials of shell commands, I would highly recommend the book Data Science at the Command Line. References "],["references-3.html", "References", " References "]]
