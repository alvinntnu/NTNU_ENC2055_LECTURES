[["index.html", "ENC2055: Introduction to Programming Languages for Linguistic Analysis Course Syllabus Course Objective Textbook Course Website Assignments AI Tools Contributing to the Lecture Notes Course Demo Data Questions? Necessary Packages Interative Live Environment", " ENC2055: Introduction to Programming Languages for Linguistic Analysis Alvin Chen (陳正賢） 2024-09-27 Course Syllabus Welcome to ENC2055 Introduction to Programming Languages for Linguistic Analysis. This is a graduate-level course tailored to those who are interested in computational text analytics and data science in general. Have you decided to embark upon a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offering necessary skills and knowledge in important disciplines. This introductory course in basic computational coding would be a prerequisite course for many other advance-level courses. In particular, our faculty in the Linguistics track is dedicated to research on both theoretical and applied linguistics. Along with courses offered by other faculty members, this course provides a necessary foundation for the burgeoning discipline of computational text analytics. Please note that while this course has no prerequisite from the students, it will turn out to be a prerequisite for a lot of advanced courses, such as Corpus Linguistics and Computational Linguistics. If you plan to step into this computational way of language processing, you need to take this course (or at least you need to learn how to code.) Course Objective The objective of this course is to provide a comprehensive introduction to programming languages with a special focus on its application in linguistic analyses. This course is especially tailored to those who do not have any background or experiences in coding. We will start from the very basic concepts, such as data types, variable assignments, control structures, to more complex procedures such as routines, functions, and other exploratory project-based tasks. The course consists of a series of theme-based hands-on tutorials, which demonstrate how the flexibility of the programming language can help you become a more efficient and productive data scientist. Specifically, this course will use the language R as our featuring programming language and introduce you to , Rstudio, and a collection of R packages designed to work together to make linguistic analyses fast, fluent, and fun. In addition, we will briefly touch upon a few important constructs with another popular language in data science, Python . By the end of the course, students should have a working knowledge of coding and an initial ability to advance a project independently as a data scientist. In this course, we will not be dealing with complex maths like: \\[ f(x)=\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} x^{2}} \\] \\[ P(A) = \\sum P(\\{ (e_1,\\dotsc,e_N) \\}) = \\binom{N}{k} \\cdot p^kq^{N-k} \\] We will not be dealing with linguistic theories as well. No transformations. No movements. No bindings. This course is all about (text) data processing and computational coding. library(tidyverse) data(&quot;USArrests&quot;) # ?USArrests ## check description of the dataset head(USArrests) USArrests %&gt;% mutate(States = row.names(.)) %&gt;% pivot_longer(cols = c(&quot;Murder&quot;, &quot;Assault&quot;, &quot;UrbanPop&quot;, &quot;Rape&quot;), names_to = &quot;ArrestTypes&quot;, values_to = &quot;Rates&quot;) %&gt;% ggplot(aes(ArrestTypes, Rates, fill = ArrestTypes)) + geom_boxplot() USArrests %&gt;% mutate(States = row.names(.)) %&gt;% pivot_longer(cols = c(&quot;Murder&quot;, &quot;Assault&quot;, &quot;UrbanPop&quot;, &quot;Rape&quot;), names_to = &quot;ArrestTypes&quot;, values_to = &quot;Rates&quot;) %&gt;% filter(States %in% c(&quot;California&quot;, &quot;Massachusetts&quot;, &quot;New York&quot;, &quot;Washington&quot;)) %&gt;% ggplot(aes(States, Rates, fill = ArrestTypes, group = ArrestTypes)) + geom_col(position = &quot;dodge&quot;) + labs(title = &quot;Arrest Types (Rates) by Selective States&quot;) ## data() ## check more built-in datasets In the lecture notes, the text boxes in light blue refer to codes that you need to run in either terminal or your R/Python console. The text boxes in black background show the outputs of the code processing. We will follow this presentation convention throughout the entire lecture notes. print(&quot;Hello! R!&quot;) [1] &quot;Hello! R!&quot; Textbook Throughout the semester, we will follow the materials provided on our course website (see below). We will not use a particular textbook for the course. However, I do like to recommend Wickham &amp; Grolemund (2017) for its simplicity. Also, another great book for R lovers, by Davies (2016): And two more comprehensive books for Python basics: Gerrard (2016) and Sweigart (2020): Course Website We have a course website. You may need a password to get access to the course materials. If you are an officially enrolled student, please ask the instructor for the access code. Assignments Students are expected to complete the exercises included in each chapter/topic. The assignment due is always one week after the chapter/topic is completed. No late submission will be accepted. Students need to follow the Ch3 Code Format Convention for their submitted scripts. AI Tools I encourage the use of AI tools to help with R/Python learning. R Wizard: A chatGPT-powered specialist in R programming, skilled in Data Science, Statistics, and Finance, providing accurate and useful guidance. Python: A highly sophisticated GPT tailored for Python Contributing to the Lecture Notes Although I have tried every possible way to make sure that the contents are correct, I may still accidentally make mistakes in the materials. If you spot any errors and would like make suggestions for better solutions, I would be more than happy to hear from you. To contribute your ideas, let’s use Hypothes.is, which is an amazing tool for website annotations. Go to Hypothes.is, and click the “get-started” on the top-right corner of the homepage. Install the the add-on for chrome, or other browser. To add an annotation, select some text and then click the on the pop-up menu. To see the annotations of others, click the in the upper right-hand corner of the page. Please turn on the Hypothes.is add-on when you are reading the course lecture notes, and you will see all public/shared annotations made by other course participants. See Quick Start Guide for Students and Annotation Tips for Students. At the beginning of the semester, I will share with the class a link to invite all the enrolled students to join a private group for annotation. But one can always provide feedbacks via the public annotations of the website. Course Demo Data Dropbox Demo Data Directory Questions? For more information related to this course, please see the FAQ on our course website or post your questions on the Moodle Discussion Forum. Please avoid sending coding questions via private email. Instead, post your questions on the Moodle Discussion Forum so that classmates with similar queries can benefit from the discussions and feedback. Necessary Packages In this course, we will need the following R packages for tutorials and exercises. library(chatgpt) library(dplyr) library(foreign) library(gganimate) library(ggplot2) library(ggrepel) library(ggridges) library(Hmisc) library(quanteda.textplots) library(quanteda.textstats) library(quanteda) library(readr) library(readtext) library(reticulate) library(showtext) library(stringr) library(tibble) library(tidyr) library(tidytext) library(tidyverse) Interative Live Environment This is not working right now. I am fixing this. Binder is an open-source tool that allows us to create an interactive environment to run our codes and scripts on the cloud. Through the link, we can launch an interactive RStudio environment in the web browser and run the R codes and scripts from the lecture notes. The cloud-based environment is pre-configured with essential tools required for the course, such as the R kernel, RStudio, and R libraries. The Binder platform offers a solution to compatibility issues arising from the use of different operating systems, such as Windows, Mac OS, and Linux. The use of Binder in the classroom enables us to work with a consistent and reliable platform for the R programming needs. Please note that any changes in the interactive Binder environment will not be retained upon closing the session. References Davies, T. M. (2016). The book of R: A first course in programming and statistics (1st ed.). No Starch Press, Inc. Gerrard, P. (2016). Lean python: Learn just enough python to build useful tools. Apress. Sweigart, A. (2020). Automate the boring stuff with python: Practical programming for total beginners. 2nd edition. No Starch Press. "],["intro-ds.html", "Chapter 1 Data Science and R 1.1 What is Data Science? 1.2 Working Pipeline for Data Science 1.3 Why R? 1.4 tidyverse 1.5 More Skills", " Chapter 1 Data Science and R 1.1 What is Data Science? Data Science is an interdisciplinary subject, which integrates knowledge of statistics, computer science and other domina-specific areas. A graph by Drew Conway may summarize the essense of Data Science: 1.2 Working Pipeline for Data Science Hadley Wickham’s R for Data Science describes six important steps for data analysis: 1.3 Why R? According to a report by KDnuggets, among all the languages used by data scientists, python and R are the most popular two languages: It is true that Python now seems much more popular among IT developers. That being said, you may consult this article, &lt;&lt; Why R is the Best Data Science Language to Learn Today? &gt;&gt;, for a more comprehensive review of the strengths of R. The general tendency is that: if you want to go into the industry and take developers or programmers as your future career, you can choose Python; if you are planning to settle yourself in the academia, I would definitely recommend R. Here are a list of strengths for R language: powerful statistical analysis data visualization exploratory analysis re-usable reports tidyverse consistent grammar/syntax high readability of the codes, similar to human languages ( %&gt;% is a unique R feature!) In this course, our main objective is to introduce you to the world of coding. A high-level programming language like R would be a very friendly start, especially for those who have no background of computing. So, let us enjoy the journey of a simple yet powerful language learning! In fact, now it seems that data scientists are expected to be multilingual and well-versed in the proper coding language to deal with the target tasks efficiently. Also, R and RStudio have developed toward this aim by expanding its capacity of integrating the Python language. Now RStudio can be a Single Home for R &amp; Python. Similarly, Jupyter Lab/Notebook (a Python IDE) can run R codes seamlessly as well. 1.4 tidyverse In this course, we will be working on a collection of packages included in tidyverse. This is a unique integrated package in R, which can help you deal with data in a massively convenient way. It is hoped that the user can easily call particular functions and make use of the pipe operator %&gt;% to concatenate all procedures serially, just like the natural human languages. The pipe operator in R is used to simplify code by chaining multiple operations into a readable flow. The native pipe operator (|&gt;) was introduced in R 4.1.0, while the non-native pipe (%&gt;%) is from the magrittr package, which is part of the tidyverse. In particular, we will work on the following major libraries from tidyverse: ggplot2: Data visualization dplyr: Data wrangling tidyr: Data wrangling stringr: String manipulation readr: Data importing purrr: Functional programming to avoid loops tibble: Powerful data structure Here is a quick example to show the efficiency of the tidyverse-style R. We first prepare a simple collection of texts, which include 720 sentences. library(tidyverse) head(sentences) ## `sentences` dataset is included in `stringr` [1] &quot;The birch canoe slid on the smooth planks.&quot; [2] &quot;Glue the sheet to the dark blue background.&quot; [3] &quot;It&#39;s easy to tell the depth of a well.&quot; [4] &quot;These days a chicken leg is a rare dish.&quot; [5] &quot;Rice is often served in round bowls.&quot; [6] &quot;The juice of lemons makes fine punch.&quot; ## Prepare a data frame for analysis corp &lt;- data.frame(id = seq(1:length(sentences)), texts = str_to_lower(sentences)) corp With a few R commands concatenated by the pipe %&gt;%, we can see the distributions of the vowel and consonant percentages of all the sentences in the text collection. corp %&gt;% mutate(NumOfChars = nchar(texts), VowelPer = str_count(texts,&#39;[aeiou]&#39;)/NumOfChars, ConPer = str_count(texts,&#39;[^aeiou]&#39;)/NumOfChars) %&gt;% pivot_longer(c(&quot;VowelPer&quot;, &quot;ConPer&quot;), names_to = &quot;Segment&quot;,values_to = &quot;Percent&quot;) %&gt;% ggplot(aes(Segment, Percent, fill=Segment)) + geom_boxplot(notch=TRUE) 1.5 More Skills Data scientists are now becoming more and more popular. To know more about this job, one thing you may want to know is what kinds of skills are needed? The following two graphs were taken from &lt;The Most in Demand Skills for Data Scientists&gt;: While people still have various definitions regarding what data science encompasses, there are indeed several practical fields that have been commonly regarded as part of the definitions of Data Science. According to Mason and Wiggins (2010) A Taxonomy of Data Science, data science can be defined according to five crucial steps: Obtain: pointing and clicking does not scale. (Data collection via copy-pasting is limited.) Scrub: the world is a messy place. (Clean data are hard to come by.) Explore: You can see a lot by looking. (A picture is worth a thousand words.) Models: always bad, sometimes ugly. (Chance-level observations are everywhere.) iNterpret: “The purpose of computing is insight, not numbers.” This OSEMN (awesome!!) model should give you a much clearer picture of what you need to become a proficient data scientist. What we do here in this course is just a start….Take a deep breath:) "],["r-fundamentals.html", "Chapter 2 R Fundamentals 2.1 Installing R 2.2 Installing RStudio 2.3 My Current Version 2.4 The Interface of Rstudio 2.5 Assignment 2.6 Data Structure 2.7 Function 2.8 Vectorization 2.9 Script 2.10 Library 2.11 Setting 2.12 Seeking Help 2.13 Language Learning Ain’t Easy! 2.14 Keyboard Shortcuts", " Chapter 2 R Fundamentals Download R: R-Project IDE: RStudio 2.1 Installing R Download the installation file: http://cran.r-project.org 2.2 Installing RStudio After you install R, you may install RStudio. RStudio is an editor which can help you write R codes. A good analogy is that R is the engine and Rstudio is the dashboard of the car. Please download the right version that is compatible with your PC operating system. https://www.rstudio.com/download Choose RStudio Desktop Important notes: Do not have Chinese characters in your directory names or on the path to the files Do not have spaces and weird symbols in your file path: D:/R D:/Rstudio /User/Alvinchen/ 2.3 My Current Version sessionInfo() R version 4.3.1 (2023-06-16) Platform: aarch64-apple-darwin20 (64-bit) Running under: macOS Sonoma 14.6.1 Matrix products: default BLAS: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib; LAPACK version 3.11.0 locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 time zone: Asia/Taipei tzcode source: internal attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] lubridate_1.9.3 forcats_1.0.0 stringr_1.5.0 dplyr_1.1.3 [5] purrr_1.0.2 readr_2.1.4 tidyr_1.3.0 tibble_3.2.1 [9] ggplot2_3.4.4 tidyverse_2.0.0 reticulate_1.34.0 loaded via a namespace (and not attached): [1] rappdirs_0.3.3 sass_0.4.7 utf8_1.2.4 generics_0.1.3 [5] icons_0.2.0 xml2_1.3.5 stringi_1.7.12 lattice_0.21-9 [9] hms_1.1.3 digest_0.6.33 magrittr_2.0.3 timechange_0.2.0 [13] evaluate_0.23 grid_4.3.1 bookdown_0.36 fastmap_1.1.1 [17] jsonlite_1.8.7 Matrix_1.6-1.1 formatR_1.14 fansi_1.0.5 [21] scales_1.2.1 jquerylib_0.1.4 cli_3.6.1 rlang_1.1.1 [25] munsell_0.5.0 withr_2.5.2 cachem_1.0.8 yaml_2.3.7 [29] tools_4.3.1 tzdb_0.4.0 colorspace_2.1-0 vctrs_0.6.4 [33] R6_2.5.1 png_0.1-8 lifecycle_1.0.3 pkgconfig_2.0.3 [37] bslib_0.5.1 pillar_1.9.0 gtable_0.3.4 glue_1.6.2 [41] Rcpp_1.0.11 highr_0.10 xfun_0.41 tidyselect_1.2.0 [45] rstudioapi_0.15.0 knitr_1.45 farver_2.1.1 htmltools_0.5.7 [49] labeling_0.4.3 rmarkdown_2.25 compiler_4.3.1 2.4 The Interface of Rstudio When you start Rstudio, you will see an interface as follows: Figure 2.1: Rstudio Interface Rstudio Interface: Editor: You create and edit R-related files here (e.g., *.r, *.Rmd etc.) Console: This is the R engine, which runs the codes we send out either from the R-script file or directly from the console input Output: You can view graphic outputs here The R console is like a calculator. You can type any R code in the console after the prompt &gt; and run the code line by line by pressing enter. 1 + 1 [1] 2 log(10) [1] 2.302585 1:5 [1] 1 2 3 4 5 Or alternatively, we can create an R script in Rstudio and write down lines of R codes to be passed to the R console. This way, we can run the whole script all at once. This is the idea of writing a program. In the above example (Figure 2.1), I wrote a few lines of codes in a R script file (cf. the Editor frame) and asked R to run these lines of codes in the R Console. And the graphic output of the R script was printed in the Output frame. Exercise 2.1 Please create a new R script in Rstudio. You may name the script as “ch-2-NAME.R” (Please use your own name). Please write the following codes in the script and pass the whole script to the R Console. scores &lt;- rnorm(1000, mean = 75, sd = 5.8) plot(density(scores)) hist(scores) boxplot(scores) Exercise 2.2 Find the answer to the following mathematical calculation with R. \\(2^{2+1}-4+64^{(-2)^{2.25-\\frac{1}{4}}}\\) = 16777220 2.5 Assignment R works with objects of many different classes, some of which are defined in the base R while others are defined by specific libraries/environments/users. You can assign any object created in R to a variable name using &lt;-: x &lt;- 5 y &lt;- &quot;wonderful&quot; Now the objects are stored in the variables. You can print out the variables by either making use of the auto-printing (i.e., the variable name itself auto-prints its content) or print(): x [1] 5 print(x) [1] 5 y [1] &quot;wonderful&quot; print(y) [1] &quot;wonderful&quot; 2.6 Data Structure In R, the most primitive object is a vector. There are three types of primitive vectors: (a) numeric, (b) character, and (c) Boolean vectors. In our previous examples, x is a numeric vector of one element; y is a character vector of one element. The following code shows an example of a Boolean vector z. z &lt;- TRUE z [1] TRUE All elements in the vector have to be of the same data type. The vectors we’ve created so far are vectors of only ONE ELEMENT. You use c() to create a vector of multiple elements. Within the parenthesis, you concatenate each element of the vector by ,: x2 &lt;- c(1, 2, 3, 4, 5, 6) x2 [1] 1 2 3 4 5 6 y2 &lt;- c(&quot;wonderful&quot;, &quot;excellent&quot;, &quot;brilliant&quot;) y2 [1] &quot;wonderful&quot; &quot;excellent&quot; &quot;brilliant&quot; z2 &lt;- c(TRUE, FALSE, TRUE) z2 [1] TRUE FALSE TRUE Other data structures that we often work with include: List: a vector-like structure, but can consist of elements of different data types Matrix: a two-dimensional vector, where all elements have to be of the same data type Data Frame: a spreadsheet-like table, where columns can be of different data types ex_list &lt;- list(&quot;First element&quot;, 5:10, TRUE) print(ex_list) [[1]] [1] &quot;First element&quot; [[2]] [1] 5 6 7 8 9 10 [[3]] [1] TRUE ex_array &lt;- matrix(c(1,5,6,3,8,19),byrow = T, nrow = 2) ex_array [,1] [,2] [,3] [1,] 1 5 6 [2,] 3 8 19 ex_df &lt;- data.frame( WORD = c(&quot;the&quot;, &quot;boy&quot;, &quot;you&quot;,&quot;him&quot;), POS = c(&quot;ART&quot;,&quot;N&quot;,&quot;PRO&quot;,&quot;PRO&quot;), FREQ = c(1104,35, 104, 34) ) ex_df The following graph shows you an intuitive understanding of the data structures in R. We will discuss more on data structures in Chapter 4. 2.7 Function Function is also an object class. There are many functions pre-defined in the R-base libraries. class(c) [1] &quot;function&quot; class(vector) [1] &quot;function&quot; class(print) [1] &quot;function&quot; To instruct R to do things more precisely, a function call usually has many parameters to specify. Take the earlier function matrix() for example. It is a pre-defined function in the R base library. ex_array &lt;- matrix(c(1,5,6,3,8,19),byrow = T, nrow = 2) ex_array [,1] [,2] [,3] [1,] 1 5 6 [2,] 3 8 19 When creating a matrix, we specify the values for the parameters, byrow = and nrow =. These specifications provide clues for R to create a matrix with N rows and arrange the numbers by rows. The actual values of the parameters that we use, i.e., T and 2, are referred to as arguments. Parameter is a variable in the declaration of function. Argument is the actual value of this variable that gets passed to function. Most importantly, we can define our own function, which is tailored to perform specific tasks. All self-created functions need to be defined first in the R environment before you can call them. Define own functions: print_out_user_name &lt;- function(name = &quot;&quot;){ cat(&quot;The current username is: &quot;, name, &quot;\\n&quot;) } Call own functions: print_out_user_name(name = &quot;Alvin Cheng-Hsien Chen&quot;) The current username is: Alvin Cheng-Hsien Chen print_out_user_name(name = &quot;Ada Lovelace&quot;) The current username is: Ada Lovelace Exercise 2.3 Please define a function called make_students_happy(), which takes a multi-element numeric vector, and returns also a numeric vector, with the value of each element to be the square root of the original value multiplied by 10. student_current_scores &lt;- c(20, 34, 60, 87, 100) make_students_happy(old_scores = student_current_scores) [1] 44.72136 58.30952 77.45967 93.27379 100.00000 2.8 Vectorization Many operations in R are vectorized, meaning that operations occur in parallel in certain R objects. This allows you to write code that is efficient, concise, and easier to read than in non-vectorized languages. The simplest example of vectorized functions is when adding two vectors together. x &lt;- 1:4 y &lt;- 6:9 z &lt;- x + y z [1] 7 9 11 13 Without vectorization, you may need to do the element-wise vector adding as follows: z &lt;- numeric(length = length(x)) for(i in 1:length(x)){ z[i] &lt;- x[i]+y[i] } # endfor z [1] 7 9 11 13 Other common vectorized functions include: x &gt;= 5 [1] FALSE FALSE FALSE FALSE x &lt; 2 [1] TRUE FALSE FALSE FALSE y == 8 [1] FALSE FALSE TRUE FALSE For more information on vectorization, please watch the following YouTube clip from Roger Peng. 2.9 Script In our earlier demonstrations, we ran R codes by entering each procedure line by line. We can create one script file with the Editor of Rstudio and include all our R codes in the file, which usually has the file extension of .R. And then we can run all commands included in the whole script all at once in the Rstudio (i.e., sending everything in the script file to the R console). First you open the *.R script file in Rstudio, which should appear in the Editor frame of the Rstudio. To run the whole script from start to the end, select all lines in the script file and press ctrl/cmd + shift + enter. To run a particular line of the script, put your mouse in the line and press ctrl/cmd + enter. 2.10 Library R, like other programming languages, comes with a huge database of packages and extensions, allowing us to do many different tasks without worrying about writing the codes all from the scratch. In CRAN Task Views, you can find specific packages that you need for particular topics/tasks. To install a package (i.e., library): install.packages(&quot;tidyverse&quot;) install.packages(c(&quot;ggplot2&quot;, &quot;devtools&quot;, &quot;dplyr&quot;)) In this course, I would like to recommend all of you to install the package tidyverse, which is a bundle including several useful packages for data analysis. During the installation, if you are asked about whether to install the package from source, please enter yes (See below for more detail). During the R package installation, if you see messages like installation of package XXX had non-zero exit status, this indicates that the package has NOT been properly installed in your R environment. That is, something is WRONG (See below as well). You need to figure out a way to solve the issues indicated in the error messages so that you can successfully install the package in your R system. Before you install R packages from source, you need to install a few R tools for your operating system. These tools are necessary for you to build the R packages from the source. For MacOS Catalina users, please install the following applications on your own. They are necessary for building R packages from source. Command Line Tools for Xcode 11.X You may need to log in with your Apple ID and find the download page. clang7/8 from https://cran.r-project.org/bin/macosx/tools/ gfortran6.1 from https://cran.r-project.org/bin/macosx/tools/ For Windows users, please install Rtools from CRAN (Please install the version according to your R version). After you install all the above source-building tools, you can now install the package tidyverse from source. Please install the package from the source. This step is very important because some of the dependent packages require you to do so. However, for the other packages, I would still recommend you to install the packages in a normal way, i.e., installing NOT from source, but from the compiled version on CRAN. 2.11 Setting Always set your default encoding of the files to UTF-8: 2.12 Seeking Help In the process of (learning) programming, one thing you will never be able to dodge is feeling desperate for help. Here are some useful sources from which you may get additional assistance. Within Rstudio, in the R console, you can always use ? to check the documentation of a particular function (cf. Figure 2.2). When you run the command, you will see the documentation popping up in the output frame of the Rstudio. ?log ?read.table Figure 2.2: Help 1 If you need help from others, the first step is to create a reproducible example. The goal of a reproducible example is to package your problematic code in such a way that other people can run it and feel your pain. Then, hopefully, they can provide a solution and put you out of your misery. Figure 2.3: Help 2 So before you seek help from others (or before you yell at others for help, cf. Figure 2.3) : First, you need to make your code reproducible. This means that you need to capture everything, i.e., include any library() calls and create all necessary objects (e.g., files). The easiest way is to check the objects listed in the Environment tab of the Rstudio and identify objects that are relevant to your problematic code chunk. Second, you need to make it minimal. Strip away everything that is not directly related to your problem. This usually involves creating a much smaller and simpler R object than the one you’re facing in real life or even using built-in data. That sounds like a lot of work! And it can be, but it has a great payoff: 80% of the time creating an excellent reproducible example reveals the source of your problem. It’s amazing how often the process of writing up a self-contained and minimal example allows you to answer your own question. The other 20% of time you will have captured the essence of your problem in a way that is easy for others to play with. This substantially improves your chances of getting help! The following is a list of resources where people usually get external assistance quickly: http://www.r-project.org/mail.html http://stackoverflow.com/ Quick R: http://www.statmethods.net/ R CRAN Task Views: https://cran.r-project.org/web/views/ R for Data Science (Second Edition) Text Mining with R R communities: R-Bloggers: https://www.r-bloggers.com/ kaggle: https://www.kaggle.com/ stackoverflow: https://stackoverflow.com/questions/tagged/r rstudio: https://community.rstudio.com/ 2.13 Language Learning Ain’t Easy! Learning R is like learning another foreign language. It is a long journey. You can’t expect yourself to learn all the vocabulary of the new language in one day. Also, you will forget things you learn all the time. Everyone’s been there. When your script does not work as expected, don’t be frustrated. Take a break and resume later. What I can say is that: it is always NORMAL to debug a script for hours or even days via endless searches on Google. That being said, here I would like to share with you some of the most common problems we may run into: You created an R script file (*.r) and opened it in the Rstudio, but the script didn’t work simply because you didn’t execute the script in R console (i.e., you didn’t send the script to R console.) If you get an error message, saying \"object not found\", check the object name again and see if you have mistyped the name of the object. If not, check your current environment and see if you have forgot to execute some assignment commands in the previous parts of the script (i.e., the object has NOT even been created yet). If you get an error message, saying \"function not found\", check the function and see if you have the correct name. Or more often, check if you have properly loaded the necessary libraries where the function is defined. To understand the meaning of the error messages is crucial to your development of R proficiency. To achieve this, you have to know very well every object name you have created in your script (as well as in your environment). For example: What type of object is it? (i.e., the class of the object, e.g., vector, list, data.frame?) For primitive vectors, what data type does the vector belong to? (e.g., numeric, character, boolean,factor?) What is the dimensionality of the object? (nrows, ncols?) Sometimes the script fails simply because of the obvious syntactic errors. Pay attention to all the punctuations in every R command. They are far more important (or lethal) than you think. They include: ,: commas between arguments inside a function \": quotes for strings/characters (): parentheses for functions {}: curly brackets for control structures From my experiences, about 80 percent of the errors may in the end boil down to a simple typo. No kidding. Copy-and-paste helps. DO NOT assume that your R script always works as intended! Always keep two questions in mind: Did R produce the intended result? What is included in the R object name? 2.14 Keyboard Shortcuts The best way to talk to a computer is via the keyboard. Scripting requires a lot of typing. Keyboard shortcuts may save you a lot of time. Here are some of the handy shortcuts: Crtl/Command + Enter: run the current line (send from the script to the console) Crtl/Command + A: select all Crtl/Command + C: copy Ctrl/Command + X: cut Ctrl/Command + V: paste Ctrl/Command + Z: undo (Mac) Alt/Option + Left/Right: move cursor by a word (Windows) Ctrl + Left/Right: move cursor by a word (Mac) Command + Left/Right: move cursor to the beginning/end of line (Windows) Home/End: move cursor to the beginning/end of line (Mac) Command + Tab: switch in-between different working windows/apps Ctrl/Command + S: save file Command + Shift + C: comment/uncomment selected lines Exercise 2.4 Make yourself familiar with the iris data set, which is included in R. Exercise 2.5 Use ? to make youself familiar with the following commands: str,summary, dim, colnames, names, nrow, ncol, head, and tail. What information can you get with all these commands? Exercise 2.6 Write a function to compute the factorial of a non-negative integer, x, expressed as x!. The factorial of x refers to x multiplied by the product of all integers less than x, down to 1. For example, 3! = 3 x 2 x 1 = 6. The special case, zero factorial is always defined as 1. Confirm that your function produce the same results as below: 5! = 120 120! = 6.689503e+198 0! = 1 # A Sample Format for your Function myfac &lt;- function(x){ } ##(i) myfac(5) [1] 120 ##(ii) myfac(120) [1] 6.689503e+198 ##(iii) myfac(0) [1] 1 "],["code-format-convetion.html", "Chapter 3 Code Format Convention 3.1 Assignment &lt;- 3.2 Comment # 3.3 Script Naming 3.4 Object Naming 3.5 Whitespace 3.6 Indention and Linebreaks 3.7 More References 3.8 Template for Script Assignments", " Chapter 3 Code Format Convention Like the first time we learn English writing, we need to know the conventional writing styles and formats in coding as well. This is very important because scripts of good formats would increase their readability. This would save us a lot of time in case of future debugging and maintenance. This chapter will discuss common practices among most R users. 3.1 Assignment &lt;- In R, people normally use &lt;- to assign values to object names. In other languages such as Python, people often use =. Although R still understands the value-assignment when you use =, I would still suggest using &lt;- to avoid the chance of confusing your R kernel. x1 &lt;- &quot;This is a sentence.&quot; ## recommended x2 = &quot;This is a sentence.&quot; ## not recommended x1 [1] &quot;This is a sentence.&quot; x2 [1] &quot;This is a sentence.&quot; 3.2 Comment # When you write codes, you would need to commit your code extensively. This is very important because we often forget why and how we write it this way. Each line of your R script can include comments by placing any strings after the # character. These comments are ignored by R and are not processed. To improve the readability of your code, you can use additional symbol characters (e.g., - and =) after the # to separate different code chunks. # ==================== # Variable Assignment # ==================== x &lt;- &quot;This is a sentence&quot; # ==================== # Variable Printing # ==================== x [1] &quot;This is a sentence&quot; 3.3 Script Naming When naming your R script files, it is important to use meaningful strings that are composed of alphanumeric characters only. It is not advisable to use Chinese characters. For multi-word names, it is suggested to connect words using a hyphen - to improve readability and avoid any potential confusion. It is important to avoid being overly creative when naming your files to ensure that they are easily identifiable and understood by others. # Recommended my-first-script.R my-first-assignment.R # NOT Recommended my first script.R 語料庫assingment1.R 3.4 Object Naming As you work on your script, you’ll be creating numerous objects. It’s important to take some time to carefully consider how you name each of these objects. Choose names that are both intuitive and meaningful. While it’s tempting to keep names simple for the sake of ease in typing, it’s important to prioritize clarity and ease of understanding. Here are some principles: Use nouns for the object names (e.g., PTT_corpus) Use verbs for the function names (e.g., generate_ngrams()) Connect multiword names with _ (e.g., PTT_corpus_segmented) Avoid using characters/strings that have been used by R (e.g., vector, c, mean, sum, T etc.) 3.5 Whitespace For operators (i.e., =, +, -, &lt;-), they are usually embraced by white spaces, which would make your script easier to read: ## Recommended grade_average &lt;- mean(midterm * 0.5 + final * 0.5) ## NOT Recommended grade_average&lt;-mean(midterm*0.5+final*0.5) For : and ::, usually we do not put whitespaces around them: # Recommended x &lt;- c(1:10) tidyr::separate() # NOT Recommended x &lt;- c(1 : 10) tidyr :: separate() : is an expression in R to create a sequence of numbers. For example, c(1:10) is the same as c(1,2,3,4,5,6,7,8,9,10). :: is an expression to access a particular object/function from a library without having the entire library loaded in your current R environment. For example, tidyr::separate() calls the function separate() from the library tidyr but the other objects in tidyr are still NOT included. You cannot use the other objects defined in tidyr. When there are two functions with the same name from different libraries in your current R session, it is necessary to explicitly specify the package name in your function call to avoid confusion and ensure the intended function is called. For parentheses (, if it is in the control structure, we usually put a whitespace before the initial (: # for-loop for (i in 1:10) { print(i) } [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 [1] 7 [1] 8 [1] 9 [1] 10 # if-conditional x &lt;- 2 if (x == 1) { print(&quot;The answer is 1!&quot;) } else { print(&quot;The answer is greater than 1!&quot;) } [1] &quot;The answer is greater than 1!&quot; But if the parenthesis is in the function call (i.e., where we specify the arguments of the parameters), we don’t put a whitespace before the initial (: mean(x) ggplot(aes(x = money, y = achievement)) For curly brackets, we usually put a line break after the initial { and the ending } should be one single line. Also, as sometimes you would embed many different control structures at the same time, leading to many ending } lines, it is always good to commit properly which ending } goes with which control structure. ## Embeded Control Structures for (i in 1:10) { if (i &lt; 5) { print(i) } else { print(i + 10) } #endif } #endfor [1] 1 [1] 2 [1] 3 [1] 4 [1] 15 [1] 16 [1] 17 [1] 18 [1] 19 [1] 20 In programming, we often need to do the same task multiple times. This can be time-consuming and inefficient if we have to write out each step every time. Instead, we can use a for-loop to tell the computer to repeat a certain task for a specific number of times. For example, if we want to print out the numbers 1 to 10, we could use a for-loop to tell the computer to repeat the process of printing out each number until it gets to 10. A for-loop works by defining a starting value, an ending value, and a step size. The computer then repeats the task for each value in between the starting and ending values, incrementing by the step size each time. Control structures are another important concept in programming. They allow us to control the flow of our program based on certain conditions. For example, if we want to run a certain task only if a certain condition is met, we can use an if-statement. If the condition is true, the computer will execute the task, and if the condition is false, the computer will skip over that task and move on to the next one. Control structures also allow us to repeat tasks until a certain condition is met. This is called a while-loop. The computer will repeat the task until the condition is no longer true. We talk more about these constructs in Ch 5 Conditions and Loops. 3.6 Indention and Linebreaks R does not care about line breaks, white spaces, or tabs in your R script. But these formatting characters are important because you need all these characters to help you quickly keep track of the script’s structure. Make good use of the indention to increase the readability of your script. long_function_name &lt;- function(a = &quot;a long argument&quot;, b = &quot;another argument&quot;, c = &quot;another long argument&quot;) { # As usual code is indented by two spaces. } y &lt;- matrix(data = c(2, 5, 7, 8), # data source nrow = 2, # two rows byrow = TRUE) # filling values by row y [,1] [,2] [1,] 2 5 [2,] 7 8 3.7 More References Readability of your code is an art. Please consult the following recommended readings if you are interested in more principles of clean code. 3.8 Template for Script Assignments When you submit your R scripts, please follow the format specified below. Important notes include: Please include the practice codes discussed in each chapter. Please specify the start and end of each of your exercise solution. Please indicate very clearly the chapter number and title as well as the exercise number in your script. Please name your R script as follows: ch-2-alvin.R, ch-3-alvin.R Please provide your descriptions/explanations in comments # (when you are asked to get familiar with packages, functions, or data sets). Finally, in case that you still don’t know where to find the exercises for assignments, please look for the exercise green box in each chapter (see below). All exercises are numbered. Exercise 3.1 This is a demo of the Exercise Box you would look for in each chapter. "],["subsetting.html", "Chapter 4 Subsetting 4.1 Vector 4.2 Factor 4.3 List 4.4 Data Frame 4.5 Tibble (Self-Study)", " Chapter 4 Subsetting Subsetting is a crucial skill in data analysis. It involves selecting a specific subset of elements from a data structure such as a vector, matrix, data frame, or list. In Chapter 2, we provided a brief overview of data structures in R. In this section, we will delve deeper into each type of data structure and explore different techniques for subsetting them. By mastering subsetting, you will be able to efficiently extract the necessary information from your data for analysis and visualization. In particular, we will look at three main topics: Subsetting: Subsetting refers to extracting specific elements from a larger data structure, such as a vector or a data frame, based on their position or label. Conditional Subsetting: Conditional subsetting involves extracting elements from a data structure based on some specific conditions or criteria. Sorting: Sorting involves ordering the elements of a data structure, such as a vector or a data frame, in a specific way based on their values. 4.1 Vector Subsetting As we have shown in Chapter 2 R Fundamentals, there are three types of primitive vectors in R: Character vectors Numeric vectors Boolean vectors You can access a particular subset of a vector by using [ ] right after the object name. Within the [], you can make use of at least three types of indices: Subsetting with numeric indices char.vec &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;,&quot;four&quot;, &quot;five&quot;) char.vec[1] [1] &quot;one&quot; You can also retrieve several elements from a vector all at once, using a numeric vector as the indices, c(), in the []: ## method 1 which.to.extract &lt;- c(1, 4) char.vec[which.to.extract] [1] &quot;one&quot; &quot;four&quot; ## method 2 char.vec[c(1, 4)] [1] &quot;one&quot; &quot;four&quot; Subsetting with Boolean indices You can also use a Boolean vector as the index: ## method 1 whether.to.extract &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE) char.vec[whether.to.extract] [1] &quot;one&quot; &quot;three&quot; ## method 2 char.vec[c(T, F, T, F, F)] [1] &quot;one&quot; &quot;three&quot; Subsetting with negative numeric indices If you use negative integers as indices within the square brackets, R will return a new vector with the elements at those indices removed. In other words, negative indexing allows you to exclude certain elements from the original vector. This is a useful technique for filtering out specific values or observations that you don’t need for a particular analysis or operation. char.vec[-2] [1] &quot;one&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; However, please note that the original vector is still the same in length: char.vec [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; If you want to save the shortened/filtered vector, there are at least two alternatives: Assign the shortened vector to a new object name; Assign the shortened vector to the old object name. ## Alternative (a) char.vec.short &lt;- char.vec[-2] char.vec.short [1] &quot;one&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; char.vec [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; ## Alternative (b) char.vec &lt;- char.vec[-2] char.vec [1] &quot;one&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; For the two alternatives, which one would be better? Why? The indices in R start with one, meaning that the first element of a vector or the first row/column of a matrix is indexed as one. This is different from some other programming languages, such as Python, which use zero-based indexing, meaning that the first element of a vector or the first row/column of a matrix is indexed as zero. It is important to be aware of this difference when working with data in R, especially if you are coming from a programming background that uses zero-based indexing. Using the wrong index can lead to unexpected results or errors in your code. Conditional Subsetting In R, we often need to extract specific elements from a data structure that meet certain conditions. We can achieve this by using conditional subsetting. One useful function for conditional subsetting is which(). which() returns the indices of the elements in a vector, matrix, or array that satisfy a certain condition. The condition can be specified using a logical expression, which returns a logical vector with TRUE for the elements that meet the condition and FALSE for those that do not. For example, suppose we have a vector x that contains some values and we want to extract the indices of all the elements that are greater than 5. We can do this using the which() function in the following way: x &lt;- c(3, 7, 2, 9, 6, 1) indices &lt;- which(x &gt; 5) Here, which(x &gt; 5) returns the indices of the elements in x that are greater than 5, which are 2, 4, and 5. Therefore, indices contains the vector 2 4 5. Once we have the indices of the elements that satisfy the condition, we can use them to subset the original data structure. For example, we can extract the elements of x that are greater than 5 as follows: ## method 1 x[indices] [1] 7 9 6 ## method 2 x[which(x &gt; 5)] [1] 7 9 6 You may use different logical operators to check each element of the vector according to some criteria. R will decide whether elements of the vector satisfy the condition given in the logical expression and return a Boolean vector of the same length. Common logical operators include: ==: equal to &amp;: and |: or &gt;: greater than &gt;=: greater than or equal to &lt;: less than &lt;=: less than or equal to !=: not equal This can be very useful for vector conditional subsetting: num.vec &lt;- c(1:20) num.vec &gt; 10 [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [13] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE num.vec[num.vec &gt; 10] [1] 11 12 13 14 15 16 17 18 19 20 num.vec[num.vec != 10] [1] 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 19 20 We can also combine conditions using logical operators like &amp; (and) and | (or). For example, if we want to check whether elements in a vector are greater than 18 or less than 2, we can use the | (or) operator to combine two conditions like this: num.vec[num.vec &gt; 18 | num.vec &lt; 2] [1] 1 19 20 Exercise 4.1 Use conditional subsetting to subset the strings in char.vec that meet the following conditions. Specifically, subset strings in char.vec that belong to either “one” or “three” using conditional subsetting. Your task is to find the right target.indices. char.vec &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;) char.vec[target.indices] [1] &quot;one&quot; &quot;two&quot; Exercise 4.2 Use the sample() function to create a numeric vector, named num.vec, of length 10, containing 10 different integers randomly selected from the range 1 to 100. Then use conditional subsetting to subset only even numbers from the numeric vector. Your job is to find the right target.indices. ## random sample integers num.vec [1] 31 79 51 14 67 42 50 43 97 25 ## conditional subsetting num.vec[target.indices] [1] 14 42 50 Sorting Sorting elements from a data structure is a common task when working with data. The sort() and order() functions are useful for sorting elements in ascending or descending order based on their values. The sort() function is used to sort elements in a data structure, such as a vector, matrix, or data frame. It arranges the elements in increasing or decreasing order, based on the values. The order() function is similar to sort(), but it returns the indices of the sorted elements, rather than the sorted elements themselves. This can be useful when we want to rearrange a data structure based on the order of its elements. The order() function also has optional arguments that can be used to customize the sorting process, such as specifying the sorting order and the way ties are handled. # Example vector vec &lt;- c(7, 1, 5, 3, 9) # Using sort() sort(vec) [1] 1 3 5 7 9 # Using order() order(vec) [1] 2 4 3 1 5 vec[order(vec)] [1] 1 3 5 7 9 First, we used sort() to sort the elements of vec in ascending order. The resulting vector has the same elements as vec, but they are sorted from smallest to largest. In the second example, we used order() to obtain the index positions that would sort vec in ascending order. We then used these index positions to subset the elements of vec in the desired order. This achieves the same result as using sort() directly. Exercise 4.3 Create a vector, named m, which includes the lowercase letters from a to j in an alphabetical order. hint: You can do this by using the letters pre-loaded R object to create a vector of all lowercase letters and then selecting the subset of letters from a to j using the [ ] notation. m [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; Exercise 4.4 Create a vector n containing the same letters as m, but in a random order. Create another vector, named n, which includes small letters from a to j (in a random order). hint: You can use the sample() function for random sampling. Also, your results may vary because of the random ordering of the letters. n [1] &quot;e&quot; &quot;h&quot; &quot;d&quot; &quot;i&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;g&quot; &quot;f&quot; &quot;j&quot; Exercise 4.5 Determine whether the letters in n (i.e., the one with the random order) are in the same positions as they are in m (i.e., the one with the alphabetical order). First, count how many letters are in the same position in n as they are in m; second, identify which letters are in the same position in both vectors. hint: check table() FALSE TRUE 9 1 [1] &quot;j&quot; 4.2 Factor In Chapter 2, we discussed several data structures in R, but one important structure that we did not cover is the factor. Factors are similar to vectors, but with an important difference: they have a limited number of possible values, called levels, which can be thought of as categories or groups. Factors are often used in statistical analysis as grouping variables, to separate observations into different sub-groups. In this section, we will explore the basics of factors and how to work with them in R. We usually create a factor from a numeric or character vector. To create a factor, use factor(): sbj_gender_num &lt;- c(1, 0, 0, 1, 1, 0, 1) sbj_gender_num [1] 1 0 0 1 1 0 1 sbj_gender_num_fac &lt;- factor(x = sbj_gender_num) sbj_gender_num_fac [1] 1 0 0 1 1 0 1 Levels: 0 1 But please note that the numbers that you see in a factor do not represent numeric values. Instead, they are labels in the form of digits. sbj_gender_char &lt;- c(&quot;female&quot;,&quot;male&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;,&quot;male&quot;,&quot;female&quot;) sbj_gender_char [1] &quot;female&quot; &quot;male&quot; &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;male&quot; &quot;female&quot; sbj_gender_char_fac &lt;- factor(x = sbj_gender_char) sbj_gender_char_fac [1] female male male female female male female Levels: female male For a factor, the most important information is its levels, i.e., the limited set of all possible values this factor can take. We can extract the levels as a vector of character strings using levels(): levels(sbj_gender_num_fac) [1] &quot;0&quot; &quot;1&quot; levels(sbj_gender_char_fac) [1] &quot;female&quot; &quot;male&quot; When do we need a factor? In data annotation, we often use arbitrary numbers as labels for certain categorical variables. For example, we may use arbitrary numbers from 1 to 4 to label learners’ varying proficiency levels: 1 = beginners, 2 = low-intermediate, 3 = upper-intermediate, 4 = advanced. When we load the data into R, R may first treat the data as a numeric vector: sbj_prof_num&lt;- c(1, 2, 4, 4, 2, 3, 3, 1, 1) sbj_prof_num [1] 1 2 4 4 2 3 3 1 1 However, these numbers may be confusing: R may perform operations that make sense on numbers but may not make sense on categorical variables. For example, it doesn’t make sense to calculate the mean or the sum of categories such as “beginner”, “intermediate”, and “advanced”. They are not semantically transparent because numbers do not have meanings. In this case, we can convert the numeric vector into a factor and re-label these numeric values as categorical labels that are more semantically transparent. We can do this by setting more arguments in factor(), such as levels=..., labels=.... sbj_prof_fac &lt;- factor( x = sbj_prof_num, levels = c(1:4), labels = c(&quot;beginner&quot;, &quot;low-inter&quot;, &quot;upper-inter&quot;, &quot;advanced&quot;) ) sbj_prof_fac [1] beginner low-inter advanced advanced low-inter upper-inter [7] upper-inter beginner beginner Levels: beginner low-inter upper-inter advanced sbj_prof_num [1] 1 2 4 4 2 3 3 1 1 Compare the the auto-print outputs of sbj_prof_fac and sbj_prof_num and examine their differences. levels = ...: this argument specifies all possible values this factor can take labels = ...: this argument provides own intuitive labels for each level It should now therefore be clear that labels = ... is a good way for us to re-label any arbitrary annotations into meaningful labels. By using the levels and labels arguments, we can make the factors more intuitive and easier to understand, particularly if the original data uses arbitrary values or codes to represent categories. In addition, we can determine whether the ranking of the levels is meaningful. If the order of the factor’s levels is meaningful, we can set the argument ordered = TRUE: sbj_prof_fac_ordered &lt;- factor( x = sbj_prof_num, levels = c(1:4), labels = c(&quot;beginner&quot;, &quot;low-inter&quot;, &quot;upper-inter&quot;, &quot;advanced&quot;), ordered = T ) sbj_prof_fac_ordered [1] beginner low-inter advanced advanced low-inter upper-inter [7] upper-inter beginner beginner Levels: beginner &lt; low-inter &lt; upper-inter &lt; advanced Now from the R console we can see not only the levels of the factor but also the signs &lt;, indicating their order. Using this ordered factor, we can perform relational comparison: sbj_prof_fac_ordered[1] [1] beginner Levels: beginner &lt; low-inter &lt; upper-inter &lt; advanced sbj_prof_fac_ordered[4] [1] advanced Levels: beginner &lt; low-inter &lt; upper-inter &lt; advanced sbj_prof_fac_ordered[1] &lt; sbj_prof_fac_ordered[4] [1] TRUE But we cannot do the comparison for unordered factors (characters neither): sbj_prof_fac[1] [1] beginner Levels: beginner low-inter upper-inter advanced sbj_prof_fac[4] [1] advanced Levels: beginner low-inter upper-inter advanced sbj_prof_fac[1] &lt; sbj_prof_fac[4] Warning in Ops.factor(sbj_prof_fac[1], sbj_prof_fac[4]): &#39;&lt;&#39; not meaningful for factors [1] NA The difference between vector and factor may look trivial for the moment but they are statistically very crucial. The choice of whether to instruct R to treat a vector as a factor, or even an ordered factor, will have important consequences in the implementation of many statistical methods, such as regression or other generalized linear modeling. Rule of thumb: Always pay attention to what kind of object class you are dealing with:) 4.3 List A List is like a vector, which is a one-dimensional data structure. However, the main difference is that a List can include a series of objects of different classes: # A list consists of # (i) numeric vector, # (ii) character vector, # (iii) Boolean vector list.example &lt;- list( &quot;one&quot; = c(1, 2, 3), &quot;two&quot; = c(&quot;Joe&quot;, &quot;Mary&quot;, &quot;John&quot;, &quot;Angela&quot;), &quot;three&quot; = c(TRUE, TRUE) ) list.example $one [1] 1 2 3 $two [1] &quot;Joe&quot; &quot;Mary&quot; &quot;John&quot; &quot;Angela&quot; $three [1] TRUE TRUE Please note that not only the class of each object in the List does not have to be the same; the length of each list element may also vary. You can subset a List in two ways: [...]: This always returns a List back [[...]]: This returns the object of the List element, which is NOT NECESSARILY a List list.example[1] $one [1] 1 2 3 list.example[[1]] [1] 1 2 3 list.example[[&quot;one&quot;]] [1] 1 2 3 We can also subset a List by the names of its elements. Before you try the following codes in the R console, could you first predict the outputs? ind &lt;- c(&quot;one&quot;, &quot;three&quot;) list.example[ind] list.example[[ind]] Exercise 4.6 Create a list that contains, in this order: a sequence of 20 evenly spaced numbers between -4 and 4; (hint: check seq()) a 3 x 3 matrix of the logical vector c(F,T,T,T,F,T,T,F,F) filled column-wise; a character vector with the two strings “don”, and “quixote”; a factor containing the observations c(\"LOW\",\"MID\",\"LOW\",\"MID\",\"MID\",\"HIGH\"). [[1]] [1] -4.0000000 -3.5789474 -3.1578947 -2.7368421 -2.3157895 -1.8947368 [7] -1.4736842 -1.0526316 -0.6315789 -0.2105263 0.2105263 0.6315789 [13] 1.0526316 1.4736842 1.8947368 2.3157895 2.7368421 3.1578947 [19] 3.5789474 4.0000000 [[2]] [,1] [,2] [,3] [1,] FALSE TRUE TRUE [2,] TRUE FALSE FALSE [3,] TRUE TRUE FALSE [[3]] [1] &quot;don&quot; &quot;quixote&quot; [[4]] [1] LOW MID LOW MID MID HIGH Levels: HIGH LOW MID Exercise 4.7 Based on Exercise 4.6, extract row elements 2 and 1 of columns 2 and 3, in that order, of the logical matrix. [,1] [,2] [1,] FALSE FALSE [2,] TRUE TRUE Exercise 4.8 Based on Exercise 4.6, obtain all values from the sequence between -4 and 4 that are greater than 1. [1] 1.052632 1.473684 1.894737 2.315789 2.736842 3.157895 3.578947 4.000000 Exercise 4.9 Make yourself familiar with the function which(). Based on Exercise 4.6, use which(), to determine which indices in the factor are assigned the “MID” level. [1] 2 4 5 4.4 Data Frame Subsetting A data.frame is the most frequently used object that we will work with in data analysis. It is a typical two-dimensional spreadsheet-like table. Normally, the rows are the subjects or tokens we are analyzing; the columns are the variables or factors we are interested in. We can also use [... , ... ] to subset a data frame. The indices in [... , ...] are Row-by-Column. ex_df &lt;- data.frame( WORD = c(&quot;the&quot;, &quot;boy&quot;, &quot;you&quot;,&quot;him&quot;), POS = c(&quot;ART&quot;,&quot;N&quot;,&quot;PRO&quot;,&quot;PRO&quot;), FREQ = c(1104,35, 104, 34) ) ex_df You can subset a particular row of the data frame: ex_df[1, ] ex_df[c(1, 3), ] You can subset a particular column of the data frame: ex_df[, 1] [1] &quot;the&quot; &quot;boy&quot; &quot;you&quot; &quot;him&quot; ex_df[, c(1, 3)] ex_df[, c(&quot;WORD&quot;, &quot;FREQ&quot;)] Please compare the following two ways of accessing a column from the data frame. Can you tell the differences in the returned results? ex_df[, c(&quot;FREQ&quot;)] [1] 1104 35 104 34 ex_df[, c(&quot;FREQ&quot;), drop = FALSE] Conditional Subsetting We can also make use of which() to perform conditional subsetting. Can you subset rows whose FREQ &lt; 50? Can you subset rows whose POS are either PRO or N? Sorting We can also make use of order() to sort the data frame into a specific order based on the values of the columns. Can you sort the data frame ex_df based on the values of the column WORD, arranging the rows in descending alphabetical order?” Can you sort the data frame ex_df based on the values of the column FREQ in descending order. Exercise 4.10 Create and store the following data frame as dframe in your R workspace. person should be a character vector sex should be a factor with levels F and M funny should be a factor with levels Low, Mid, and High Exercise 4.11 Stan and Francine are 41 years old, Steve is 15, Hayley is 21, and Klaus is 60. Roger is extremely old–1,600 years. Following Exercise 4.10, append these data as a new numeric column variable in dframe called age. Exercise 4.12 Following Exercise 4.11, write a single line of code that will extract from dframe just the names and ages of any records where the individual is male and has a level of funniness equal to Low OR Mid. 4.5 Tibble (Self-Study) A tibble is a new data structure with lots of advantages. For the moment, we treat tibble and data.frame as the same structures, with the former being an augmented version of the latter. In fact, almost all functions that work with a data.frame are compatible with a tibble. Now the tibble is the major structure that R users work with under the tidy framework. If you are interested in the power of tibbles, the best place to start with is the chapter on Tibbles in R for data science. require(tibble) ex_tb &lt;- tibble( WORD = c(&quot;the&quot;, &quot;boy&quot;, &quot;you&quot;,&quot;him&quot;), POS = c(&quot;ART&quot;,&quot;N&quot;,&quot;PRO&quot;,&quot;PRO&quot;), FREQ = c(1104,35, 104, 34)) ex_tb There is another way to create a tibble. You can use tribble(), short for transposed tibble. tribble() is customized for data entry in code: column headings are defined by formulas (i.e. they start with ~); entries are separated by commas. This makes it possible to lay out small amounts of data in easy-to-read form. ex_tb_2 &lt;- tribble( ~WORD, ~POS, ~FREQ, #----|--------|------ &quot;the&quot;, &quot;ART&quot;, 1104, &quot;boy&quot;, &quot;N&quot;, 35, &quot;you&quot;, &quot;PRO&quot;, 104, &quot;him&quot;, &quot;PRO&quot;, 34 ) ex_tb_2 You can subset a tibble in exactly the same ways as you work with a data.frame: ex_tb[1,] ex_tb[,1] ex_tb[,c(1:3)] Exercise 4.13 Please compare again the following codes and see if you can tell the major differences between tibble and data.frame? ex_tb[,c(&quot;FREQ&quot;)] # indexing tibble ex_df[,c(&quot;FREQ&quot;)] # indexing data.frame [1] 1104 35 104 34 There are three major advantages with tibble() when compared with data.frame(): As of R 4.0.0, the default setting for stringsAsFactors is now FALSE by default. That is, both tibble and data.frame no longer convert all character vectors to factors by default When auto-printing the contents, tibble would only display the first ten rows, but data.frame would print out everything. This could be devastating! (Imagine that you have a table with hundreds of thousands rows.) The auto-printing of the tibble is a lot more informative, providing additional attributes of the tibble such as (a) row and column numbers and (b) data type of each column The parameter stringsAsFactors = ... in data.frame() specifies whether characters vectors are converted to factors. The factory-fresh default has been TRUE previously but has been changed to FALSE from R 4.0.0+. If you still would like to have this as default, you can revert by setting options(stringsAsFactors = TRUE). Exercise 4.14 Download the csv file, data-word-freq.csv from the DEMO_DATA Dropbox Drive and load the CSV data into R using two different functions: the default read.csv() and the read_csv from the readr package. Please discuss the differences of the objects loaded from these two methods. ## Please download the csv file from `DEMO_DATA` drive wf_df = read.csv( file = &#39;demo_data/data-word-freq.csv&#39;, stringsAsFactors = T) str(wf_df) &#39;data.frame&#39;: 3135 obs. of 3 variables: $ WORD : Factor w/ 2476 levels &quot;__add__&quot;,&quot;__dict__&quot;,..: 2213 1633 2213 1804 1510 171 82 82 1510 171 ... $ CORPUS: Factor w/ 2 levels &quot;perl&quot;,&quot;python&quot;: 1 1 2 2 1 1 1 2 2 2 ... $ FREQ : int 346 243 229 194 166 160 151 148 138 137 ... wf_df require(readr) ## you may need to install this package wf_tb = readr::read_csv( file = &#39;demo_data/data-word-freq.csv&#39;) wf_tb "],["conditions-and-loops.html", "Chapter 5 Conditions and Loops 5.1 Conditions 5.2 if Statements 5.3 for 5.4 while loop 5.5 Toy Example", " Chapter 5 Conditions and Loops In programming, controlling the flow of execution is an essential task. When you start to work on more sophisticated programs with R, you will often need to control the flow and order the execution in your code. This is where the concept of flow control comes in. There are two types of typical scenarios where flow control is used: Execute some parts of the code or skip others, based on certain criteria or conditions (i.e., an expression that evaluates to TRUE or FALSE) Repeat a particular code chunk a certain number of times, which is often referred to as loops. In this Chapter, we will explore these core programming techniques using: if-else statements for and while loops However, in Chapter 11, we will talk about loops more and point you to the idiomatic ways of dealing with loops in R (i.e., with Tidyverse syntax). 5.1 Conditions Before we talk about the flow controls, let’s deal with the more fundamental element of the control structures: what is a condition? To that end, let’s first explore a few important concepts: Boolean Values Comparison Operators Boolean Operators Boolean Values Unlike numbers or characters, the Boolean data type has only two values, TRUE and FALSE. In R, the Boolean values TRUE and FALSE lack the quotes you place around strings, and they are always uppercase. cond1 &lt;- TRUE class(cond1) [1] &quot;logical&quot; Comparison Operators Comparison Operators are important in programming. They compare two values and evaluate down to a single Boolean value. They are also referred to as relational operators. Comparison Operators in R Operator Meaning == Equal to != Not equal to &gt; Greater than &lt; Less than &gt;= Greater than or equal to &lt;= Less than or equal to X %in% Y X Is a member of Y | These comparison operators return TRUE or FALSE depending on the values we give them. 45 == 45 [1] TRUE 45 &gt; 50 [1] FALSE 45 != 4 [1] TRUE Please note that these operators work not only with numbers but also characters as well. a &lt;- &quot;run&quot; b &lt;- &quot;run&quot; c &lt;- &quot;walk&quot; all &lt;- c(&quot;run&quot;,&quot;walk&quot;,&quot;march&quot;) a == b [1] TRUE a == c [1] FALSE c %in% all [1] TRUE When using comparison operators, be careful of the data types (i.e., numeric or character). ## Create objects a &lt;- &quot;42&quot; ## char vec b &lt;- 42 ## num vec ## Comparison operations a == 42.0 [1] TRUE a &gt; 40 [1] TRUE a == b [1] TRUE Please note from the above example that R does implicit data type conversion, which means it automatically converts one data type to another as needed. This can occur in a variety of situations, such as when performing arithmetic operations on different types of data or when assigning a value of one data type to a variable of another data type. Please note the difference between == and =. The == operator asks whether two objects are the same as each other while the = operator assigns the value/object on the right into the variable name on the left. In R, the preferred way of assignment is to use &lt;- operator instead. Boolean Operators There are four Boolean operators in R to combine two conditions: &amp; (AND), | (inclusive OR), xor() (exclusive OR), and ! (NOT). That is, we can combine two or more conditions with these operators for more complex conditions. ## Inclusive OR x &lt;- 47 x &gt; 30 | x &lt; 50 [1] TRUE ## AND x &lt;- 55 x &gt; 30 &amp; x &lt; 50 [1] FALSE ## NOT x &lt;- 55 x &gt; 50 [1] TRUE !x &gt; 50 [1] FALSE ## Create objects a &lt;- c(TRUE, FALSE, TRUE, FALSE) b &lt;- c(FALSE, TRUE, TRUE, FALSE) ## Exclusive vs. Inclusive OR xor(a, b) [1] TRUE TRUE FALSE FALSE a|b [1] TRUE TRUE TRUE FALSE Can you analyze the following given piece of R code and predict its output before execution? # Exclusive OR x &lt;- c(1, 2, 3) y &lt;- c(1, 3, 5) z &lt;- xor(x %in% y, y %in% x) print(z) # Inclusive OR z2 &lt;- x %in% y | y %in% x print(z2) The Boolean operators follow the order of operations in math. That is, R evaluates the ! (not) operator first, then the &amp; (and) operator, and then the | (or) operators. ## TRUE &amp; FALSE &amp; TRUE 2 + 2 == 4 &amp; 2 + 2 == 5 &amp; 2 * 2 == 2 + 2 [1] FALSE ## TRUE &amp; TRUE &amp; TRUE 2 + 2 == 4 &amp; !2 + 2 == 5 &amp; 2 * 2 == 2 + 2 [1] TRUE Elements of Flow Control Flow control statements often start with a condition and are followed by a block of code. A quick recap: Conditions: Any Boolean expressions can be a potential condition, which evaluates down to a Boolean value (i.e., TRUE or FALSE). A flow control statement decides what to do and what to skip based on whether the condition is TRUE or FALSE. Block of Code: Lines of R code can be grouped together in blocks, using initial and ending curly brackets { and }. The beginning and ending of the block of code are clearly indicated. 5.2 if Statements The main purpose of if is to control precisely which operations are carried out in a given code chunk. An if statement runs a code chunk only if a certain condition is true. This conditional expression allows R to respond differently depending on whether the condition is TRUE or FALSE. The basic template of if is as follows: ## Simple Binary Condition if(CONDITION IS TRUE){ DO THIS CODE CHUNK 1 } else { DO THIS CODE CHUNK 2 } ## endif ## More Complex Conditions if (condition1) { expr1 } else if (condition2) { expr2 } else if (condition3) { expr3 } else { expr4 } ## endif The condition is placed in the parenthesis after if within (). The condition must be an expression that returns only a single logical value (TRUE or FALSE). If it is TRUE, the code chunk 1 in the curly braces will be executed; if the condition is not satisfied, the code chunk 2 in the curly braces after else will be executed. Suppose you want to simulate a password checking system. The system requires you to enter a password to access it. The system has a password stored on the server. The gatekeeper of the system will check whether your input password matches the one stored on the server. If your input password does not match the one on the server, you will be banned from accessing the system. correct_passcode &lt;- 987 ## system correct passcode input &lt;- 113 ## assuming that you have the input 113 ## Passcode checking &amp; output if(input == correct_passcode){ writeLines(&quot;Congratulations! Now you may get in!&quot;) } else{ writeLines(&quot;Sorry! Wrong password.&quot;) } Sorry! Wrong password. If the input matches the system passcode, you will be granted access to the system. correct_passcode &lt;- 987 ## system correct passcode input &lt;- 987 ## assuming that you have the input 987 ## Passcode checking &amp; output if(input == correct_passcode){ writeLines(&quot;Congratulations! Now you may get in!&quot;) } else{ writeLines(&quot;Sorry! Wrong password.&quot;) } Congratulations! Now you may get in! Now we can ask R to prompt the user to input data directly in the R console, using the readline() function. (Note: Please run the entire code chunk all at once.) ## ------------------------------------- ## ## Run the entire code chunk all at once ## ## ------------------------------------- ## ## System Correct Passcode correct_passcode &lt;- 987 ## User&#39;s input input &lt;- readline(prompt = &quot;Please enter your password:&quot;) ## Passcode checking &amp; output if (input == correct_passcode) { writeLines(&quot;Congratulations! Now you may get in!&quot;) } else { writeLines(&quot;Sorry! Wrong password.&quot;) } ## endif In R, there are two very similar functions, readline() and readLines(). Please check the documentations of these two to make sure you understand their differences. readline() is used to read a single line of input from the user in the R console. It prompts the user for input and waits for them to enter a response, then returns that response as a character string. readLines() is used to read multiple lines of input from a file or other external source. It reads in all the lines of a file or input stream and returns them as a character vector. 5.3 for The for loop is a core programming construct in R, which allows you to repeat a code chunk a certain number of times. Typically, you would use a for loop to iterate over elements of a vector, list or data frame and/or perform a certain operation a fixed number of times. The general structure of the for loop statement is to repeat the operation included a code chunk while incrementing an index or a counter. The basic for loop template is as follows: for(LOOP_INDEX in LOOP_VECTOR){ DO THIS CODE CHUNK } The LOOP_INDEX is a placeholder that represents an element in the LOOP_VECTOR. When the loop begins, the LOOP_INDEX starts off as the first element in the LOOP_VECTOR. When the loop reaches the end of the brace, the LOOP_INDEX is incremented, taking on the next element in the LOOP_VECTOR. This process continues until the loop reaches the final element of the LOOP_VECTOR. At this point, the code chunk is executed for the last time, and the loop exits. For example, if we have a character vector with a few words in it. We can use a for loop to get the number of characters for each element (i.e., word) in the vector. word_vec &lt;- c(&quot;apple&quot;,&quot;banana&quot;,&quot;watermelon&quot;,&quot;papaya&quot;) for(w in word_vec){ word_nchar &lt;- nchar(w) writeLines(as.character(word_nchar)) } 5 6 10 6 For the above example, there is another way to write the for loop: for(i in 1:length(word_vec)){ word_nchar &lt;- nchar(word_vec[i]) writeLines(as.character(word_nchar)) } 5 6 10 6 In our first example, the LOOP_INDEX serves as the exact element object in the LOOP_VECTOR. In our second example, the LOOP_INDEX serves as the index of the element object in the LOOP_VECTOR. Try the following code chunk and examine the differences in the outputs between print() and writeLines(). Do you know why the first for-loop does not work? # writeLines() (This for-loop does not work!!) for(i in 1:length(word_vec)){ word_nchar &lt;- nchar(word_vec[i]) writeLines(word_nchar) } # `print()` for(i in 1:length(word_vec)){ word_nchar &lt;- nchar(word_vec[i]) print(word_nchar) } Please note that the control structures are to direct the flow of execution of the codes. Therefore, the control structure itself DOES NOT return any object. That is, you CANNOT assign a for-loop structure to an object. It is NOT meaningful and grammatical. The following code chunk would give you an error. ####################################################### ## WARNING!!! This code chunk is UNGRAMMATICAL!!!!!! ## ####################################################### numOfChars &lt;- for(i in 1:length(word_vec)){ word_nchar &lt;- nchar(word_vec[i]) writeLines(as.character(word_nchar)) } Exercise 5.1 Use the data set from stringr::sentences for this exercise. Create a for-loop structure to get the number of characters for all sentences in the stringr::sentences. Present your results in a data frame of three columns: Column 1 includes an unique integer ID for each sentence; Column 2 includes the texts of each sentence; Column 3 includes the number of characters of each sentence. Note: A for-loop structure is needed for this exercise. But it may not necessarily be the most efficient way. The data set stringr::sentences includes 720 English sentences. The first six sentences are shown here. [1] &quot;The birch canoe slid on the smooth planks.&quot; [2] &quot;Glue the sheet to the dark blue background.&quot; [3] &quot;It&#39;s easy to tell the depth of a well.&quot; [4] &quot;These days a chicken leg is a rare dish.&quot; [5] &quot;Rice is often served in round bowls.&quot; [6] &quot;The juice of lemons makes fine punch.&quot; Your results may look like the following data frame: Exercise 5.2 Use the data set stringr::fruit for this exercise. Create a for-loop and if-statement to instruct R to go through each fruit name and print out those fruit names that start with the vowel letters only (i.e., a, e, i, o, u). [1] &quot;apple&quot; [1] &quot;apricot&quot; [1] &quot;avocado&quot; [1] &quot;eggplant&quot; [1] &quot;elderberry&quot; [1] &quot;olive&quot; [1] &quot;orange&quot; [1] &quot;ugli fruit&quot; 5.4 while loop There is another type of loop. Unlike the for loop, which repeats a code chunk by going through every element in a vector/list/data frame, the while loop repeats a code chunk UNTIL a specific condition evaluates to FALSE (It’s like the opposite of if-statement) The basic template is as follows: while(LOOP_CONDITION){ DO THIS CODE CHUNK (UNTIL THE LOOP_CONDITION BECOMES FALSE) } Upon the start of a while loop, the LOOP_CONDITION is evaluated. If the condition is TRUE, the braced code chunk is executed line by line till the end of the chunk. At this point, the LOOP_CONDITION will be checked again. The loop terminates immediately when the condition is evaluated to be FALSE. That is, the code chunk will be repeated as long as the condition is true. Based on the template above, it is important to note that the code chunk executed must somehow cause the loop to exit. In particular, the code chunk needs to change the values of certain objects, which would eventually lead to the change of the LOOP_CONDITION. If the LOOP_CONDITION never evaluates to FALSE, the loop will continue infinitely, and R will crash. To avoid this, the code chunk must modify the values of certain objects that affect the loop condition. This way, as the loop runs, the loop condition will eventually be met, and the loop will exit. Let’s come back to our password checker. This time let’s create a dumb checker. When you give a wrong password which is smaller than the true passcode, it will automatically approach the right answer for you (and of course no real-world application would do that!) ## -------------------------------------------- ## ## Please run the entire code chunk all at once! ## ## -------------------------------------------- ## ## Create a program app_v1 &lt;- function(ans = 90, guess = 83) { while (guess != ans) { cat(&quot;Your `guess` is too small! \\nThe system will take care for you!\\n&quot;) guess &lt;- guess + 1 cat(&quot;Now the system is adjusting your `guess` to &quot;, guess, &quot;\\n\\n&quot;) } ## endwhile cat(&#39;Great! The passcode is finally cracked.\\n&#39;) } ## endfunc ## Run the program app_v1(ans = 90, guess = 83) Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 84 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 85 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 86 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 87 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 88 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 89 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 90 Great! The passcode is finally cracked. Exercise 5.3 The current implementation of app_v1() works fine when the guess value is smaller than the actual ans value. However, if the guess value is larger than the actual ans value, the script will enter an infinite loop and crash. To solve this issue, we need to create an updated version of the program called app_v2(), which should automatically adjust the guessed number to approach the actual answer. This adjustment can be done by incrementally adding or subtracting one from the guessed number until it matches the actual answer. The updated app_v2() program should take two parameters as input: ans and guess. It should then compare the guess value to the actual ans value. If the guess value is equal to ans, the program should print a message stating that the guess is correct. Examples are provided below. app_v2(ans = 90, guess = 87) Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 88 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 89 Your `guess` is too small! The system will take care for you! Now the system is adjusting your `guess` to 90 app_v2(ans = 90, guess = 93) Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 92 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 91 Your `guess` is too large! The system will take care for you! Now the system is adjusting your `guess` to 90 app_v2(ans = 90, guess = 90) This is spooky. Your guess is correct!! 5.5 Toy Example Now we are playing the Guess Game. The game is as follows: The program will pick a random number from 1 to 100. A user has to guess which number the computer has picked. Every time the user makes a wrong guess, the computer will tell the user whether the correct answer is higher or lower. We first pack the game as an R function object: ## -------------------------------------------- ## ## Please run the entire code chunk all at once! ## ## -------------------------------------------- ## ## Create a function object guessMyNumber &lt;- function() { ## Randomly select an integer from 1 to 100 ans &lt;- sample(1:100, size = 1) ## Initialize the variable `guess` guess &lt;- -1 ## Instructions for user writeLines(&quot;Now I am thinking of a number between 1 and 100.&quot;) ## As long as user&#39;s guess is not the answer while (guess != ans) { ## Read the prompt input from user guess &lt;- readline(prompt = &quot;Please guess my number(1~100):&quot;) ## Convert input string into integer guess &lt;- as.numeric(guess) ## if user&#39;s guess is smaller than the answer if (guess &lt; ans) { writeLines(&quot;The answer is HIGHER.&quot;) ## if user&#39;s guess is larger than the answer } else if (guess &gt; ans) { writeLines(&quot;The answer is LOWER&quot;) ## correct guess } else{ writeLines(paste0(&quot;Good Job! You had the correct answer! My number is &quot;, guess)) } ## endif } ## endwhile } ## endfunc Please note that in the above function definition, we have included a line of code to initialize the guess to -1 before the while loop: ## Initialize the variable `guess` guess &lt;- -1 This step is crucial because the variable guess is not defined before it is used in the while loop condition. The code tries to compare guess with ans but guess has not been defined yet. This results in an error message such as Error in while (guess != ans) : missing value where TRUE/FALSE needed. To fix the issue, the guess variable needs to be initialized to some value before the while loop. For example, a default value could be set for the initial guess. And guess is initialized to -1 instead of any other number because the range of possible answers is 1 to 100, and -1 is not a valid guess within this range. Initializing guess to -1 guarantees that the loop will run at least once, as the default first guess (-1) will always be different from the value of ans, which is randomly selected from the range of valid guesses. Once the user enters a valid guess, guess is updated to that value, and the loop continues until the user’s guess matches the value of ans. After you load the above code chunk and create the guessMyNumber() function in your current R environment, you can play the game by running the function guessMyNumber(): guessMyNumber() The above code demonstrates how to create a guessing game in R using a function called guessMyNumber(). Initially, the function does not accept any parameters. However, one can create a function with a parameter (e.g., guess), which allows the user to specify initial values for the game. For example: ## -------------------------------------------- ## ## Please run the entire code chunk all at once! ## ## -------------------------------------------- ## ## Create a function object guessMyNumber_1 &lt;- function(guess) { ## Randomly select an integer from 1 to 100 ans &lt;- sample(1:100, size = 1) ## Instructions for user writeLines(&quot;Now I am thinking of a number between 1 and 100.&quot;) ## As long as user&#39;s guess is not the answer while (guess != ans) { ## Read the prompt input from user guess &lt;- readline(prompt = &quot;Please guess my number(1~100):&quot;) ## Convert input string into integer guess &lt;- as.numeric(guess) ## if user&#39;s guess is smaller than the answer if (guess &lt; ans) { writeLines(&quot;The answer is HIGHER.&quot;) ## if user&#39;s guess is larger than the answer } else if (guess &gt; ans) { writeLines(&quot;The answer is LOWER&quot;) ## correct guess } else{ writeLines(paste0(&quot;Good Job! You had the correct answer! My number is &quot;, guess)) } ## endif } ## endwhile } ## endfunc The revised function guessMyNumber_1() now accepts a parameter guess that enables the user to specify the initial guess value when they start playing the game. The user can start the game by running the function and specifying the initial guess value, like this: guessMyNumber_1(guess = 15). However, the revised function has two potential issues: It is not very intuitive to ask the user to provide a guess value when they have not even started the game yet. (They probably do not even know the objective of the game.) If the user specifies a guess value that happens to be the computer’s selection, the game will not even start. To address these issues, we may also try the following revision (guessMyNumber_2()): ## -------------------------------------------- ## ## Please run the entire code chunk all at once! ## ## -------------------------------------------- ## ## Create a function object guessMyNumber_2 &lt;- function(guess = -1) { ## Randomly select an integer from 1 to 100 ans &lt;- sample(1:100, size = 1) ## Instructions for user writeLines(&quot;Now I am thinking of a number between 1 and 100.&quot;) ## As long as user&#39;s guess is not the answer while (guess != ans) { ## Read the prompt input from user guess &lt;- readline(prompt = &quot;Please guess my number(1~100):&quot;) ## Convert input string into integer guess &lt;- as.numeric(guess) ## if user&#39;s guess is smaller than the answer if (guess &lt; ans) { writeLines(&quot;The answer is HIGHER.&quot;) ## if user&#39;s guess is larger than the answer } else if (guess &gt; ans) { writeLines(&quot;The answer is LOWER&quot;) ## correct guess } else{ writeLines(paste0(&quot;Good Job! You had the correct answer! My number is &quot;, guess)) } ## endif } ## endwhile } ## endfunc The revised guessMyNumber_2() provides two possibilities for the user to start the game. The user can either start the game with a random guess (by specifying the value for the parameter guess), e.g., guessMyNumber_2(guess = 15), or start the game by accepting the default setting of the parameter guess (e.g., -1), e.g., guessMyNumber_2(). The decision of whether or not to include parameters in a program or function ultimately depends on the developer’s design goals and the intended use of the program by its users. Parameters allow for greater flexibility and customization, as users can provide input values to the function to modify its behavior. However, sometimes the program may be designed to have a fixed set of inputs and outputs, in which case parameters may not be necessary. The design choice should prioritize the usability and user experience of the program, taking into consideration the user’s level of expertise, potential use cases, and any potential issues that may arise from the inclusion or exclusion of parameters. Exercise 5.4 The above guessMyNumber() can be improved. Sometimes naughty (careless) users would not input numbers as requested. Instead, they may accidentally (or on purpose) enter characters that are NOT digits at all in their guesses or digits that are not within the range of 1 to 100. Please improve the guessMyNumber() function in R to handle non-digit and out-of-range inputs from users. If a user enters non-digit characters or digits outside the range of 1 to 100, the program should send a warning message. Additionally, once the user guesses the correct answer, the program should record the number of guesses the user made in total. The desired output of the revised function should be similar to the example output provided. &gt; guessMyNumber_v2() Now I am thinking of a number between 1 and 100. Please guess my number(1~100):% Invalid guess. Please enter a number between 1 and 100. Please guess my number(1~100):1000000 Invalid guess. Please enter a number between 1 and 100. Please guess my number(1~100):-10 Invalid guess. Please enter a number between 1 and 100. Please guess my number(1~100):50 The answer is LOWER Please guess my number(1~100):25 The answer is LOWER Please guess my number(1~100):10 The answer is HIGHER. Please guess my number(1~100):15 The answer is HIGHER. Please guess my number(1~100):19 The answer is LOWER Please guess my number(1~100):18 The answer is LOWER Please guess my number(1~100):17 The answer is LOWER Please guess my number(1~100):16 Good Job! After 8 guess(es), you got the correct answer! My number is 16! "],["functions.html", "Chapter 6 Functions 6.1 A Quick Start 6.2 Why do we need functions? 6.3 Functions with parameters 6.4 Recap of Important Concepts So Far 6.5 RETURN Statements 6.6 Parameters Order 6.7 Stacking Functions 6.8 Local and Global Scope 6.9 Exception Handling", " Chapter 6 Functions We have been using R functions in the default base R package, such as c(), list(), sample(). R provides a lot of useful built-in functions like these, but we can write our own task-specific functions as well. A function is like a mini-program within a program. In this unit, we discuss how to write functions in R. 6.1 A Quick Start To better understand how a function works, let’s create a simple one. The following code chunk creates a function object, named hello(). hello &lt;- function() { print(&quot;How are you doing?&quot;) } A function object includes several important elements: We use the function() keyword to define a new function object. After the function() keyword is the code block ({...}), which contains the body of the function. That is, the code block is the set of instructions that perform the desired task. The body of the function function(Parameter1, Parameter2, ...) can take arguments (inputs) that are passed to the function when it is called (See next section.) Every function is assigned to a user-defined name (e.g., hello in the above example.) Once a function is defined using the function() keyword in R, the code within the body of the function will not be executed immediately. Rather, the code will only be executed when the function is called. Whenever the function is called, it will execute the code within its body, carrying out the specific tasks defined in the function’s code block. This allows for code reuse and modularity, as the same code can be called multiple times with different inputs, rather than having to rewrite the same code multiple times. hello() [1] &quot;How are you doing?&quot; hello() [1] &quot;How are you doing?&quot; 6.2 Why do we need functions? A major advantage of creating functions in our programs is to group codes that get executed multiple times. Without a function defined, one may need to copy-and-paste same code chunks many times. Second, with functions, it is easier to update the programs. We often try to avoid duplicating code because if we need to update the code (e.g., to fix a bug in the original code), we don’t have to change the code everywhere we have copied it. In short, functions can greatly reduce the chances of duplicating code, rendering the programs shorter, easier to read and update. 6.3 Functions with parameters When we use the built-in R functions like cat(), length(), or matrix(), we can pass them values, called arguments, in the parentheses. That is, some functions have parameters and users can pass values to each parameter as arguments. In our self-defined functions, we can also define a function which accepts arguments. hello &lt;- function(name) { cat(&quot;How are you doing,&quot;, name) } hello(name = &quot;Alvin&quot;) How are you doing, Alvin The new hello() function has a parameter called name. Parameters are variables that expect arguments in the function call. When a function is called with an argument (e.g., Alvin), this argument is stored in the parameter (e.g., name). More specifically, when the function hello(name = 'Alvin') is called: The argument \"Alvin\" is assigned to the parameter name; The program then continues the code block of the function; Within the code block, the parameter name is automatically set to Alvin. It is important to note that the value stored in the parameter is forgotten when the function returns. That is, we cannot access the parameter name in the main program: cat(name) Error in eval(expr, envir, enclos): object &#39;name&#39; not found In short, the parameters of a function are destroyed after a function call hello(name = 'Alvin') returns. In the function definition, we can specify default values to the parameters. For example, if the function hello() is defined as follows, users can decide whether to accept the default argument or assign the parameter name with a new argument: hello &lt;- function(name = &quot;Alvin&quot;) { cat(&quot;How are you doing,&quot;, name) } ## call 1 hello() How are you doing, Alvin ## call 2 hello(name = &quot;Superman&quot;) How are you doing, Superman 6.4 Recap of Important Concepts So Far To utilize a function object, there are several key steps: We need to define the function by creating it using hello &lt;- function(){...} and assigning it with an object name like any other objects in R. Then we can call the now-created function using hello(). The function call will start the execution of the code block in the function by first passing or assigning the arguments/values to the parameters within the function (e.g., hello(name = 'Alvin')). A value being passed to a function in a function call is an argument, (e.g., Alvin) Variables that have arguments assigned to them are parameters, (e.g., name =). 6.5 RETURN Statements When we define a function, we can specify what the return values should be using the return() statement. The returned values, i.e., the output object of the function, can then be assigned to a new object name for later use in the program. In R, there are many built-in functions that return values: num &lt;- sample(1:10, 3) num [1] 3 5 8 When a function returns nothing, by default the return value of the function is NULL, which is a unique data type in R referring to NoneType. out &lt;- cat(&quot;This is a sentence&quot;) # `cat()` has no return This is a sentence out NULL Now how about the hello() function we created earlier? We didn’t specify the return() statement in the function definition. out &lt;- hello(name = &quot;John&quot;) How are you doing, John out NULL In the definition of hello(), we did not specify the return() statement; therefore, by default, this function returns NULL. But how come we can still see the outputs of the function? In the code block of hello() definition, the cat() displays text on the R console only. Therefore, displaying texts in the R console and returning the values are two different things. Exercise 6.1 The function hello() prints a message to the console. Without any change of the function definition hello(), how can we capture the messages printed in the console by hello() and save them to an object named out? Your task is to modify the following code chunk so that out can store the messages printed by hello(name=\"Alvin\"). Keep in mind that you cannot modify the definition of hello() itself, so you will need to use a different approach to capture its output. out &lt;- hello(name=&quot;Alvin&quot;) out Exercise 6.2 Can you try to create a revised version of hello(), which returns the strings so that one can assign the outputs of the hello() to another object name? (Please note that in the following example, the return value out is not a NULL anymore.) out &lt;- hello2(name = &quot;Alvin&quot;) out [1] &quot;How are you doing, Alvin&quot; hello2(name = &quot;John&quot;) [1] &quot;How are you doing, John&quot; 6.6 Parameters Order We’ve seen functions with parameters. When a function has many parameters, there are two alternatives to assign the arguments to the parameters in the function call. First, we can assign the arguments to the parameters specified in the function call: set.seed(123) sample(x = c(1:10), size = 5, replace = FALSE, prob = NULL) [1] 3 10 2 8 6 Alternatively, we can assign the arguments to the parameters according to the order of the parameters in th function definition without specifying the parameter names: set.seed(123) sample(c(1:10), 5, FALSE, NULL) [1] 3 10 2 8 6 In the function definition, we can also assign default values to the parameters. For example, in the documentation of sample(x, size, replace = FALSE, prob = NULL), we can see that the parameters replace= and prob= have default values. That means in the function call we can use these default values as the arguments without specifying them in the call. sample(c(1:10), 5) [1] 5 4 6 8 1 6.7 Stacking Functions A function can also call another function within its code block. When this happens, the execution of the code would move to the called function before returning to the original function call. For instance, we can define two functions in R, hello() and email(). ## Main function hello &lt;- function(name) { user_email &lt;- email(user = name) out &lt;- paste0(&quot;How are you doing, &quot;, name, &quot;. &quot;, user_email) return(out) } ## Embedded function email &lt;- function(user) { out &lt;- paste0(&quot;Your email is: &quot;, tolower(user), &quot;@whatever.org&quot;) return(out) } Within the code block of hello(), we can make a function call to email(). This means that email() is embedded within hello(), and when hello() is called, it will execute its code block and then move to execute the code block of email(). After the code block of email() is executed, the control will move back to hello() to complete its execution. ## call `hello()` hello(&quot;Alvin&quot;) [1] &quot;How are you doing, Alvin. Your email is: alvin@whatever.org&quot; This ability to call functions within functions is useful in situations where a function needs to perform multiple tasks, and each task can be implemented using a separate function. Rather than writing all the code in a single function, we can call other functions from within it to keep the code organized and easier to understand. 6.8 Local and Global Scope Now we know that functions allow us to organize code into reusable blocks that perform specific tasks. We only have to write code once and can call it multiple times with different inputs. When we call a function, it creates its own workspace or environment, called the local scope. Any variables created or modified inside the function only exist within that function and are known as local variables. These variables cannot be accessed or modified outside of the function. On the other hand, variables that are created outside of any functions are said to exist in the global scope. These variables are known as global variables and are accessible to all functions and code blocks in the program. Any changes made to global variables inside a function will affect the global variable’s value outside the function. A scope is like the life-span of the variable. A local scope is created whenever a function is called. Any variables created in the function exist within the function’s local scope. When the function returns, the local scope is destroyed, and these local variables are forgotten (i.e., removed from the memory of the current working environment). The global scope is created when the main program starts (e.g., your current R session). When the program terminates, the global scope is destroyed, and all the global variables are forgotten. There are a few important considerations for variable scope: Code in the global scope (i.e., outside of all functions) cannot use any local variables (i.e., variables within functions). Code in a local scope can access global variables. Code in a local scope can modify the values of global variables. Code in a function’s local scope cannot use variables in any other local scope. We can use the same name for different variables if they are in different scopes (e.g., they can be local variables within different functions). While using global variables within functions in small programs may not cause significant issues, it is generally considered bad practice to rely on global variables in larger programs. One issue with using global variables in local functions is that they can be accessed and modified by any part of the program, making it difficult to track changes and debug the code. Another issue is that global variables can make it difficult to reuse functions in other parts of the program or in other programs. In short, functions that rely on global variables are less modular and less portable, and can make code maintenance and updates more difficult in the long run. The following code chunk shows that local variables cannot be accessed in the global scope. customer &lt;- function() { id &lt;- 123 age &lt;- 25 nation &lt;- &quot;TW&quot; } customer() cat(id) Error in cat(id): argument 1 (type &#39;closure&#39;) cannot be handled by &#39;cat&#39; The following code chunk shows that local scopes cannot use variables in other local scopes. customer &lt;- function() { id &lt;- 123 age &lt;- 25 nation &lt;- &quot;TW&quot; print(age) } client &lt;- function() { age &lt;- 50 } client() customer() # returning `age` from `customer()` not from `client()` [1] 25 The following code chunk shows a local scope can access global variables. customer &lt;- function() { age &lt;- 25 cat(&quot;The customer works at&quot;, company) } company &lt;- &quot;NTNU&quot; customer() The customer works at NTNU Technically, it is OK to use the same variable name for a global variable and local variable in different scopes. But to make your life easier, please avoid doing this. customer &lt;- function() { age &lt;- 25 cat(age) } client &lt;- function() { age &lt;- 55 cat(age) } age &lt;- 100 customer() 25 client() 55 cat(age) 100 6.9 Exception Handling In programming, errors and warnings can occur during the execution of code, and these events can cause the program to terminate abruptly or produce unexpected results. In order to prevent the termination of the main program and to handle these events in a more controlled way, developers often include exception handling in their code. For example, if we create a function myLog(), which takes a number and computes the log value of the number with the specified base, there are a few cases where the return values may be problematic: myLog &lt;- function(x, myBase) { return(log(x, myBase)) } ## OK myLog(100, 10) [1] 2 myLog(8, 2) [1] 3 ## Not OK myLog(10, -1) ## base is negative Warning in log(x, myBase): NaNs produced [1] NaN myLog(-10, 10) ## x is negative Warning in log(x, myBase): NaNs produced [1] NaN myLog(&quot;100&quot;, 10) ## x is not numeric Error in log(x, myBase): non-numeric argument to mathematical function To make sure that the function myLog() does not terminate the main program when encountering errors or warnings, it is often a good idea to include exception handling in the function code block. In R, exception handling can be achieved using the tryCatch() function. This function allows the programmer to specify what should happen when an error or warning occurs during the execution of a block of code. The structure of tryCatch() includes a try block where the code is executed, and a catch block where the handling of errors and warnings is specified. If an error or warning occurs in the try block, the code in the catch block is executed instead of terminating the program. Its structure is as follows: result &lt;- tryCatch({ ##----- original_code -----## }, warning = function(w) { ##----- warning_handler_code -----## }, error = function(e) { ##----- error_handler_code -----## }, finally = { ##----- cleanup_code -----## }) ## endtry tryCatch() includes the following important elements: expr: the expression/code to be evaluated. warning: When the expr causes a warning, the program execution immediately moves to the code in the warning code block. error: When the expr causes an error, the program execution immediately moves to the code in the error code block. Now let’s try to include tryCatch() in our code block of the function myLog(): myLog &lt;- function(x, myBase) { tryCatch({ ##----- original_code -----## return(log(x, myBase)) }, warning = function(w) { ##----- warning_handler_code -----## if (x &lt; 0) print(&quot;WARNING!! `x` must be a positive number&quot;) if (myBase &lt; 0) print(&quot;WARNING!! `myBase` must be a positive number&quot;) }, error = function(e) { ##----- error_handler_code -----## if (!is.numeric(x) | !is.numeric(myBase)) print(&quot;ERROR!! Either `x` or `myBase` must be a positive number not a string&quot;) }, finally = { ##----- cleanup_code (optional) -----## ## print(&#39;Function completed!!&#39;) }) ## endtrycatch } ## endfunc myLog(100, 10) [1] 2 myLog(100, exp(1)) ## same as `log(100)` with natural E as the base [1] 4.60517 myLog(10, -1) ## Warning [1] &quot;WARNING!! `myBase` must be a positive number&quot; myLog(&quot;w12&quot;, 0) ## Error [1] &quot;ERROR!! Either `x` or `myBase` must be a positive number not a string&quot; myLog(8, &quot;2&quot;) ## Error [1] &quot;ERROR!! Either `x` or `myBase` must be a positive number not a string&quot; In R, cat(), writeLines(), and print() are three functions that can be used to print out output to the console (or other output devices). However, they differ in their syntax and output format. Here are the main differences between these functions: x &lt;- c(1:10) ## num vec y &lt;- letters[1:10] ## char vec z &lt;- factor(y) ## factor cat() This function is used to concatenate and print objects. It can take one or more objects as input and concatenates them with a separator (by default, a space). The output is not formatted and is printed as a single string. cat(x) ## cat numbers 1 2 3 4 5 6 7 8 9 10 cat(y) ## cat characters a b c d e f g h i j Please pay particular attention to how cat() prints the values of a factor: cat(z) ## cat factor 1 2 3 4 5 6 7 8 9 10 cat(&quot;Numbers:&quot;, x, &quot;Characters:&quot;, y) ## concatenated strings Numbers: 1 2 3 4 5 6 7 8 9 10 Characters: a b c d e f g h i j writeLines() This function writes character vectors to a connection, with one element per line. By default, each element is appended with a line break. writeLines(y) ## char vec a b c d e f g h i j Please note that this function only takes character vectors as the input and does not do implicit data conversion (if the input is a numeric vector) writeLines(x) ## !!Not working with num vec Error in writeLines(x): can only write character objects This function cannot take a factor either: writeLines(z) ## !!Not Working with factor Error in writeLines(z): can only write character objects Unlike cat(), it cannot concatenate character strings. writeLines(&quot;Characters:&quot;, y) ## !! Not Working.. Error in file(con, &quot;w&quot;): invalid &#39;description&#39; argument print() print() is a generic function that prints the object to the console or a file. print() is called implicitly whenever an object name is typed into the console (i.e., auto-printing) or passed as an argument to a function that expects a printed output. It applies formatting to the object being printed, such as adding quotes to character strings and displaying factors as levels rather than as numeric codes. If the input is a numeric vector, print() does implicit data conversion and prints the numeric values to the console. print(x) ## print num vec [1] 1 2 3 4 5 6 7 8 9 10 If the input is a character vector, print() prints the characters to the console, each of which is embraced with double quotes indicating its character data type. print(y) ## print char vec [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; This function prints a factor as well: print(z) ## print factor [1] a b c d e f g h i j Levels: a b c d e f g h i j This function does not concatenate character strings either. print(x, y) Warning in print.default(x, y): NAs introduced by coercion Error in print.default(x, y): invalid printing digits -2147483648 Exercise 6.3 Create a function that produces a simple animation, i.e., the zigzag outputs as shown below. The function will slowly create a back-and-forth zigzag pattern with the laps and the indent size (i.e., the maximum number of spaces that the zigzag pattern can go) as the parameters of the function. The animation should be generated slowly, so that the pattern can be clearly seen as it is being created. Additionally, the user should be able to stop the animation at any time by pressing CTRL+C. zigzag(lap = 10, indent_max = 20) Please note that the user can interrupt the program/function by pressing CTRL+C and your function should stop properly (using tryCatch). Exercise 6.4 Create a function that allows the user to play a game of rock, paper, scissors against the computer. The function should have the following features: The user should be prompted to enter their move as a text input (i.e., rock, paper, or scissors). The computer should randomly select a move. The program should ensure that the user’s input is a valid move. After each round, the program should report the result of the game as a text output, either “You win!”, “You lose.”, or “It’s a tie.” The user should be able to play as many rounds as they wish until they choose to quit. When the user decides to quit, the program should provide a summary of the number of games won, lost, and tied. An example of how the function works is provided below. "],["data-manipulation.html", "Chapter 7 Data Manipulation 7.1 Dataset 7.2 rename() 7.3 Pipe %&gt;% 7.4 mutate() 7.5 select() 7.6 filter() 7.7 arrange() 7.8 group_by() and summarize() 7.9 count() 7.10 Tidy Data 7.11 Exercises", " Chapter 7 Data Manipulation In this chapter, we will be introducing two powerful R packages, dplyr and tidyr, which are designed to simplify and streamline data manipulation and exploration tasks. These packages provide a consistent and intuitive “grammar” for working with data frames, which are a fundamental data structure in R. We first load the library: library(dplyr) library(tidyr) library(ggplot2) The dplyr package in R provides a set of key verbs that are used for data manipulation tasks. These verbs make it easy to perform common data manipulation operations in a concise and intuitive way. Here is a brief overview of each of the key verbs: %&gt;%: This is the “pipe” operator, which is used to connect multiple verb actions together into a pipeline. It allows you to easily chain together different data manipulation actions, making it easier to perform complex manipulations with just a few lines of code. mutate(): This verb is used to add new variables or transform existing variables in a data frame. It allows you to create new columns based on calculations or transformations of existing columns. select(): This verb is used to return a subset of the columns of a data frame. It allows you to select specific columns or exclude columns based on column names, column indices, or logical conditions. filter(): This verb is used to extract a subset of rows from a data frame based on logical operators. It allows you to filter rows based on conditional subetting. group_by(): This verb is used to group a data frame into sub-tables according to a grouping factor. It allows you to create groups based on one or more variables, and then perform calculations or summarizations on each group separately. summarise(): This verb is used to generate summary statistics of different variables in a data frame. It allows you to calculate summary statistics such as mean, median, or standard deviation for one or more variables, possibly within strata. arrange(): This verb is used to reorder the rows of a data frame according to a particular variable. It allows you to sort the rows of a data frame in ascending or descending order based on one or more variables. rename(): This verb is used to rename variables in a data frame. It allows you to rename one or more variables by specifying the new names as a named vector. In the second library tidyr, we will focus on: pivot_longer(): to tidy the data from wide to long pivot_wider(): to tidy the data from long to wide Exercise 7.1 In dplyr, there are several useful functions for joining two data frames based on a common set of variables: inner_join() left_join() right_join() full_join() anti_join() Please check the documentations of all the above functions of merging data frames. Now suppose you have two data frames: orders and customers, as shown below. The orders data frame contains information about customer orders, and the customers data frame contains information about customers. Show your code that produces the following merged versions of the two data sets: orders &lt;- data.frame( order_id = c(1, 2, 3, 4, 5), customer_id = c(101, 102, 103, 104, 105), product = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;D&quot;), stringsAsFactors = FALSE ) customers &lt;- data.frame( customer_id = c(101, 102, 103, 104, 106), name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;, &quot;Dave&quot;, &quot;Eve&quot;), age = c(30, 25, 35, 40, 45), stringsAsFactors = FALSE ) Version A Version B Version C Version D 7.1 Dataset The dateaset we use in this chapter is a student performance dataset from kaggle. library(readr) student &lt;- read_csv(&quot;demo_data/data-students-performance.csv&quot;) student Usually we would start from an overview of the dataset, using summary() and str(): summary(student) gender race/ethnicity parental level of education Length:1000 Length:1000 Length:1000 Class :character Class :character Class :character Mode :character Mode :character Mode :character lunch test preparation course math score reading score Length:1000 Length:1000 Min. : 0.00 Min. : 17.00 Class :character Class :character 1st Qu.: 57.00 1st Qu.: 59.00 Mode :character Mode :character Median : 66.00 Median : 70.00 Mean : 66.09 Mean : 69.17 3rd Qu.: 77.00 3rd Qu.: 79.00 Max. :100.00 Max. :100.00 writing score Min. : 10.00 1st Qu.: 57.75 Median : 69.00 Mean : 68.05 3rd Qu.: 79.00 Max. :100.00 str(student) spc_tbl_ [1,000 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) $ gender : chr [1:1000] &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;male&quot; ... $ race/ethnicity : chr [1:1000] &quot;group B&quot; &quot;group C&quot; &quot;group B&quot; &quot;group A&quot; ... $ parental level of education: chr [1:1000] &quot;bachelor&#39;s degree&quot; &quot;some college&quot; &quot;master&#39;s degree&quot; &quot;associate&#39;s degree&quot; ... $ lunch : chr [1:1000] &quot;standard&quot; &quot;standard&quot; &quot;standard&quot; &quot;free/reduced&quot; ... $ test preparation course : chr [1:1000] &quot;none&quot; &quot;completed&quot; &quot;none&quot; &quot;none&quot; ... $ math score : num [1:1000] 72 69 90 47 76 71 88 40 64 38 ... $ reading score : num [1:1000] 72 90 95 57 78 83 95 43 64 60 ... $ writing score : num [1:1000] 74 88 93 44 75 78 92 39 67 50 ... - attr(*, &quot;spec&quot;)= .. cols( .. gender = col_character(), .. `race/ethnicity` = col_character(), .. `parental level of education` = col_character(), .. lunch = col_character(), .. `test preparation course` = col_character(), .. `math score` = col_double(), .. `reading score` = col_double(), .. `writing score` = col_double() .. ) - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 7.2 rename() The column names in the student data frame are not ideal. Some of these names include spaces, which can make it challenging to reference these columns in R. While this can be frustrating, it’s common to encounter messy data sets in the real world. Fortunately, we can easily rename the column names to be more R-friendly, allowing us to work with the data more effectively. rename(student, race = `race/ethnicity`, parent_edu = `parental level of education`, prep_course = `test preparation course`, math = `math score`, reading = `reading score`, writing = `writing score`) -&gt; student1 student1 Please note that in our earlier code, we save the output of rename() to a new object named student1. In other words, the object student1 should contain a new data frame with all column names corrected as shown. Most importantly, the original data frame student is still available in the working memory. Even though we created a new object, the original data frame remains untouched, and we can still access it if we need to. 7.3 Pipe %&gt;% Next, we will explore the powerful pipe operator %&gt;% in R, which is a popular R idiom for chaining multiple operations together. This operator is one of my favorites and can significantly simplify complex code. Let’s begin by considering two expressions that are equivalent and produce identical results: ## method1 sum(c(1:10)) [1] 55 ## method2 c(1:10) %&gt;% sum() [1] 55 The pipe operator %&gt;% is a powerful tool that passes the object returned by the expression on its left to the function on its right. Essentially, it enables a more readable and concise syntax for chaining multiple operations together. By default, the object from the left-hand side is passed as the first argument of the function on the right-hand side. This pipe-based syntax greatly enhances the readability of the code, making it easier to conceptualize complex operations. For example, consider the following code, which involves multiple layers of embedded structures: this code would be difficult to follow and understand without the use of the pipe operator. sqrt(sum(abs(c(-10:10)))) [1] 10.48809 But the above code can be re-written with the %&gt;% as follows: source_data &lt;- c(-10:10) # create a vector source_data %&gt;% abs() %&gt;% # take each element&#39;s absolute value sum() %&gt;% # sum all elements sqrt() # take the square root of the sum [1] 10.48809 The pipe operators make the entire codes more human-readable. With the pipe operator and formatting line breaks, developers can easily include annotations to improve their code readability, making it easier to communicate the thought process behind the code to others who may be reviewing or collaborating on the code. Now we understand the idiomatic expression of %&gt;%, our earlier rename() can be re-written as follows as well (cf. student1 and student1a): student %&gt;% rename( race = `race/ethnicity`, parent_edu = `parental level of education`, prep_course = `test preparation course`, math = `math score`, reading = `reading score`, writing = `writing score` ) -&gt; student1a student1a From now on, we will use the pipe-based syntax more often. The %&gt;% pipe operator, which originates from the magrittr library, has become a ubiquitous tool within the tidyverse suite of packages. Nevertheless, with the release of R 4.1, a new native pipe operator, namely |&gt;, has been introduced to the base R package. c(-10:10) |&gt; # create a vector abs() |&gt; # take each element&#39;s absolute value sum() |&gt; # sum all elements sqrt() [1] 10.48809 For more detail, you can check this YouTube clip: 7.4 mutate() Now imagine that you would like to create a new variable called final_grade, which is a weighted average of the student’s academic performance. Let us assume that you have the following weights in mind: math (50%), reading (25%), writing (25%). You can use mutate() to create a new column (i.e., variable) in your data frame: student1 %&gt;% mutate(final_grade = math * 0.5 + reading * 0.25 + writing * 0.25) We can create more than one new variables as well: student1 %&gt;% mutate(language = reading*0.5 + writing*0.5, ## new var 1 final_grade = math + language) ## new var 2 In the above practices of mutate(), we did not save the output of mutate() to a new object name. We only print the output directly to the console. In other words, the original data frame is still the same (i.e., student, student1); no new variables have been created with respect to these original data frames. 7.5 select() select() is to select particular columns of the data frame that you would like to focus on. You can perform conditional selection by specifying criteria for the column names. You can select just one column student1 %&gt;% select(math) Or multiple columns: student1 %&gt;% select(math, reading, writing) Through selection, you can re-order the columns as well: student1 %&gt;% select(reading, writing, math) You can select columns within a range of column names: student1 %&gt;% select(math:writing) You can also remove columns using select() student1 %&gt;% select(-c(race:lunch)) You can easily perform conditional selection through select() as well. To make this process more convenient, dplyr provides a set of selection helpers that you can use to filter columns based on various criteria. These helpers enable you to select columns that match certain patterns or conditions, such as columns that start with a certain string (starts_with()), columns that contain a specific character string (contains()), or columns that satisfy a regular expression (matches()). By using these helpers, you can easily and efficiently perform complex column selection operations without having to write lengthy code. student1 %&gt;% select(starts_with(c(&quot;l&quot;,&quot;m&quot;))) student1 %&gt;% select(contains(&quot;_&quot;)) student1 %&gt;% select(ends_with(&quot;ing&quot;)) student1 %&gt;% select(one_of(&quot;gender&quot;, &quot;race&quot;, &quot;parent_edu&quot;)) 7.6 filter() While select() subsets columns, filter()subsets rows. Most importantly, we can perform conditional selection on rows as well, i.e., to extract specific rows that meet specific criteria. One of the primary use cases for filter() is to extract rows based on their values in a particular column (i.e., variable). one logical condition student1 %&gt;% filter(math &gt; 90) AND &amp; conditions: student1 %&gt;% filter(math &lt; 40 &amp; reading &lt; 40) OR | conditions: student1 %&gt;% filter(math &lt; 40 | reading &lt; 40) XOR xor conditions: student1 %&gt;% filter(xor(math &lt; 40, reading &lt; 40)) Please check the row numbers of the above three filtered data frames. Any connection? Please check Chapter 4.1 Vector and Chapter 5 Conditions for more logical operations. Exercise 7.2 Suppose you have a data frame my_data with the following columns: “name”, “age”, “gender”, “income”, “education”, “occupation”. Write a single dplyr pipeline that accomplishes the following tasks: Filter the rows to include only those where the “gender” variable is “male” Filter the rows to include only those where the “education” variable is either “bachelor” or “master” Select only the columns “name”, “age”, and “occupation”. Please note that all of the above data transformations should be completed using a single pipeline of code. You cannot use multiple pipelines to perform the specified operations. ## Your code should look like this: my_data %&gt;% ....... %&gt;% ....... %&gt;% ....... %&gt;% ....... %&gt;% ....... ## The resulting data frame will autoprint itself in the console library(dplyr) my_data &lt;- data.frame( name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;, &quot;David&quot;, &quot;Emily&quot;), age = c(25, 30, 40, 35, 27), gender = c(&quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;), income = c(55000, 60000, 45000, 70000, 50000), education = c(&quot;bachelor&quot;, &quot;master&quot;, &quot;bachelor&quot;, &quot;doctorate&quot;, &quot;master&quot;), occupation = c(&quot;engineer&quot;, &quot;teacher&quot;, &quot;lawyer&quot;, &quot;doctor&quot;, &quot;writer&quot;) ) my_data Your single pipeline code should produce the following: 7.7 arrange() We can arrange the rows of the data frame according to a particular column/variable. student1 %&gt;% arrange(math) In R, the default sorting order for rows is ascending. To sort your data in descending order, you can use the desc() function on the variable name. student1 %&gt;% arrange(desc(math)) 7.8 group_by() and summarize() group_by() and summarize() are two functions that are often coupled together to generate summary statistics from a data frame by grouping the data based on a particular variable or factor. They are commonly used to analyze data by specific subgroups, such as gender, age, or location, allowing for deeper insights into the data. For example, in the dataset of student1, we can potentially group students into sub-groups in terms of their gender, age, or parental education levels. student1 %&gt;% group_by(gender) %&gt;% summarize(math_average = mean(math), math_median = median(math), math_sd = sd(math)) In the above example, we use group_by(gender) to split the original data into two sub data frames (i.e., one for male students and another for female students) based on students’ gender. Then, we use summarize() to compute summary statistics (i.e., mean, median, or standard deviation of the math scores) for each gender subgroup. In other words, summarize() allows us to apply specific functions to every sub data frame created by group_by(). Therefore, in the output data frame created by summarize(), its row number should be the same as the number of the grouping factor’s levels. In summary, group_by() and summarize() are essential tools for exploring and analyzing data by subgroups. They work in two steps: We split the data frame into smaller sub data frames based on a grouping variable using group_by(); We summarize each sub data frame with specific statistics/functions using the summarize(). In summarize(), there are a lot of powerful and useful functions that can be applied to the sub-data-frames created by group_by(). Please check the documentation of summarize() to learn how to use the following functions within summarize(): Center: mean(), median() Spread: sd(), IQR(), mad() Range: min(), max(), quantile() Position: first(), last(), nth(), Count: n(), n_distinct() Logical: any(), all() Another more complex example. We can quickly extract the number of students by gender and at the same time extract the 90%-quantile and 10%-quantile of their math scores by each gender group. student1 %&gt;% group_by(gender) %&gt;% summarize(N=n(), RANK90TH=quantile(math, 0.9), RANK10TH=quantile(math, 0.1)) Exercise 7.3 Using the student1 dataset, create a single pipeline of code that computes the percentage of students who fail on their maths (i.e., have a math score less than 60) for each gender. Please report the results as percentages rounded to the second decimal point. 7.9 count() One of the most-often used feature when we have data frames is to tally the frequencies of the subjects according to some of the columns. These columns are often categorical variables in the forms of character or factor (serving as grouping factors). The function count() is born for this. For example, we can create a frequency distribution of male and female students of different parental levels of education (i.e., parent_edu x gender contingency table): student1 %&gt;% count(parent_edu, gender) The function count() can be seen as a short-hand for group_by() + summarize(). Can you create the same frequency distributions as above using only group_by() and summarize()? Exercise 7.4 Continuing the above example, how can you create another column, which includes the percentage of male and female students for those of the same parental level education (see below)? Exercise 7.5 Suppose you have a data frame called my_data with the following columns: “id”, “name”, “gender”, “age”, “income”, “education”, and “occupation”. Write a dplyr pipeline that performs the following operations: Filter out rows where the “gender” variable is missing or where the “age” variable is less than 18 or greater than 65. Group the resulting data frame by “gender”. For each group, calculate the mean and standard deviation of “age” and “income”, the percentage of people with a graduate degree (“education” &gt; “college”), and the percentage of people who are doctors or engineers. Round all numeric data to the second decimal point. Convert all the column names to uppercase. Arrange the resulting data frame by “mean_income” in descending order. Please note that all of the above data transformations should be completed using a single pipeline of code. You cannot use multiple pipelines to perform the specified operations. library(dplyr) my_data &lt;- data.frame( id = 1:10, name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;, &quot;Dave&quot;, &quot;Eve&quot;, &quot;Frank&quot;, &quot;Grace&quot;, &quot;Heidi&quot;, &quot;Isaac&quot;, &quot;Judy&quot;), gender = c(&quot;F&quot;, &quot;M&quot;, NA, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;), age = c(25, 35, 45, 20, 60, 55, 40, 50, 30, 70), income = c(50000, 75000, 100000, 40000, 80000, 90000, 70000, 85000, 60000, 95000), education = c(&quot;high school&quot;, &quot;college&quot;, &quot;master&quot;, &quot;high school&quot;, &quot;college&quot;, &quot;college&quot;, &quot;master&quot;, &quot;PhD&quot;, &quot;college&quot;, &quot;PhD&quot;), occupation = c(&quot;teacher&quot;, &quot;engineer&quot;, &quot;manager&quot;, &quot;sales&quot;, &quot;analyst&quot;, &quot;doctor&quot;, &quot;teacher&quot;, &quot;manager&quot;, &quot;analyst&quot;, &quot;doctor&quot;) ) # Your code here Your code should produce the following output: 7.10 Tidy Data The concept of a tidy dataset is an important aspect of data science. According to Wickham &amp; Grolemund (2017), a tidy dataset must adhere to the following three principles: Each variable should have its own column. Each observation should have its own row. Each value should have its own cell. Despite these rules, real-life datasets are often not tidy. Rather than relying on others to clean up the data, it is crucial for data scientists to learn how to handle messy datasets themselves. Two common strategies for reshaping untidy datasets, as suggested by Wickham &amp; Grolemund (2017), are: Spreading one variable across multiple columns (from long to wide; pivot_wider()) Gathering one observation scattered across multiple rows (from wide to long; pivot_longer()) 7.10.1 A Long-to-Wide Example A common use case for data reshaping from long to wide is when values in a specific column could potentially be spread into several independent columns/variables. Let’s consider a dataset of students’ exam scores in different academic subjects, where each student has multiple scores in different exams. scores &lt;- data.frame( student_id = factor(rep(1:5, each = 3)), subject = rep(c(&quot;math&quot;, &quot;english&quot;, &quot;science&quot;), 5), score = sample(60:100, size = 15, replace = TRUE) ) scores The dataset is currently in a long format, which is not tidy because each subject/participant has more than one value in the dataset. In particular, the column subject contains values (i.e., math, english, science) that can be potentially spread into independent columns. In other words, we can reshape the dataset into a wide format, where each row represents a single student and their scores in different academic subjects are in separate columns. scores_wide &lt;- scores %&gt;% pivot_wider( names_from = subject, values_from = score, values_fill = 0 ) scores_wide The function tidyr::pivot_wider() reshapes the dataset into a wide format. There are two important parameters in pivot_wider(): names_from = ...: The column to take variable names from. Here it’s subject. values_from = ...: The column to take values from. Here it’s score. values_fill = ...: Default value to use for missing values in the new columns. In this case, we set it to 0 to represent missing exam scores. Having data in a wide format makes it easier to work with the new variables that have been originally embedded. (It would be more difficult if we want to complete the following computation based on the original long dataset.) scores_wide %&gt;% mutate(gpa = math*0.3 + english*0.4 + science*0.3) %&gt;% ggplot(aes(student_id, gpa, fill=student_id)) + geom_col() + labs(x = &quot;Student ID&quot;, y = &quot;GPA Scores&quot;, fill = &quot;Student ID&quot;) + scale_fill_grey() + theme_bw() 7.10.2 A Wide-to-Long Example A common use case for data reshaping from wide to long is when working with datasets that contain repeated measures. Let’s say we have a dataset with three variables: subject ID, pre-treatment blood pressure measurement, and post-treatment pressure measurement. data &lt;- data.frame( subject_id = c(1, 2, 3, 4), pre_treatment_bp = c(120, 130, 125, 118), post_treatment_bp = c(115, 127, 120, 112) ) data Using pivot_longer(), we can reshape the data so that we have a single column for time point (indicating whether it is pre- or post-treatment), and a single column for blood pressure measurements (indicating the BP measurement values): data_long &lt;- data %&gt;% pivot_longer( cols = c(&quot;pre_treatment_bp&quot;, &quot;post_treatment_bp&quot;), names_to = &quot;time_point&quot;, values_to = &quot;bp_measurement&quot; ) data_long Now we can see that we have a new column time_point that specifies whether the measurement was taken pre- or post-treatment, and the bp_measurement column contains the actual blood pressure measurements. The tidyr::pivot_longer() function is specifically designed for reshaping data from a wide format to a long format. This function takes three important parameters: cols = ...: This parameter specifies the set of columns whose names will be converted into values of a new column. names_to = ...: This parameter specifies the name of the new variable that will contain the above column names after the transformation. In this case, the new column name is called time_point. values_to = ...: This parameter specifies the name of the new variable that will contain the values after the transformation. In this case, the new column name is called bp_measurement. The conversion of a data frame from a wide format to a long format is an important step in data analysis and visualization. By transforming the data frame into a long format and having one independent column/variable, such as time_point, we can perform statistical analysis and visualization more efficiently and effectively. ## Easier to plot boxplots ## for each time point data_long %&gt;% ggplot(aes(time_point, bp_measurement, fill = time_point)) + geom_boxplot() It seems to be more intuitive that we have the pre-treatment boxplot goes before (on the left) the post-treatment boxplot. Any idea on how to do this? Please get familiar with tidyr::separate() and tidyr::unite(), which are two important functions to manipulate the columns of the data frame. Exercise 7.6 Suppose you have a data frame called my_data that contains measurements of a variable for different individuals at different days. The data frame has four columns: “id”, “day_1_value”, “day_2_value”, and “day_3_value”. Each row of the data frame represents a single measurement of the variable for a specific individual on a specific day. Write a dplyr pipeline that performs the following operations: Calculate the minimum, maximum, and mean values of the measurements for each subject and store the results in new columns. Pivot the data frame longer so that the “minimum”, “maximum”, and “mean” values are combined into a single column called “ValueType”. Create a new column called “Value” that includes each individual’s record for each value type. This column will have the minimum, maximum, and mean values for each subject repeated for each value type. Round the numeric data to the second decimal point. Please note that all of the above data transformations should be completed using a single pipeline of code. You cannot use multiple pipelines to perform the specified operations. my_data &lt;- data.frame( id = c(1, 2, 3, 4, 5), day_1_value = c(23, 25, 20, 21, 24), day_2_value = c(27, 29, 30, 28, 26), day_3_value = c(18, 19, 22, 20, 21) ) my_data Your expected output: 7.11 Exercises Exercise 7.7 Using the dataset demo_data/data-students-performance.csv, load it into R and filter the data to show only female students with math scores less than 40. The output should display only the gender and math columns.” Exercise 7.8 Using the same dataset from Exercise 7.7, compute the mean and standard deviation of math scores for different races. Additionally, please include the number of students for each race sub-group. Exercise 7.9 Using the same dataset from Exercise 7.7, create a summary data frame that includes the following information for students of different genders and parental education levels: The number of students The mean math score The standard deviation of math score Exercise 7.10 Using the same dataset from Exercise 7.7, transform the parent_edu variable in the dataset into an ordered factor, where the levels are defined according to the following order: some high school &lt; high school &lt; some college &lt; associate's degree &lt; bachelor's degree &lt; master's degree. Once the transformation is complete, create a summary data frame that includes the number of students, math mean scores, and math standard deviations, for students of different genders and parental education levels. The output should only include the following columns: gender, parent_edu, N, math_mean, and math_sd. Ensure that the parent_edu variable is ordered in the summary data frame according to the specified levels. Exercise 7.11 Using the same dataset from Exercise 7.7, create the following graphs using ggplot2. Your output graphs should be as close to the graphs provided below as possible. This is a grouped error plot. This chart displays the mean math scores of male and female students across different levels of parental education, which is represented on the x-axis. The color of the error bars, points, and lines represents the gender of the students, and the error bars show the 95% confidence intervals (computed based on mean_cl_boot()) of the mean math scores. This graph is a grouped bar plot with prep_course on the x-axis and math scores on the y-axis. The bars are grouped by gender and the mean score for each group is shown using a bar with error bars for the 95% confidence interval. The fill color of each bar corresponds to the gender. The graph allows the comparison of math scores between the two genders for each preparation course. This graph is a grouped bar plot with prep_course on the x-axis and math, reading, and writing scores on the y-axis. The bars are grouped by gender and the mean score for each group is shown using a bar with error bars for the 95% confidence interval. The fill color of each bar corresponds to the gender. The graph is split into three facets, one for each academic subject. The graph allows the comparison of scores between genders for each preparation course and across different subjects. The R code creates a grouped boxplot with one boxplot for each level of parental education and each gender. The fill color of the boxplots distinguishes between male and female students, and the x-axis displays the different levels of parental education, with labels aligned and rotated 90 degrees for readability. The y-axis shows the range of math scores in the dataset. Exercise 7.12 In this exercise, please first download the dataset demo_data/data-word-freq.csv. You may use readr::read_csv() to load the dataset into R. This is a dataset including word frequencies in two different corpora. For example, the word the appears 346 times in perl corpus but 229 times in python corpus. In other words, each row in word_freq in fact represents the combination of (WORD, CORPUS) because the column FREQ contains the values for those variables. In addition, the same word appears twice in the dataset in the rows (e.g., the, a). Please transform word_freq into a wider format, where the word frequencies in each corpus can be independent columns (as shown in the second table). If the word appears in only one of the corpora, its frequency would be 0. require(readr) word_freq &lt;- read_csv(&quot;demo_data/data-word-freq.csv&quot;) word_freq Exercise 7.13 Using the dataset from Exercise 7.12, filter the dataset by including only words with four or more characters and where the sum of their frequencies in Perl and Python Corpus is between 10 and 100. Then create a scatterplot with labeled points using ggplot2 and ggrepel. The x-axis represents the frequency of the word in Perl Corpus and the y-axis represents the frequency of the word in Python Corpus. The points are colored according to the preference for either Perl or Python Corpus based on the difference in their frequency values in the two corpora (if the frequency values are the same in both corpora, the preference of the word is deemed neutral). The words that have a frequency greater than 20 in either Perl or Python corpus are presented with word labels in the final graph. References Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. "],["data-visualization.html", "Chapter 8 Data Visualization 8.1 Why Visualization? 8.2 ggplot2 8.3 Variables and Data Type 8.4 One-variable Graph 8.5 Two-variable Graph 8.6 stat function 8.7 More Aesthetic Features 8.8 More Layers 8.9 Saving Plots 8.10 Exercises on iris 8.11 Exercises on COVID-19", " Chapter 8 Data Visualization 8.1 Why Visualization? Data visualization is very important. I would like to illustrate this point with two interesting examples. Datasaurus Dozen Dataset First, let us take a look at an interesting dataset—Datasaurus, which is available in demo_data/data-datasaurus.csv (source: Datasaurus data package. This data set was first created by Alberto Cairo). ## load data library(tidyverse) df &lt;- read_csv(&quot;demo_data/data-datasaurus.csv&quot;) Table 8.1: An Interesting Dataset group x y dino 95.38460 36.794900 dino 98.20510 33.718000 away 91.63996 79.406603 away 82.11056 1.210552 h_lines 98.28812 30.603919 h_lines 95.24923 30.459454 v_lines 89.50485 48.423408 v_lines 89.50162 45.815179 x_shape 84.84824 95.424804 x_shape 85.44619 83.078294 star 82.54024 56.541052 star 86.43590 59.792762 high_lines 92.24840 32.377154 high_lines 96.08052 28.053601 dots 77.92604 50.318660 dots 77.95444 50.475579 circle 85.66476 45.542753 circle 85.62249 45.024166 bullseye 91.72601 52.623353 bullseye 91.73554 48.970211 slant_up 92.54879 42.901908 slant_up 95.26053 46.008830 slant_down 95.44349 36.189702 slant_down 95.59342 33.234129 wide_lines 77.06711 51.486918 wide_lines 77.91587 45.926843 This data set includes 1846 rows (items), with three columns describing the properties of the items: group, x and y. As we have a grouping factor group, we can break the data set into several subsets by group and for each subset we compute their respective mean scores and standard deviations of x and y. According to the summary statistics of each sub-group (cf. Table 8.2), they all look quite similar in terms of each group’s mean and standard deviation of x and y: Table 8.2: An Interesting Dataset - Summary group x_mean y_mean x_sd y_sd away 54.266 47.835 16.770 26.940 bullseye 54.269 47.831 16.769 26.936 circle 54.267 47.838 16.760 26.930 dino 54.263 47.832 16.765 26.935 dots 54.260 47.840 16.768 26.930 h_lines 54.261 47.830 16.766 26.940 high_lines 54.269 47.835 16.767 26.940 slant_down 54.268 47.836 16.767 26.936 slant_up 54.266 47.831 16.769 26.939 star 54.267 47.840 16.769 26.930 v_lines 54.270 47.837 16.770 26.938 wide_lines 54.267 47.832 16.770 26.938 x_shape 54.260 47.840 16.770 26.930 df %&gt;% group_by(group) %&gt;% summarize( mean_x = mean(x), mean_y = mean(y), std_dev_x = sd(x), std_dev_y = sd(y), ) So it may be tempting for us to naively conclude that all groups show similar behaviors in x and y measures. But what if we plot all items according to their x and y values by group ? ggplot(df, aes(x = x, y = y, color = group)) + geom_point(alpha = .7, size = .8) + theme(legend.position = &quot;none&quot;) + facet_wrap( ~ group, ncol = 3) + labs(title = &quot;Scatter Plots of Each Group&quot;, x = &quot;X Values&quot;, y = &quot;Y Values&quot;) See? When we visualize our data, sometimes the patterns reveal themselves. What you see in numbers may sometimes be very misleading. Simpson’s Paradox Another example is Simpson’s Paradox, which refers to a statistical phenomenon where an association between two variables in a population emerges, disappears or reverses when the population is divided into sub-groups. For example, the following graph shows the association/correlation between x and y for the entire population. Based on the above graph, you would probably conclude that when x increases, y decreases. That is, the correlation analysis suggests a negative relationship between x and y when the entire population is analyzed as a whole. However, if we plot the scatter plots by groups (i.e., a Z grouping factor), you may get the opposite conclusions. All correlations between x and y in each sub-group are now positive. That is, the association you observe in the population now is reversed in each sub-group. 8.2 ggplot2 R is famous for its power in data visualization. In this chapter, I will introduce you a very powerful graphic library in R, ggplot2. For any data visualization, there are three basic elements: Data: The raw material of your visualization, i.e., a data frame. Aesthetics: The mapping of your data to aesthetic attributes, such as x, y, color, size, linetype, fill. Geometric Objects: The layers of geometric objects you would like to include on the plots, e.g., lines, points, bars, boxplots, etc. I will demonstrate some basic functions of ggplot2, with the pre-loaded dataset mpg: library(tidyverse) mpg To begin with data visualization, it is crucial to have a clear understanding of the dataset. This includes comprehending the definitions of the rows and columns in the data. For instance, in the dataset named mpg, every row pertains to a specific vehicle, while the columns comprise the following information: model: manufacturer model name displ: engine displacement, in litres (排氣量) hwy: highway miles per gallon cty: city miles per gallon cyl: number of cylinders (汽缸數目) class: car type drv: the type of drive train, where f = front-wheel drive (前輪驅動), r = rear wheel drive (後輪驅動), 4 = 4wd (四輪傳動) There are two very useful functions for exploration of a data frame: str() and summary(). str(mpg) tibble [234 × 11] (S3: tbl_df/tbl/data.frame) $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... $ trans : chr [1:234] &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... $ class : chr [1:234] &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... summary(mpg) manufacturer model displ year Length:234 Length:234 Min. :1.600 Min. :1999 Class :character Class :character 1st Qu.:2.400 1st Qu.:1999 Mode :character Mode :character Median :3.300 Median :2004 Mean :3.472 Mean :2004 3rd Qu.:4.600 3rd Qu.:2008 Max. :7.000 Max. :2008 cyl trans drv cty Min. :4.000 Length:234 Length:234 Min. : 9.00 1st Qu.:4.000 Class :character Class :character 1st Qu.:14.00 Median :6.000 Mode :character Mode :character Median :17.00 Mean :5.889 Mean :16.86 3rd Qu.:8.000 3rd Qu.:19.00 Max. :8.000 Max. :35.00 hwy fl class Min. :12.00 Length:234 Length:234 1st Qu.:18.00 Class :character Class :character Median :24.00 Mode :character Mode :character Mean :23.44 3rd Qu.:27.00 Max. :44.00 To begin with, I like to use one simple example to show you how we can create a plot using ggplot2. With the dataset mpg, we can look at the relationship between displ and hwy: whether the engine displacement has to do with the car miles per gallon. We can draw a scatter plot as shown below. ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() A ggplot object often includes lat least three important components: ggplot() initializes the basic frame of the graph, with data = mpg specifying the data frame on which the plot is built aes() further specifies the mapping of axises and the factors in the data frame. aes(x = displ, y = hwy) indicates that displ is mapped as the x axis and hwy as y axis + means that you want to add one layer of the graph to the template. geom_point() means that you want to add a layer of point graph. 8.3 Variables and Data Type When creating the graphs for your data, you need to know very well the data type of all the variables to be included in the graph. There are at least three important data types you need to know: Categorical variables: these variables usually have only limited set of discrete values, i.e., levels. They are usually coded as character vector or factor in R. Numeric variables: these variables are continuous numeric values. They are usually coded as numeric vector. Date-Time variables: these variables, although being numeric sometimes, refer to calendar dates or times. They are usually coded as Date-TimeClasses in R. The general principle in data visualization is that always pay attention to the data type for variables on the x-axis and y-axis. 8.4 One-variable Graph If your graph includes only one variable from the data, usually this would indicate that you are interested in the distribution of the variable. 8.4.1 Continuous Variable Histogram Density plot ggplot(data = mpg, aes(hwy)) + geom_histogram(color=&#39;white&#39;) ggplot(data = mpg, aes(hwy)) + geom_density(kernel=&quot;gaussian&quot;) We can also combine the histogram and density plots into one: Any thoughts about how to do that? The way we examine the distribution of the continuous variable (i.e., numbers) is to divide the entire range of values into a series of intervals, i.e., bins, and then count how many values in the data set fall into each interval. In other words, the shape of your histogram may vary depending on two parameters: Number of bins: the number of intervals you have Bin width: the size of each interval Changes of either of the parameters would lead to a histogram of a different shape. ggplot(data = mpg, aes(hwy)) + geom_histogram( color = &#39;white&#39;, fill = &#39;steelblue&#39;, alpha = 0.7, bins = 10 ) + scale_x_continuous(breaks = seq(10, 46, 1)) ggplot(data = mpg, aes(hwy)) + geom_histogram( color = &#39;white&#39;, fill = &#39;steelblue&#39;, alpha = 0.7, binwidth = 2 ) + scale_x_continuous(breaks = seq(10, 46, 1)) ## You can check the min or max of each bin g &lt;- ggplot(data = mpg, aes(hwy)) + geom_histogram(color = &quot;white&quot;) ## Auto-print the ggplot g ## Checking bin interval min and max ggplot_build(g)$data[[1]]$xmin [1] 11.58621 12.68966 13.79310 14.89655 16.00000 17.10345 18.20690 19.31034 [9] 20.41379 21.51724 22.62069 23.72414 24.82759 25.93103 27.03448 28.13793 [17] 29.24138 30.34483 31.44828 32.55172 33.65517 34.75862 35.86207 36.96552 [25] 38.06897 39.17241 40.27586 41.37931 42.48276 43.58621 ggplot_build(g)$data[[1]]$xmax [1] 12.68966 13.79310 14.89655 16.00000 17.10345 18.20690 19.31034 20.41379 [9] 21.51724 22.62069 23.72414 24.82759 25.93103 27.03448 28.13793 29.24138 [17] 30.34483 31.44828 32.55172 33.65517 34.75862 35.86207 36.96552 38.06897 [25] 39.17241 40.27586 41.37931 42.48276 43.58621 44.68966 8.4.2 Categorical Variable Bar plot ggplot(data = mpg, aes(x = class)) + geom_bar() When creating the bar plot, we can also use the normalized frequencies of each category, instead of the raw frequency counts. Any idea? Exercise 8.1 How can we create a bar plot as above but with the bars arranged according to the counts in a descending order from left to right? (see below) Hint: check reorder() 8.5 Two-variable Graph If your graph includes two variables, then very likely one variable would go to the x-axis and the other, y-axis. Depending on their data types (categorical or numeric), you may need to create different types of graphs. 8.5.1 Continuous X, Continuous Y Scatter Plot ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() We can add a regression line to the scatter plot: ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth(method=&#39;lm&#39;, formula= y~x) 8.5.2 Categorical X, Continuous Y Boxplot ggplot(data = mpg, aes(x = class, y = hwy)) + geom_boxplot() If you would like to know more about boxplots, please check this blog post. The following illustration is taken from the blog post, which shows the meanings of different boxplot parts. Error Plot ggplot(data = mpg, aes(x = class, y = hwy)) + stat_summary(fun.data = mean_cl_boot, geom = &quot;pointrange&quot;) The functions, mean_cl_normal() and mean_cl_boot() are two wrappers around functions from Hmisc library. The mean_cl_normal() computes the mean and the confidence limits based on a t-distribution. The mean_cl_boot would produce a less assumption laden bootstrapped confidence interval. ggplot(data = mpg, aes(x = class, y = hwy)) + stat_summary(fun.data = mean_cl_normal, geom = &quot;pointrange&quot;) If you run into problems plotting the error plot using stat_summary(), probably you did not have the necessary packages installed in your current R environment. Please make sure that you have installed the package tidyverse or ggplot2 properly without any error messages in the process of installation. Also, if you run into an error here, please try to install the tidyverse from source again. (For the other relevant packages, it is ok to install those packages in a normal way from CRAN). For more detail, please refer back to Chapter 2.10. 8.5.3 Categorical X, Categorical Y Bubble Plot Heatmap ggplot(data = mpg, aes(x = manufacturer, y = class)) + geom_count() + theme(axis.text.x = element_text(angle=-90)) We can also create a heatmap for two categorical variables: Exercise 8.2 Please create a heat map as shown above. In order to create a heat map, you may also need the frequency counts of each level combination. Also, please include these frequency counts in the heat map as well. Hint: geom_tile(); geom_text() 8.6 stat function Previously, we used the stat_summary() function to create an error plot. Now let’s explore other stat_*() functions available in ggplot2. When plotting data, sometimes we supply values directly for the aesthetic mapping (i.e., y values). However, other times, the values are derived from the data through specific transformations or computations. In ggplot2, some geom functions automatically perform data transformations before creating the plot, such as: geom_bar(): computes the frequency counts for the levels of x geom_smooth(): computes the best fit of the data geom_boxplot(): computes the necessary statistics for the boxplots. In addition, ggplot2 provides several statistics transformation functions, which take the form of stat_*(). These functions usually: Compute the transformed values based on the original y values in the aesthetic mapping. Create a geom layer that corresponds to the transformed values. Each stat_*() function has its own default geom object, just as each geom_*() function has its own default statistics transformation. To access the computed/transformed values produced by stat_&lt;compute variable&gt;(), we can use the following two methods: after_stat(&lt;compute variable&gt; or ..&lt;computed variable&gt;... You can check the function documentation for all the computed variables produced by the stat_*() function. The following code demonstrates how to use stat_count() to compute frequency counts and then a bar plot: ## aes can be put in `ggplot()` ggplot(data = mpg, aes(x = class)) + stat_count() ## or `stat_count() ggplot(data = mpg) + stat_count(aes(x=class)) We can change the default geom in stat_count(), as shown below. ## We can change the default geom in stat_count() ggplot(data = mpg, aes(x = class)) + stat_count(geom = &quot;point&quot;) So we can make use of the computed variable (e.g., after_stat(count), after_stat(prop)) from stat_count() to create more advanced graphs: ggplot(data = mpg, aes(x = class)) + stat_count( aes(y = after_stat(prop), group =1), geom=&quot;bar&quot;, fill=&quot;white&quot;, color=&quot;lightgrey&quot; ) + stat_count( aes(y = after_stat(prop), label = round(after_stat(prop),2), group =1), geom= &quot;text&quot;, color = &quot;royalblue&quot; ) This code snippet creates a bar plot with text labels using the computed prop variable. The prop in stat_count() is defined as the groupwise proportion, which means that it is computed based on the number of observations in each group. By default, the group parameter is set to x. This means that stat_count will calculate the proportion of different x values in different x groups. The final result will be either 1 or 0, which cannot be seen in the plot. If you want to calculate the prop in the entire dataset, you can set the group parameter to a fixed value. The type of the value is not important, but it needs to be fixed to ensure that the proportion is calculated based on the entire dataset rather than just a subset of the data. 8.7 More Aesthetic Features 8.7.1 color Now I would like to demonstrate how we can add additional aesthetic mappings to your graphs. Earlier we create a scatter plot using the following code: ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() The above plot includes two variables into the graph, x = displ and y = hwy. The additional aesthetic features may include things like colors, sizes, shapes, line-types, widths etc. The idea is that we can introduce a third variable into the plot by mapping the variable to one of these aesthetic features, i.e., modifying these aesthetic attributes based on the value of that third variable. For example, you can add color = ... in the aes(x = ..., y = ..., color = ...) to create the graphs on the basis of another grouping factor. ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() In the above example, color is an aesthetics (put in the aes()). This would suggest that the color of each point is now mapped to the variable drv. In this case, points belonging to different groups of drv would be of different colors—different drive train types have different colors in points. Note that the x-coordinates and y-coordinates are aesthetics too, and they are mapped to the displ and hwy variables, respectively. Now we enrich the graph by further mapping the color to the third variable drv, which indicates whether a car is front wheel drive, rear wheel drive, or 4-wheel drive. If you would like to know more about the color names available in R, I would highly recommend this R Color Cheat Sheet. Exercise 8.3 When creating a graph, the aesthetic feature color can also be specified within the geom_*() as well. Please compare the following two ways of color specification and describe their respective functional differences. ## Method 1 ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() ## Method 2 ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point(color = &quot;steelblue&quot;) 8.7.2 alpha Transparency (alpha) can sometimes be helpful in data visualization. ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point(alpha = .5, size = 4) 8.7.3 size We can also map a grouping factor to the aesthetic feature size. That is, different groups will be represented by geometric objects of varying sizes. ggplot(data = mpg, aes(x = displ, y = hwy, size = drv, color= drv)) + geom_point(alpha = .5) 8.7.4 fill For bar plots or histograms, we can fill the bars with different colors by adding fill = ... in the aes(). ggplot(data = mpg, aes(x = class, fill = class)) + geom_bar(color = &#39;white&#39;) ggplot(data = mpg, aes(x = class, y = hwy, fill = class)) + geom_boxplot(color = &#39;black&#39;, size = 0.2, notch = TRUE) In ggplot2, both color and fill are two aesthetic mappings that can be used to distinguish between groups in a plot. color changes the outline color of a graphical element, while fill changes the interior fill color or pattern of a graphical element. 8.7.5 shape We can map a third variable to the graph using shape as well. ggplot(data = mpg, aes(x = displ, y = hwy, shape = drv)) + geom_point() And of course you can map both shape and color to the same third variable: ggplot(data = mpg, aes( x = displ, y = hwy, color = drv, shape = drv )) + geom_point() Exercise 8.4 In Section 8.5, we talked about how to create a bubble plot. ggplot(data = mpg, aes(x = manufacturer, y = class)) + geom_count() + theme(axis.text.x = element_text(angle=-90)) Please adjust the codes to create a similar bubble plot with not only the sizes but also the colors of the bubbles indicating the varying token numbers in each level combination. 8.8 More Layers 8.8.1 geom_... Layers The ggplot object consists of layers of geometric objects. We can also add another geom_*() object, such as a smooth line by using the +: ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Could you predict what kind of graph you would get with the following code? ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth(method = &quot;lm&quot;) 8.8.2 Labels and Annotations We can add self-defined labels of the x and y axes and main/sub titles to the graphs using labs(). (By default, ggplot2 will utilize the original variable names [i.e., column names] for x and y labels.) ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + labs(title = &quot;Correlation between Displacement and Highway Miles per Gallon&quot;, x = &quot;Displacement&quot;, y = &quot;Miles/Per Gallon&quot;) ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + labs(x = &quot;Engine Displacement (litres)&quot;, y = &quot;Highway Miles per Gallon&quot;, title = &quot;Scatter Plot -- DISPL by HWY&quot;, color = &quot;Drive Train Type&quot;) 8.8.3 Facets Sometimes we may want to create plots based on a conditional factor. For example, we can check the relationship between city milage (cty) and highway milage (hwy) for cars by different manufacturers (class). ggplot(data = mpg, aes(x = cty, y = hwy)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + facet_wrap(vars(class)) Please check facet_grid() on your own. 8.8.4 Themes We can easily change the aesthetic themes of the ggplot by adding one layer of theme_*(). ## Save the ggplot2 object graph1 &lt;- ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point(size = 3, alpha = .8) + labs( x = &quot;Engine Displacement (litres)&quot;, y = &quot;Highway Miles per Gallon&quot;, title = &quot;Scatter Plot -- DISPL by HWY&quot;, color = &quot;Drive Train Type&quot; ) ## autoprint the graph graph1 ## add theme layers graph1 + theme_bw(base_family = &quot;Times New Roman&quot;) graph1 + theme_minimal() graph1 + theme_dark() graph1 + theme_classic() graph1 + theme_light() In addition the the default theme template provided in ggplot2, you can check ggthemes library for more fancy predefined themes available for your data visualization. 8.9 Saving Plots Saving a ggplot can be easily done by ggsave(). You can first assign a ggplot object to a variable and then use ggsave() to output the ggplot object to an external file. It is recommended to use common image formats for publications, e.g., png, jpg. Also, please remember to set the width and height (in inches) of your graph. These settings will greatly affect the look of the graph in print. my_first_graph &lt;- ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point() + labs(x = &quot;Engine Displacement (litres)&quot;, y = &quot;Highway Miles per Gallon&quot;, title = &quot;Scatter Plot -- DISPL by HWY&quot;) class(my_first_graph) # check the class [1] &quot;gg&quot; &quot;ggplot&quot; # summary(my_first_graph) # check the properties of the graph my_first_graph # auto-print the ggplot ggsave( filename = &quot;my_first_plot.png&quot;, plot = my_first_graph, width = 6, height = 6 ) Useful References The R Graph Gallery is a website where you can find lots of fancy graphs created with R. Most importantly, you can study their R codes and learn how to create similar fancy graphs with your own data. Highly recommend it!! The ggplot2 Official Documentation Website provides a comprehensive list of all functions included in the package. Very useful! R Graphics Cookbook, 2nd Edition is a great on-line book, which provides hundreds of examples of high-quality graphs produced with R. 8.10 Exercises on iris The following exercises will use the preloaded dataset iris in R. iris Exercise 8.5 Please create a scatter plot showing the relationship between Sepal.Length and Petal.Length for different iris Species. Also, please add the regression lines for each species. Your graph should look as close to the sample as possible. Please create a boxplot showing the Petal.Width distributions of each iris Species. Exercise 8.6 Please make boxplots that display the distributions of Petal.Width and Sepal.Width for different iris Species on a single graph. To accomplish this task, you may have to convert your data into a longer format, which you can achieve using tidyr::pivot_longer(). Please refer to Chapter 7 for assistance with this exercise. Exercise 8.7 Create an error plot that displays the means and confidence intervals of the Petal.Width, Petal.Length, Sepal.Width, and Sepal.Length for each of the three iris species in the iris dataset. In the final plot, the error bars should be presented according to flower parts (Sepal or Petal) and measurement types (Width or Length) in separate panels. To calculate the confidence intervals, use mean_cl_normal(). You may need to reference the materials in Chapter 7 to effectively transform and manipulate the data required for this exercise. 8.11 Exercises on COVID-19 The exercises in this section utilize a dataset obtained from Kaggle, which can be found in demo_data/data-covid19.csv. To successfully complete these exercises, you may need to reference the materials in Chapter 7 beforehand. Exercise 8.8 Load the dataset in demo_data/data-covid19.csv into R as a data frame named covid19. Hint: Check readr::read_csv() In this dataset, it’s important to note that the Confirmed, Deaths, and Recovered columns all contain cumulative counts of COVID-19 cases, deaths, and recoveries, respectively, on different days. This cumulative data allows us to track the progression of the disease over time. Additionally, for countries such as Mainland China, the data is reported by Province/State (same for US). To obtain the total number of confirmed cases on a specific day for the entire country, you will need to sum up the numbers from each individual province or state first. Exercise 8.9 Use ggplot2 to create a line plot showing the number of confirmed cases by month for the following countries: Taiwan, Japan, US, UK, Germany, Netherlands, Mainland China. A sample graph is provided below. Hint: Please check the documentation of ggplot2 on Annotations: Log ticks marks. Exercise 8.10 Create a bar plot showing the top 10 countries ranked according to their number of confirmed cases of the COVID19. Exercise 8.11 Create a bar plot showing the top 10 countries ranked according to their death rates of the COVID19. (Death rates are defined as the number of deaths divided by the number of confirmed cases.) Exercise 8.12 (BONUS(Optional!!)) Create a world map showing the current outbreak of covid19. Hint: Please check ggplot2::geom_polygon() and the package library(maps). This exercise is made to see if you know how to find resources online for more complex tasks like this. Please note that the country names may not match. "],["string-manipulation.html", "Chapter 9 String Manipulation 9.1 What is Regular Expression? 9.2 String Basics 9.3 Regular Expression Grammar 9.4 Pattern Matching 9.5 Advanced Pattern Matching 9.6 Recap 9.7 More Practices 9.8 Case Study: Chinese Four-Character Idioms", " Chapter 9 String Manipulation In the field of data analysis, a significant portion of the data usually consists of text or strings. As such, it is often necessary to use various techniques for string manipulations such as finding, replacing, removing, combining, or splitting strings. A comprehensive understanding of string processing is essential for a competent data scientist. One of the most important tools for effective string manipulation is regular expressions, which can be used to match, extract, and manipulate specific patterns within strings. In this chapter, we will cover several frequently-used techniques for string manipulation, with a particular focus on regular expressions. We will use the stringr package to illustrate the various functions and methods for string manipulation. The stringr package is part of the tidyverse framework, which provides a suite of packages for data wrangling and analysis in R. library(tidyverse) ## library(stringr) # loaded when loading tidyverse 9.1 What is Regular Expression? In text processing, it is common to perform “find-and-replace” operations in documents. This is a routine task in programs like MS-Word or MS-Excel, where specific processing is performed on sets of strings by locating and modifying them. Regular expressions are a powerful tool in this domain, allowing us to create a schematic textual pattern that can be used to match other strings that fit this pattern. The ability to use one pattern for multiple matches is the beauty of regular expressions. Regular expressions have several advantages, including: Efficient pattern matching Checking the format of email addresses, phone numbers, etc. Identifying reduplicated strings Controlling date format Information extraction and text mining Extracting texts according to a specific format Identifying proper names, email addresses, phone numbers, etc. The one-to-many mapping nature of regular expressions enables us to retrieve strings with similar properties in a simpler and more coherent way. Another advantage of regular expressions is that with the knowledge of regular expression and coding capability, you can perform pattern matching tasks on any machine-readable dataset (i.e., corpora) without being limited to the platform created by the corpus provider. Without regular expression knowledge, every time we need to retrieve patterns from a corpus, we must learn the system-specific syntax for corpus query, which may limit us to what the platform is capable of. However, it is important to note that access to the full-text corpus data is required, and a licensed use must be obtained beforehand. That’s another issue !) 9.2 String Basics Before we introduce regular expressions, let’s look at some of the basic string-related functions. Functions from the library stringr often start with str_*(). There are three basic functions: str_length(): get the length of the string (i.e., number of characters) word_string &lt;- c(&quot;the&quot;, &quot;word&quot;, &quot;string&quot;) word_string %&gt;% str_length [1] 3 4 6 str_c(): combine strings into a longer one str_c(&quot;the&quot;,&quot;word&quot;,&quot;string&quot;) [1] &quot;thewordstring&quot; str_c(&quot;the&quot;,&quot;word&quot;,&quot;string&quot;,sep = &quot;_&quot;) [1] &quot;the_word_string&quot; From the above output, can you tell what is the default value for the argument str_c(..., sep = ...)? Please note that the following code generates a different result from the above. Can you tell the differences? How can you create exactly the same results by using str_c(word_string,...)? Please check ?str_c. str_c(word_string, sep = &quot;_&quot;) [1] &quot;the&quot; &quot;word&quot; &quot;string&quot; Can you predict the outputs of the following code? x &lt;- letters[1:10] y &lt;- c(&quot;suffix1&quot;, &quot;suffix2&quot;) z &lt;- LETTERS[1:10] str_c(x, y) str_c(x, z) str_c(x, z, sep = &quot;_&quot;) str_c(x, collapse=&quot;&quot;) str_c(x, collapse=&quot; &quot;) str_c(x, z, collapse=&quot;&quot;) str_c(x, z, collapse=&quot; &quot;) str_c(x, z, sep = &quot;_&quot;, collapse = &quot; &quot;) str_c(x, z, x, sep=&quot;_&quot;) When we have several vectors of the same lengths in the str_c(), we can use the argument str_c(..., sep = ...) to combine each pair of the corresponding elements in the vectors with the sep as the separator. x &lt;- letters[1:10] y &lt;- c(&quot;suffix1&quot;, &quot;suffix2&quot;) z &lt;- LETTERS[1:10] str_c(x, z, sep = &quot;_&quot;) [1] &quot;a_A&quot; &quot;b_B&quot; &quot;c_C&quot; &quot;d_D&quot; &quot;e_E&quot; &quot;f_F&quot; &quot;g_G&quot; &quot;h_H&quot; &quot;i_I&quot; &quot;j_J&quot; str_c(x, z , sep = &quot;/&quot;) [1] &quot;a/A&quot; &quot;b/B&quot; &quot;c/C&quot; &quot;d/D&quot; &quot;e/E&quot; &quot;f/F&quot; &quot;g/G&quot; &quot;h/H&quot; &quot;i/I&quot; &quot;j/J&quot; When we have only one vector (but it is a multiple-element character vector) in the str_c(), we can use the argument str_c(..., collpase = ...) to collapse the vector into a long single string with the collapse as the delimiter of the original vector elements. The default value is collapse = NULL (i.e., str_c() by default does not collapse the vector.) str_c(x, collapse = &quot;&quot;) [1] &quot;abcdefghij&quot; str_c(x, collapse = &quot; &quot;) [1] &quot;a b c d e f g h i j&quot; str_c(x, collapse = &quot;_&quot;) [1] &quot;a_b_c_d_e_f_g_h_i_j&quot; When we have one or more vectors that are of the same lengths in the str_c(), we can use the argument str_c(..., sep = ...., collpase = ...) to collapse the vectors into a long single string, with the sep as the separator for each pair of elements in the vectors, and the collapse as the delimiter of the paired elements. str_c(x, z, sep=&quot;_&quot;, collapse=&quot; &quot;) [1] &quot;a_A b_B c_C d_D e_E f_F g_G h_H i_I j_J&quot; str_c(x, z, sep=&quot;-&quot;, collapse=&quot;/&quot;) [1] &quot;a-A/b-B/c-C/d-D/e-E/f-F/g-G/h-H/i-I/j-J&quot; str_sub(): extract part of the string by positions str_sub(string = &quot;international&quot;, start = 1, end = 5) [1] &quot;inter&quot; Please note that in R most of the functions are vectorized (i.e., if you apply a vectorized function to a vector, it will perform the same operation on each element of the vector and return a new vector with the results.) ## create a vector set.seed(12) random_fruits &lt;- sample(fruit, 5) ## `fruit` is a default vector random_fruits [1] &quot;purple mangosteen&quot; &quot;loquat&quot; &quot;rambutan&quot; [4] &quot;tangerine&quot; &quot;grape&quot; ## str_sub() all strings in the vector str_sub(random_fruits, start = 1, end = 3) [1] &quot;pur&quot; &quot;loq&quot; &quot;ram&quot; &quot;tan&quot; &quot;gra&quot; 9.3 Regular Expression Grammar Now let’s look at the grammar of regular expressions in more detail. In this section, we will discuss the standard Perl-compatible regular expression syntax. This is by now the most widely used version of regular expressions in most programming languages. To start with, in stringr, there is a very useful function, str_view(STRING, PATTERN), which can show us the match of the pattern in the string in a visually intuitive way. Let’s look at a simple regular expression pattern: \".a\". It is a pattern that matches any two-character sequence where the first character can be any character (represented by the period .) and the second character must be “a”. x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) ## Show only strings with the match str_view(string = x, pattern = &quot;.a&quot;) [2] │ &lt;ba&gt;&lt;na&gt;&lt;na&gt; [3] │ p&lt;ea&gt;r ## Show all strings of the input x str_view_all(string = x, pattern = &quot;.a&quot;) [1] │ apple [2] │ &lt;ba&gt;&lt;na&gt;&lt;na&gt; [3] │ p&lt;ea&gt;r In the above example, the matches of the (regular) pattern are surrounded by &lt; &gt; in the output of str_view(). Also, we know that the pattern (i.e., .a) exists only in the second and third strings of x. You can also use RegExplain addin to test your regular expressions. RegExplain is a very useful tool for the use of regular expressions–RegExplain, which is an RStudio addin. It allows you to: interactively build your regexp and check the output of common string matching functions use the included resources to learn regular expressions consult the interactive help pages This is very useful because you can prepare the regular expressions and use them in the code chunk after you have made sure that they work properly in the RegExplain. You can install the addin using remotes: ## Please install `remotes` if you haven&#39;t install.packages(&quot;remotes&quot;) remotes::install_github(&quot;gadenbuie/regexplain&quot;) For basic syntax of regular expressions, I will use this str_view() to show you how the regular pattern works in string-matching. Depending on your purpose, we can retrieve different information from each element of x based on this regular expression: str_detect(string = x, pattern = &quot;.a&quot;) ## Boolean [1] FALSE TRUE TRUE str_match(string = x, pattern = &quot;(.)a&quot;) ## Exact matches (group) [,1] [,2] [1,] NA NA [2,] &quot;ba&quot; &quot;b&quot; [3,] &quot;ea&quot; &quot;e&quot; str_match_all(string = x, pattern = &quot;(.)a&quot;) ## Exact matches all (group) [[1]] [,1] [,2] [[2]] [,1] [,2] [1,] &quot;ba&quot; &quot;b&quot; [2,] &quot;na&quot; &quot;n&quot; [3,] &quot;na&quot; &quot;n&quot; [[3]] [,1] [,2] [1,] &quot;ea&quot; &quot;e&quot; str_extract(string = x, pattern = &quot;.a&quot;) ## Exact matches [1] NA &quot;ba&quot; &quot;ea&quot; str_extract_all(string = x, pattern = &quot;.a&quot;) ## Exact matches all [[1]] character(0) [[2]] [1] &quot;ba&quot; &quot;na&quot; &quot;na&quot; [[3]] [1] &quot;ea&quot; str_subset(string = x, pattern = &quot;.a&quot;) ## Input strings with matches [1] &quot;banana&quot; &quot;pear&quot; str_view() has two useful functions: to see how a pattern matches to print the ambiguous zero-width characters in the string. str1 &lt;- &quot;hello world!&quot; str2 &lt;- &quot;hello world!&quot; str_view(str1) [1] │ hello world! str_view(str2) [1] │ hello{\\u3000}world! With str_view(), we can check whether the string contains any unusual whitespace (i.e., all whitespaces apart from \" \" and \"\\n\"). In the above example, the \"\\u3000\" is a full-width whitespace, often used in Chinese text. 9.3.1 Metacharacters To implement the idea of one-to-many mapping, RegEx defines several metacharacters, which are of special use in regular expressions. Their meanings are NOT the same as their literal counterparts. In RegEx, . is a special character, referring to any character: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(string = x, pattern = &quot;.a.&quot;) [2] │ &lt;ban&gt;ana [3] │ p&lt;ear&gt; The above regular expression .a. is a pattern that matches any three-character sequence where: the first character can be any character (i.e., .); the second character must be a; the third character can by any character (i.e., .). What if you need to match the period . symbol literally in your string? In this case, you would need to use an escape character \\. x &lt;- c(&quot;apple&quot;, &quot;banana&quot;,&quot;pear&quot;, &quot;orange. And&quot;) str_view(string = x, pattern = &quot;.&quot;) [1] │ &lt;a&gt;&lt;p&gt;&lt;p&gt;&lt;l&gt;&lt;e&gt; [2] │ &lt;b&gt;&lt;a&gt;&lt;n&gt;&lt;a&gt;&lt;n&gt;&lt;a&gt; [3] │ &lt;p&gt;&lt;e&gt;&lt;a&gt;&lt;r&gt; [4] │ &lt;o&gt;&lt;r&gt;&lt;a&gt;&lt;n&gt;&lt;g&gt;&lt;e&gt;&lt;.&gt;&lt; &gt;&lt;A&gt;&lt;n&gt;&lt;d&gt; In regular expression syntax, the backslash \\ is used to indicate that the following character should be treated literally, rather than as a metacharacter. However, in R, the backslash itself is also a metacharacter. Therefore, to use the backslash as an escape character, you need to double it up, like \\\\. The first backslash tells the R engine that the second backslash should be taken literally, and the second backslash tells the regular expression engine that the following character should be taken literally as a period. str_view(string = x, pattern = &quot;\\\\.&quot;) [4] │ orange&lt;.&gt; And In the above example, the . metacharacter matches any character except for a newline character, so it will match all characters in the string. However, in the second example, the \\\\. regular expression matches only the period character, because the backslash is used to escape the period and tell the regular expression engine to treat it literally. So, if you have the following three elements in x. How can you identify the double quotes \" in the strings? my &quot;apple&quot; banana apple peel [1] │ my &lt;&quot;&gt;apple&lt;&quot;&gt; [2] │ banana [3] │ apple peel 9.3.2 Anchors RegEx defines a few metacharacters, which serve as anchors in pattern matching. These anchoring metacharacters allow us to find a match in a particular position of the string (e.g., at the beginning/ending of the string). Most importantly, these anchors are zero-width. ^: The start of the string x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;^a.&quot;) ## pattern for two-character match [1] │ &lt;ap&gt;ple $: The end of the string x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;.a$&quot;) ## pattern for two-character match [2] │ bana&lt;na&gt; x &lt;- c(&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;) str_view(x, &quot;^apple$&quot;) ## pattern for five-character match [2] │ &lt;apple&gt; The anchors are evaluated according to the base unit you are matching (i.e., each element in the input string vector). In our previous examples, the RegEx pattern is applied to find a match in each element of the vector. The vector includes words. Therefore, the ^ indicates a word-initial position; \\$ indicates a word-final position. If you have a vector of sentences, the ^ would indicate a sentence-initial position; \\$ would indicate a sentence-final position. See below: x &lt;- c(&quot;apple is good&quot;, &quot;banana is better than apple&quot;, &quot;an apple a day keeps the doctor away&quot;) str_view(x, &quot;^apple&quot;) [1] │ &lt;apple&gt; is good str_view(x, &quot;apple$&quot;) [2] │ banana is better than &lt;apple&gt; 9.3.3 Character Set RegEx also defines a few character sets because in our pattern-matching, the characters included in a set often show similar behaviors. Common predefined character sets include: \\\\d: a one-character pattern that matches any digit \\\\s: a one-character pattern that matches any whitespace (e.g. space, tab, newline) \\\\w: a one-character pattern that matches any alphanumeric character .: one character pattern that matches any character except the line break x&lt;-c(&quot;apple&quot;,&quot;apple123&quot;,&quot;banana1&quot;) str_view(string = x , pattern = &quot;\\\\d&quot;) [2] │ apple&lt;1&gt;&lt;2&gt;&lt;3&gt; [3] │ banana&lt;1&gt; In pattern-matching, very often you will have cases where one base unit may have more than one match. In the previous example, str_view() prints only the elements with at least one match from x. You can check whether each element of x has a match or not using str_view_all(). str_view_all(string = x , pattern = &quot;\\\\d&quot;) [1] │ apple [2] │ apple&lt;1&gt;&lt;2&gt;&lt;3&gt; [3] │ banana&lt;1&gt; Please compare the two metacharacters, \\\\w and ., and examine their differences in their matches. In particular, which characters are not included in the character set defined by \\\\w? x &lt;- c(&quot;aeiouAEIOU1234_ .\\\\$%-*()&quot;) str_view(string = x, pattern = &quot;\\\\w&quot;, html = T) #compare str_view(string = x, pattern = &quot;.&quot;, html = T) 9.3.4 Alternatives Section 9.3.3 describes a few prefined character sets in RegEx. We can also define our own character set using the square brackets [ ]. And we can use [^...] to define a complementary character set. (Please note that ^ has a different RegEx meaning within [].) [abc]: a one-character pattern that matches a, b, or c (inclusive set) [^abc]: a one-character pattern that matches any character except a, b, or c (exclusive set) x &lt;- c(&quot;grey&quot;, &quot;gray&quot;) str_view(string = x, pattern = &quot;gr[ea]y&quot;) [1] │ &lt;grey&gt; [2] │ &lt;gray&gt; If you know very well which characters you are to match/find, use inclusive character sets, i.e., [...]. If you know very well which characters you do NOT like to match/find, use exclusive character sets, i.e., [^...]. For example, what if we would like to find all non-vowel letters in the words? Instead of coming up with an inclusive character set [...], which includes all possible consonant letters, it would be more efficient if you create an exclusive character set, [^aeiou], which suggests that you need any characters that do not belong to vowels. x &lt;- c(&quot;grey&quot;, &quot;gray&quot;) str_view(string = x, pattern = &quot;[^aeiou]&quot;) [1] │ &lt;g&gt;&lt;r&gt;e&lt;y&gt; [2] │ &lt;g&gt;&lt;r&gt;a&lt;y&gt; 9.3.5 Quantifiers So far, we have mostly focused on patterns that refer to a single-character sequence, such as \\\\w, [^aeiou], [aeiou], or a fixed-size sequence (e.g., .a. for three-character sequences, gr[ea]y for four-character sequences). These patterns match a fixed-size sequence, as long as each character in the sequence satisfies the conditions defined by the regular expression. However, in many cases, we do not know the exact length of the match beforehand. The length of the match can vary, depending on the input data. For example, if we want to find words that start with the letter “a”, we need to create a regular expression that can match strings of variable lengths, (e.g., words of variable lengths with a word-initial “a”). To specify the number of occurrences of a particular unit in the regular expression (i.e., the character preceding the quantifier), we can use quantifiers. Quantifiers allow us to specify how many times a particular unit (i.e., the character preceding the quantifier) should occur in the input string. Some examples of quantifiers include: ?: 0 or 1 occurrence of the preceding character +: 1 or more occurrences of the preceding character *: 0 or more occurrences of the preceding character x &lt;- &quot;Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;CC?&quot;) [1] │ Roman numerals: MD&lt;CC&gt;&lt;C&gt;LXXXVIII str_view(x, &quot;X+&quot;) [1] │ Roman numerals: MDCCCL&lt;XXX&gt;VIII The meanings of the above regular expressions are as follows: CC?: a sequence of variable size where the first character is “C” and it could be followed by 0 or 1 occurrence of “C”. X+: a sequence of variable size consisting of 1 or more occurrences of “X”. We can specify an exact range of number of occurrences using the curly brackets { , }: {n}: exactly n occurrences of the preceding character {n,}: n or more occurrences of the preceding character {,m}: at most m occurrences of the preceding character {n,m}: between n and m occurrences of the preceding character x &lt;- &quot;Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;C{2}&quot;) [1] │ Roman numerals: MD&lt;CC&gt;CLXXXVIII str_view(x, &quot;C{2,}&quot;) [1] │ Roman numerals: MD&lt;CCC&gt;LXXXVIII str_view(x, &quot;C{2,3}&quot;) [1] │ Roman numerals: MD&lt;CCC&gt;LXXXVIII When we use the quantifiers, be very careful about the scope of the quantifier. By default, the quantifier takes only its preceding character as the scope. If you need to specify the number of occurrences for a group of characters or a specific sub-pattern, you need to put the sub-pattern in a group (...) and then put the quantifier right after the group. x &lt;- &quot;aaabbbababcdf&quot; str_view(x, &quot;ab{2,}&quot;) # the scope of the quantifier is `b` [1] │ aa&lt;abbb&gt;ababcdf str_view(x, &quot;(ab){2,}&quot;) # the scope of the quantifier is `ab` [1] │ aaabbb&lt;abab&gt;cdf 9.3.6 Greedy vs. Non-greedy match The earlier example, as repeated here, shows you that when the RegEx locates the pattern in the string, it prefers to find a longest match that satisfies the pattern. x &lt;- &quot;Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;C{2,}&quot;) [1] │ Roman numerals: MD&lt;CCC&gt;LXXXVIII In the above example, the substring CC should have satisfied the RegEx C{2,} already but the RegEx returns CCC as the first match. This is the idea of greedy match. In other words, by default, when we apply quantifiers in our regular expressions, the RegEx engine assumes a greedy match (i.e., to find a longest possible match). To cancel this default greedy match, we can add ? after the quantifiers. It applies to all quantifiers we’ve looked at (e.g., ?, +, *). Before running the following code chunk, please predict their respective outputs. x &lt;- &quot;Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;CL?&quot;) # find longest match str_view(x, &quot;CL??&quot;) # find shortest match str_view(x, &quot;CLX+&quot;) # find longest match str_view(x, &quot;CLX+?&quot;) # find shortest match str_view(x, &quot;CLX*&quot;) # find longest match str_view(x, &quot;CLX*?&quot;) # find shortest match Please make sure not to confuse the two ? metacharacters. When a quantifier ? is coupled with another ?, the first ? specifies 0 or 1 occurrence of the preceding character whereas the second ? indicates a preference for the shortest possible match. 9.3.7 Group and Back-reference # `fruit` is a preloaded vector from `stringr` x &lt;- fruit %&gt;% head(10) x [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; Let’s take a look at a more complex example now. Suppose we want to extract English fruit names that have a letter repeated twice in a row (e.g., “apple”, “bell pepper”). The main challenge is that we don’t know which letters will be repeated in the word. In our previous examples, a quantifier requires a specific character (or a character group) as its anchor scope. So how can we construct our regular expression? Can we use the metacharacter . along with a quantifier {2} to match any possible alphanumeric character that repeats twice, as shown below? str_view(x, &quot;.{2}&quot;) [1] │ &lt;ap&gt;&lt;pl&gt;e [2] │ &lt;ap&gt;&lt;ri&gt;&lt;co&gt;t [3] │ &lt;av&gt;&lt;oc&gt;&lt;ad&gt;o [4] │ &lt;ba&gt;&lt;na&gt;&lt;na&gt; [5] │ &lt;be&gt;&lt;ll&gt;&lt; p&gt;&lt;ep&gt;&lt;pe&gt;r [6] │ &lt;bi&gt;&lt;lb&gt;&lt;er&gt;&lt;ry&gt; [7] │ &lt;bl&gt;&lt;ac&gt;&lt;kb&gt;&lt;er&gt;&lt;ry&gt; [8] │ &lt;bl&gt;&lt;ac&gt;&lt;kc&gt;&lt;ur&gt;&lt;ra&gt;&lt;nt&gt; [9] │ &lt;bl&gt;&lt;oo&gt;&lt;d &gt;&lt;or&gt;&lt;an&gt;&lt;ge&gt; [10] │ &lt;bl&gt;&lt;ue&gt;&lt;be&gt;&lt;rr&gt;y The results are not what we have expected. The above regular expression only gives us all two-character sequences from each word. The quantifier {2} in the regular expression only indicates the number of occurrences of its preceding character (i.e., .), but it says nothing about the requirement that the two characters have to be exactly the same. In other words, the regular expression \".{2}\" matches “any two-character sequences”. To solve the problem of matching English fruit words with a repeated character, we need to introduce the concept of back-reference. The issue with using . to match any character is that the quantifier alone cannot distinguish between characters that repeat in the word. To address this, we need to ask the RegEx engine to remember the previously matched character and quantify the number of occurrences of the remembered character. To achieve this, we can use a parenthesis to create a group that captures the previously matched character, and then use a back-reference to refer to that group and ensure that the same character is repeated. For example: str_view(x, &quot;(.)\\\\1&quot;) ## two-character sequence [1] │ a&lt;pp&gt;le [5] │ be&lt;ll&gt; pe&lt;pp&gt;er [6] │ bilbe&lt;rr&gt;y [7] │ blackbe&lt;rr&gt;y [8] │ blackcu&lt;rr&gt;ant [9] │ bl&lt;oo&gt;d orange [10] │ bluebe&lt;rr&gt;y In this regular expression: .: matches any character. (.): the parenthesis creates a group to capture the matched character. Internally, the RegEx engine numbers all groups serially from left to right. \\\\1: This is a back-reference to the first group. The same logic applies to the second group of the regular expression (i.e., \\\\2). This back-reference ensures that the second character of the match has to be the same as the previous match (i.e., the same character is repeated). Therefore, (.)\\\\1 means that when the RegEx engine matches a character (i.e., . = any character), there must be another occurrence of the same character following it. Exercise 9.1 With the same set of fruit names in x, how do we match fruits with a abab pattern, such as “banana”? [4] │ b&lt;anan&gt;a Exercise 9.2 With the same set of fruit names in x, how do we match fruits with a abba pattern, such as “pepper”? [5] │ bell p&lt;eppe&gt;r Exercise 9.3 With the same set of fruit names in x, please find fruit names which has at least one letter that is the same as their initial letters [3] │ &lt;avoca&gt;do [6] │ &lt;bilb&gt;erry [7] │ &lt;blackb&gt;erry [10] │ &lt;blueb&gt;erry 9.4 Pattern Matching This section will show you examples of how we can make use of regular expressions to process strings. In stringr, there are a list of verbs that we can use with regular expressions: In the following example, we use the upper-casing STRING to refer to the character vector to which the PATTERN (i.e., regular expression) is matched. STRING: a vector of text sequences (e.g., x with 10 English fruit names) PATTERN: a regular expression for a match ## `STRING` ## use ten fruit names as an example x &lt;- fruit %&gt;% head(10) x [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; 9.4.1 str_detect() str_detect(STRING, PATTERN): The function can be used to find out which strings in STRING have a match with the given PATTERN. It returns a binary output, indicating whether each element of STRING has a match or not. str_detect(x, &quot;e$&quot;) [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE In the above example, the regular expression e$ is used to match the character “e” in the text-final position. As a result, two strings from x that have a match include: x[str_detect(x, &quot;e$&quot;)] [1] &quot;apple&quot; &quot;blood orange&quot; Also, with the output of str_detect(), we can easily compute the total number of strings that have (at least) one match of the pattern: sum(str_detect(x, &quot;e$&quot;)) [1] 2 9.4.2 str_subset() str_subset(STRING, PATTERN): The function can be used to subset the STRING by identifying elements that have either a full or partial match of the PATTERN (character). It returns a subset of the original STRING. str_subset(x, &quot;e$&quot;) [1] &quot;apple&quot; &quot;blood orange&quot; Please note that str_subset() in the above example is the same as the following verbose version using str_detect(): x[str_detect(x, &quot;e$&quot;)] ## verbose version [1] &quot;apple&quot; &quot;blood orange&quot; 9.4.3 str_extract() str_extract(STRING, PATTERN): The function can be used to extract the content of the first match of the PATTERN in each string of STRING. It returns a character vector including the first match of each string in STRING. str_extract(x, &quot;[aeiou]&quot;) [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;e&quot; &quot;i&quot; &quot;a&quot; &quot;a&quot; &quot;o&quot; &quot;u&quot; In the above example, the regular expression [aeiou] is used to match a one-character sequence where the character should be one of the vowel characters (i.e., [aeiou]). The function str_extract() extracts the first occurrence of the match for the specified pattern in each string in STRING. (If there is no match in the string, NA is returned.) Please compare str_extract() with str_subset() and ensure that you understand their differences. str_subset(x, &quot;[aeiou]&quot;) [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; A string in STRING may have more than one match. It is therefore important to note that str_extract() only extracts the first match of the string. To extract all matches from the strings, use str_extract_all(): ## find all vowel characters from the strings str_extract(x, &quot;[aeiou]&quot;) ## find only the first match of each string [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;e&quot; &quot;i&quot; &quot;a&quot; &quot;a&quot; &quot;o&quot; &quot;u&quot; str_extract_all(x, &quot;[aeiou]&quot;) ## find all the matches of each string [[1]] [1] &quot;a&quot; &quot;e&quot; [[2]] [1] &quot;a&quot; &quot;i&quot; &quot;o&quot; [[3]] [1] &quot;a&quot; &quot;o&quot; &quot;a&quot; &quot;o&quot; [[4]] [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; [[5]] [1] &quot;e&quot; &quot;e&quot; &quot;e&quot; [[6]] [1] &quot;i&quot; &quot;e&quot; [[7]] [1] &quot;a&quot; &quot;e&quot; [[8]] [1] &quot;a&quot; &quot;u&quot; &quot;a&quot; [[9]] [1] &quot;o&quot; &quot;o&quot; &quot;o&quot; &quot;a&quot; &quot;e&quot; [[10]] [1] &quot;u&quot; &quot;e&quot; &quot;e&quot; In regular expression matching, a string is a sequence of characters that you want to search for a specific pattern (i.e., the regular expression). The pattern is a sequence of characters that define a search pattern. A match is a sequence of characters in the string that satisfies the specified pattern. Depending on how you structure your regular expression, the match can be either the entire string or a sub-part of it. When using these functions, it’s important to note whether the returned output is a partial sub-string or the entire string, as this can have important implications in computational text analytics. ## find one vowel letter at the string-initial position str_extract(x, &quot;^[aeoiu]&quot;) [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; NA NA NA NA NA NA NA ## find strings with string-initial vowel letter str_extract(x, &quot;^[aeiou].+&quot;) [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; NA NA NA NA [8] NA NA NA In the above example, the regular expression “^[aeiou]” matches only one character that appears at the beginning of a string and is either an “a”, “e”, “o”, “i”, or “u”. The “^” symbol specifies that the character must appear at the start of the string. The match is always a one-character sequence. In contrast, the regular expression “^[aeiou].+” matches any sequence of characters (of variable lengths) that begins with one of the vowels “a”, “e”, “o”, “i”, or “u” and is followed by one or more characters of any type (represented by the “.”), regardless of what they are. The “+” symbol specifies that there must be at least one additional character present in the string. The match can be of variable lengths. 9.4.4 str_match() str_match(STRING, PATTERN): Like str_extract(), this function can be used to extract the content of the first match of the PATTERN in each string of STRING. However, it has an additional functionality: it returns not only the matches but also the capture group. The capture group refers to the characters in the regular expression that are enclosed in parentheses. str_match(x, &quot;(bl)([aeiou]+)&quot;) [,1] [,2] [,3] [1,] NA NA NA [2,] NA NA NA [3,] NA NA NA [4,] NA NA NA [5,] NA NA NA [6,] NA NA NA [7,] &quot;bla&quot; &quot;bl&quot; &quot;a&quot; [8,] &quot;bla&quot; &quot;bl&quot; &quot;a&quot; [9,] &quot;bloo&quot; &quot;bl&quot; &quot;oo&quot; [10,] &quot;blue&quot; &quot;bl&quot; &quot;ue&quot; In the above example, the regular expression \"(bl)([aeiou]+)\" has two parts, or two capture groups, separated by parentheses: “(bl)”: This capture group matches the exact sequence of characters “bl” in the string. “([aeiou]+)”: This capture group matches one or more consecutive occurrences of any vowel letter (a, e, i, o, u). So, when this regular expression is used with a function like str_match() in R, it will search for all matches of the pattern in the input string and return both the matches and the capture groups as a two-dimensional character vector. We can easily get the vowel sequences from each match: m &lt;-str_match(x, &quot;(bl)([aeiou]+)&quot;) m[,3] ## col1: match; col2: 1st group; col3: 2nd group [1] NA NA NA NA NA NA &quot;a&quot; &quot;a&quot; &quot;oo&quot; &quot;ue&quot; We can compare the results from str_extract(): str_extract(x, &quot;(bl)([aeiou]+)&quot;) [1] NA NA NA NA NA NA &quot;bla&quot; &quot;bla&quot; &quot;bloo&quot; &quot;blue&quot; Similarly, if you need information for all the matches from the string (not merely the first match), you need str_match_all(): str_match_all(x, &quot;([pb])([aeiou]+)&quot;) [[1]] [,1] [,2] [,3] [[2]] [,1] [,2] [,3] [[3]] [,1] [,2] [,3] [[4]] [,1] [,2] [,3] [1,] &quot;ba&quot; &quot;b&quot; &quot;a&quot; [[5]] [,1] [,2] [,3] [1,] &quot;be&quot; &quot;b&quot; &quot;e&quot; [2,] &quot;pe&quot; &quot;p&quot; &quot;e&quot; [3,] &quot;pe&quot; &quot;p&quot; &quot;e&quot; [[6]] [,1] [,2] [,3] [1,] &quot;bi&quot; &quot;b&quot; &quot;i&quot; [2,] &quot;be&quot; &quot;b&quot; &quot;e&quot; [[7]] [,1] [,2] [,3] [1,] &quot;be&quot; &quot;b&quot; &quot;e&quot; [[8]] [,1] [,2] [,3] [[9]] [,1] [,2] [,3] [[10]] [,1] [,2] [,3] [1,] &quot;be&quot; &quot;b&quot; &quot;e&quot; The regular expression “([pb])([aeiou]+)” consists of two capture groups enclosed in parentheses. The first capture group “([pb])” matches either the letter “p” or “b”. The second capture group “([aeiou]+)” matches one or more vowels (i.e., “a”, “e”, “i”, “o”, “u”). Therefore, this regular expression can be used to match any sequence that starts with either “p” or “b”, followed by one or more vowels. Please note that “bell pepper” has three matches and “bilberry” has two matches. Exercise 9.4 How do you use str_match() to find out all the \"c\"+Vowel structures, and at the same time identify which vowels follow the letter c? [2] │ apri&lt;co&gt;t [3] │ avo&lt;ca&gt;do [8] │ black&lt;cu&gt;rrant [,1] [,2] [1,] NA NA [2,] &quot;co&quot; &quot;o&quot; [3,] &quot;ca&quot; &quot;a&quot; [4,] NA NA [5,] NA NA [6,] NA NA [7,] NA NA [8,] &quot;cu&quot; &quot;u&quot; [9,] NA NA [10,] NA NA Exercise 9.5 Please use str_match() to find out all fruit names whose initial letter is a consonant letter (i.e., not any of a, e, i, o, u) and gets repeated in the remaining part of the word. The following is a sample output from str_match(): Column 1 refers to the match; Column 2 refers to the initial letter; Column 3 refers to the letters between the initial letter and the repeated same letter; Column 4 refers to the repeated letter, which is the same as the initial letter. [,1] [,2] [,3] [,4] [1,] NA NA NA NA [2,] NA NA NA NA [3,] NA NA NA NA [4,] NA NA NA NA [5,] NA NA NA NA [6,] &quot;bilb&quot; &quot;b&quot; &quot;il&quot; &quot;b&quot; [7,] &quot;blackb&quot; &quot;b&quot; &quot;lack&quot; &quot;b&quot; [8,] NA NA NA NA [9,] NA NA NA NA [10,] &quot;blueb&quot; &quot;b&quot; &quot;lue&quot; &quot;b&quot; 9.4.5 str_replace() str_replace(STRING, PATTERN, REPLACEMENT): The function can be used to replace the first match of the PATTERN in each string with REPLACEMENT. str_replace(string = x, pattern = &quot;[aeiou]&quot;, replacement = &quot;V&quot;) [1] &quot;Vpple&quot; &quot;Vpricot&quot; &quot;Vvocado&quot; &quot;bVnana&quot; &quot;bVll pepper&quot; [6] &quot;bVlberry&quot; &quot;blVckberry&quot; &quot;blVckcurrant&quot; &quot;blVod orange&quot; &quot;blVeberry&quot; Similarly, if you want to replace all the matches in the string, use str_replace_all(). str_replace_all(string = x, pattern = &quot;[aeiou]&quot;, replacement = &quot;V&quot;) [1] &quot;VpplV&quot; &quot;VprVcVt&quot; &quot;VvVcVdV&quot; &quot;bVnVnV&quot; &quot;bVll pVppVr&quot; [6] &quot;bVlbVrry&quot; &quot;blVckbVrry&quot; &quot;blVckcVrrVnt&quot; &quot;blVVd VrVngV&quot; &quot;blVVbVrry&quot; 9.4.6 str_split() str_split(STRING, PATTERN): The function can be use to split each string in STRING based on the PATTERN (i.e., using the PATTERN as the delimiter for tokenization). print(x) [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; str_split(x, &quot;[^aeiou]+&quot;) [[1]] [1] &quot;a&quot; &quot;e&quot; [[2]] [1] &quot;a&quot; &quot;i&quot; &quot;o&quot; &quot;&quot; [[3]] [1] &quot;a&quot; &quot;o&quot; &quot;a&quot; &quot;o&quot; [[4]] [1] &quot;&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; [[5]] [1] &quot;&quot; &quot;e&quot; &quot;e&quot; &quot;e&quot; &quot;&quot; [[6]] [1] &quot;&quot; &quot;i&quot; &quot;e&quot; &quot;&quot; [[7]] [1] &quot;&quot; &quot;a&quot; &quot;e&quot; &quot;&quot; [[8]] [1] &quot;&quot; &quot;a&quot; &quot;u&quot; &quot;a&quot; &quot;&quot; [[9]] [1] &quot;&quot; &quot;oo&quot; &quot;o&quot; &quot;a&quot; &quot;e&quot; [[10]] [1] &quot;&quot; &quot;ue&quot; &quot;e&quot; &quot;&quot; The regular expression “[^aeiou]+” matches any sequence of one or more characters that are not vowels. The str_split() function splits a string into pieces based on a specified pattern. In this case, the function will split each fruit name into multiple pieces (tokens) wherever a sequence of non-vowel characters is found. The resulting output is a list of character vectors, where each vector contains the pieces of the original string that were separated by the non-vowel characters. Please note that in the above output produced by str_split() there are empty strings. Can you figure out why? This function is very useful for tokenization in text analytics, i.e., breaking a long string into smaller pieces. ## five sentences y &lt;- sentences %&gt;% head(5) y [1] &quot;The birch canoe slid on the smooth planks.&quot; [2] &quot;Glue the sheet to the dark blue background.&quot; [3] &quot;It&#39;s easy to tell the depth of a well.&quot; [4] &quot;These days a chicken leg is a rare dish.&quot; [5] &quot;Rice is often served in round bowls.&quot; ## Break each sent into tokens ## based on any sequences of non-word characters str_split(string = y, pattern = &quot;[^\\\\w]+&quot;) [[1]] [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; &quot;planks&quot; [9] &quot;&quot; [[2]] [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; [6] &quot;dark&quot; &quot;blue&quot; &quot;background&quot; &quot;&quot; [[3]] [1] &quot;It&quot; &quot;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; &quot;a&quot; [10] &quot;well&quot; &quot;&quot; [[4]] [1] &quot;These&quot; &quot;days&quot; &quot;a&quot; &quot;chicken&quot; &quot;leg&quot; &quot;is&quot; &quot;a&quot; [8] &quot;rare&quot; &quot;dish&quot; &quot;&quot; [[5]] [1] &quot;Rice&quot; &quot;is&quot; &quot;often&quot; &quot;served&quot; &quot;in&quot; &quot;round&quot; &quot;bowls&quot; &quot;&quot; Please note that the return of str_split() is a list. If you prefer a data frame as a return, you can use str_split(..., simplify = TRUE): fields &lt;- c(&quot;Name: Hadley&quot;, &quot;Country: NZ&quot;, &quot;Age: 35&quot;) ## Default list as return fields %&gt;% str_split(&quot;[^\\\\w]+&quot;) [[1]] [1] &quot;Name&quot; &quot;Hadley&quot; [[2]] [1] &quot;Country&quot; &quot;NZ&quot; [[3]] [1] &quot;Age&quot; &quot;35&quot; ## Data frame as return fields %&gt;% str_split(&quot;[^\\\\w]+&quot;, simplify = TRUE) [,1] [,2] [1,] &quot;Name&quot; &quot;Hadley&quot; [2,] &quot;Country&quot; &quot;NZ&quot; [3,] &quot;Age&quot; &quot;35&quot; Exercise 9.6 Convert American dates American.dates to British dates using str_replace_all(). Please note that in your output, you need to preserve the original delimiters for each date. American.dates &lt;- c(&quot;7/31/1976&quot;, &quot;02.15.1970&quot;, &quot;11-31-1986&quot;, &quot;04/01.2020&quot;) [1] &quot;31/7/1976&quot; &quot;15.02.1970&quot; &quot;31-11-1986&quot; &quot;01/04.2020&quot; Exercise 9.7 Please use the default sentences vector as your input and find all patterns of “any BE verbs + words ending with ‘en’ or ‘ed’”. Please extract these matches from the sentences and your result should be a vector of these matches, as shown below. sentences[1:5] [1] &quot;The birch canoe slid on the smooth planks.&quot; [2] &quot;Glue the sheet to the dark blue background.&quot; [3] &quot;It&#39;s easy to tell the depth of a well.&quot; [4] &quot;These days a chicken leg is a rare dish.&quot; [5] &quot;Rice is often served in round bowls.&quot; length(sentences) [1] 720 [1] &quot;is often&quot; &quot;were fed&quot; &quot;is used&quot; &quot;was cooked&quot; [5] &quot;was seized&quot; &quot;is used&quot; &quot;be seen&quot; &quot;was spattered&quot; [9] &quot;been used&quot; &quot;is red&quot; &quot;was fired&quot; &quot;is ten&quot; [13] &quot;be used&quot; &quot;is used&quot; &quot;are pushed&quot; &quot;are men&quot; [17] &quot;are used&quot; &quot;were hired&quot; &quot;was covered&quot; &quot;be turned&quot; [21] &quot;were lined&quot; &quot;was ten&quot; &quot;is used&quot; &quot;are paved&quot; [25] &quot;is carved&quot; &quot;were led&quot; &quot;is needed&quot; &quot;be needed&quot; [29] &quot;were painted&quot; &quot;were mailed&quot; &quot;was pressed&quot; &quot;is seen&quot; [33] &quot;was packed&quot; &quot;be shipped&quot; &quot;was barred&quot; &quot;was crowded&quot; [37] &quot;was carved&quot; &quot;was drilled&quot; &quot;was hidden&quot; &quot;was seen&quot; [41] &quot;were pierced&quot; &quot;are jangled&quot; &quot;is tinged&quot; &quot;be tilted&quot; [45] &quot;be shortened&quot; &quot;were stamped&quot; &quot;was jammed&quot; &quot;was robbed&quot; [1] │ Rice &lt;is often&gt; served in round bowls. [2] │ The hogs &lt;were fed&gt; chopped corn and garbage. [3] │ A rod &lt;is used&gt; to catch pink salmon. [4] │ The meal &lt;was cooked&gt; before the bell rang. [5] │ The walled town &lt;was seized&gt; without a fight. [6] │ A Tusk &lt;is used&gt; to make costly gifts. [7] │ The wharf could &lt;be seen&gt; at the farther shore. [8] │ Mud &lt;was spattered&gt; on the front of his white shirt. [9] │ The pencils have all &lt;been used&gt;. [10] │ The sofa cushion &lt;is red&gt; and of light weight. [11] │ The new girl &lt;was fired&gt; today at noon. [12] │ There the flood mark &lt;is ten&gt; inches. [13] │ Corn cobs can &lt;be used&gt; to kindle a fire. [14] │ The lure &lt;is used&gt; to catch trout and flounder. [15] │ They &lt;are pushed&gt; back each time they attack. [16] │ They &lt;are men&gt; who walk the middle of the road. [17] │ Fruit flavors &lt;are used&gt; in fizz drinks. [18] │ Nine men &lt;were hired&gt; to dig the ruins. [19] │ The old pan &lt;was covered&gt; with hard fudge. [20] │ The last switch cannot &lt;be turned&gt; off. ... and 27 more Note: Because now we do not have the information of part-of-speech tags, please ignore the false positives (e.g., “Rice &lt;is often&gt; served in round bowls”, “They &lt;are men&gt; who walk the middle of the road”) for the moment. But all your matches should still be legitimate words in English. If your regular expressions inaccurately identify the following tokens as matches, you must fix your regular expression to remove these inaccurate tokens: Exercise 9.8 Please create a regular expression to extract all word tokens of the sentences from the vector x, which is defined as follows. x &lt;- c(&quot;It&#39;s a three-legged chair.&quot;, &quot;The book (you read) was quite boring!&quot;) The returned object is a list, including the word vectors of each text in x. [[1]] [1] &quot;It&#39;s&quot; &quot;a&quot; &quot;three-legged&quot; &quot;char&quot; [[2]] [1] &quot;The&quot; &quot;book&quot; &quot;you&quot; &quot;read&quot; &quot;was&quot; &quot;quite&quot; &quot;boring&quot; Exercise 9.9 In stringr::sentences, there are 720 English sentences. Please create a regular expression to subset sentences with at least one word containing a hyphen - or an apostrophe '. There are 19 sentences with at least one match. Your regular expression should match the entire word string with the symbol. [1] │ The birch canoe slid on the smooth planks. [2] │ Glue the sheet to the dark blue background. [3] │ &lt;It&#39;s&gt; easy to tell the depth of a well. [4] │ These days a chicken leg is a rare dish. [5] │ Rice is often served in round bowls. [6] │ The juice of lemons makes fine punch. [7] │ The box was thrown beside the parked truck. [8] │ The hogs were fed chopped corn and garbage. [9] │ Four hours of steady work faced us. [10] │ A large size in stockings is hard to sell. [11] │ The boy was there when the sun rose. [12] │ A rod is used to catch pink salmon. [13] │ The source of the huge river is the clear spring. [14] │ Kick the ball straight and follow through. [15] │ Help the woman get back to her feet. [16] │ A pot of tea helps to pass the evening. [17] │ Smoky fires lack flame and heat. [18] │ The soft cushion broke the &lt;man&#39;s&gt; fall. [19] │ The salt breeze came across from the sea. [20] │ The girl at the booth sold fifty bonds. ... and 700 more [1] &quot;It&#39;s&quot; &quot;man&#39;s&quot; &quot;don&#39;t&quot; &quot;store&#39;s&quot; &quot;hot-cross&quot; [6] &quot;workman&#39;s&quot; &quot;Let&#39;s&quot; &quot;sun&#39;s&quot; &quot;child&#39;s&quot; &quot;king&#39;s&quot; [11] &quot;It&#39;s&quot; &quot;don&#39;t&quot; &quot;queen&#39;s&quot; &quot;don&#39;t&quot; &quot;don&#39;t&quot; [16] &quot;don&#39;t&quot; &quot;don&#39;t&quot; &quot;pirate&#39;s&quot; &quot;neighbor&#39;s&quot; Exercise 9.10 Please extract all the word tokens that are tagged as NOUNS from the following text using a self-defined regular expressions. NOUNS are defined as words with parts-of-speech tags starting with N. x &lt;- &quot;中央(Nc) 流行(VH) 疫情(Na) 指揮(VC) 中心(Nc) 醫療(VC) 應變組(Nc) 副組長(Na) 羅一鈞(Nb) 今天(Nd) 說明(VE) ，(COMMACATEGORY) 截至(P) 12月(Nd) 1日(Nd) 全球(Nc) 累計(VJ) 至少(Da) 27(Neu) 國(Nc) 、(PAUSECATEGORY) 共有(VJ) 370(Neu) 例(Na) 確診(VA) 感染(VJ) Omicron(FW) 變異株(Na) ，(COMMACATEGORY) 多(D) 來自(VJ) 南非(Nc) 或(Caa) 具(VJ) 非洲(Nc) 國家(Na) 旅遊史(Na) 。(PERIODCATEGORY)&quot; [[1]] [1] &quot;中央(Nc)&quot; &quot;疫情(Na)&quot; &quot;中心(Nc)&quot; &quot;應變組(Nc)&quot; &quot;副組長(Na)&quot; [6] &quot;羅一鈞(Nb)&quot; &quot;今天(Nd)&quot; &quot;12月(Nd)&quot; &quot;1日(Nd)&quot; &quot;全球(Nc)&quot; [11] &quot;27(Neu)&quot; &quot;國(Nc)&quot; &quot;370(Neu)&quot; &quot;例(Na)&quot; &quot;變異株(Na)&quot; [16] &quot;南非(Nc)&quot; &quot;非洲(Nc)&quot; &quot;國家(Na)&quot; &quot;旅遊史(Na)&quot; Exercise 9.11 Please use the same Chinese texts as the input. Please use str_replace_all() along with a self-defined regular expression to remove all part-of-speech tags in the original texts. x &lt;- &quot;中央(Nc) 流行(VH) 疫情(Na) 指揮(VC) 中心(Nc) 醫療(VC) 應變組(Nc) 副組長(Na) 羅一鈞(Nb) 今天(Nd) 說明(VE) ，(COMMACATEGORY) 截至(P) 12月(Nd) 1日(Nd) 全球(Nc) 累計(VJ) 至少(Da) 27(Neu) 國(Nc) 、(PAUSECATEGORY) 共有(VJ) 370(Neu) 例(Na) 確診(VA) 感染(VJ) Omicron(FW) 變異株(Na) ，(COMMACATEGORY) 多(D) 來自(VJ) 南非(Nc) 或(Caa) 具(VJ) 非洲(Nc) 國家(Na) 旅遊史(Na) 。(PERIODCATEGORY)&quot; [1] &quot;中央 流行 疫情 指揮 中心 醫療 應變組 副組長 羅一鈞 今天 說明 ， 截至 12月 1日 全球 累計 至少 27 國 、 共有 370 例 確診 感染 Omicron 變異株 ， 多 來自 南非 或 具 非洲 國家 旅遊史 。&quot; 9.5 Advanced Pattern Matching Lookahead Assertion The regular expressions we have covered so far are designed to match patterns within an input string, and as a result, they will consume the input string during the pattern matching process. In regular expression matching, consuming characters refers to the process where the regular expression engine reads and matches characters in the input string. When a regular expression is applied to a string, the pattern matching engine moves through the string character by character, trying to find a match for the pattern. As the matching progresses, the engine consumes the characters in the input string that it has already matched. It is important to note that once the engine has consumed a character in the input string, it cannot be reused for a subsequent match. This means that the same character cannot be part of two overlapping matches. To help explain this concept, let’s look at a simple example. If we want to find the word Windows in a string only when it is followed by \"95, 98, NT, 2000\", we may use the regular expression “Windows(95|98|NT|2000)”: win &lt;- c(&quot;Windows2000&quot;, &quot;Windows&quot;, &quot;WindowsNT&quot;, &quot;Windows7&quot;, &quot;Windows10&quot;) str_view(win, &quot;Windows(95|98|NT|2000)&quot;) [1] │ &lt;Windows2000&gt; [3] │ &lt;WindowsNT&gt; The regular expression in the above example matches “Windows” followed by any of the specified version numbers. However, it will also consume the version number, resulting in matches that include irrelevant characters such as “Windows2000” or “WindowsNT”. This can be a problem when we need to replace the matched pattern with a different string. That is, it can be problematic when we need to replace the word “Windows” with “OldSystem” but only when “Windows” is followed by \"95, 98, NT, 2000\": str_replace(win, pattern = &quot;Windows(95|98|NT|2000)&quot;, replacement = &quot;OldSystem&quot;) [1] &quot;OldSystem&quot; &quot;Windows&quot; &quot;OldSystem&quot; &quot;Windows7&quot; &quot;Windows10&quot; Now you see the problem? Not only “Windows” was replaced, but also the entire string. This is not what we have expected to get. Instead, we would expect something like: [1] &quot;OldSystem2000&quot; &quot;Windows&quot; &quot;OldSystemNT&quot; &quot;Windows7&quot; [5] &quot;Windows10&quot; To solve this problem, we can use the lookahead asserttion in regular expression. PATTERN(?=...): This positive lookahead assertaion is used to match a PATTERN only when it is followed by the specified lookahead pattern (?=...). PATTERN(?!...): This negative lookahead assertion is used to match a PATTERN only when it is NOT followed by the specified lookahead pattern (?!...). The key is that the lookahead assertation does not consume characters in the match processing. str_view(win, &quot;Windows(?=95|98|NT|2000)&quot;) [1] │ &lt;Windows&gt;2000 [3] │ &lt;Windows&gt;NT In the above regular expression, the (?=95|98|NT|2000) is a positive lookahead assertion. The entire regular expression is used to match the string Windows only when it is followed by the specified lookhead pattern (i.e., any of the following strings, “95”, “98”, “NT”, “2000”), but it does not consume the characters in the lookahead assertion. This way, when we replace the matched pattern with a different string, we only replace the “Windows” part and leave the version number unchanged. str_replace(string = win, pattern = &quot;Windows(?=95|98|NT|2000)&quot;, replacement = &quot;OldSystem&quot;) [1] &quot;OldSystem2000&quot; &quot;Windows&quot; &quot;OldSystemNT&quot; &quot;Windows7&quot; [5] &quot;Windows10&quot; Similarly, we can use a negative lookahead assertion as well: str_view(win, &quot;Windows(?!7|10)&quot;) [1] │ &lt;Windows&gt;2000 [2] │ &lt;Windows&gt; [3] │ &lt;Windows&gt;NT str_replace(string = win, pattern = &quot;Windows(?!7|10)&quot;, replacement = &quot;NewSystem&quot;) [1] &quot;NewSystem2000&quot; &quot;NewSystem&quot; &quot;NewSystemNT&quot; &quot;Windows7&quot; [5] &quot;Windows10&quot; Look-behind Assertion In addition to lookahead assertion, we can also specify look-behind assertions: (?&lt;=...)PATTERN: This positive look-behind assertion can be used to match a PATTERN only when it is preceded by the specified look-behind pattern (?&lt;=...). (?&lt;!...)PATTERN: This negative look-behind assertion can be used to match a PATTERN only when it is NOT preceded by the specified look-behind pattern (?&lt;=...). Let’s look at a simple example: dollars &lt;- c(&quot;NT$200&quot;, &quot;US$200&quot;, &quot;EURO$200&quot;, &quot;POUND$200&quot;, &quot;$200&quot;) str_replace_all(dollars, &quot;(?&lt;=NT|US)\\\\$200&quot;, &quot;****&quot;) [1] &quot;NT****&quot; &quot;US****&quot; &quot;EURO$200&quot; &quot;POUND$200&quot; &quot;$200&quot; In the above example, we replace the dollar numbers “$200” with “****” only when they are preceded by “NT” or “US”. Below is an example of negative look-behind assertion. dollars &lt;- c(&quot;NT$200&quot;, &quot;US$200&quot;, &quot;EURO$200&quot;, &quot;POUND$200&quot;, &quot;$200&quot;) str_replace_all(dollars, &quot;(?&lt;!NT|US)\\\\$200&quot;, &quot;****&quot;) [1] &quot;NT$200&quot; &quot;US$200&quot; &quot;EURO****&quot; &quot;POUND****&quot; &quot;****&quot; Do you know why in the above regular expressions we have to include “\\” before the dollar sign “$”? Exercise 9.12 Please use the first ten words in the fruit vector for this exercise. Based on the fruit vocabulary, can you identify all the a that is followed by STOP_SOUNDS and replace them with “V”? STOP_SOUNDS refer to the following letters: p, t, k, b, d, g Hint: str_replace_all() dataset fruit[1:10] [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; target matches [1] │ &lt;a&gt;pple [2] │ &lt;a&gt;pricot [3] │ avoc&lt;a&gt;do [4] │ banana [5] │ bell pepper [6] │ bilberry [7] │ blackberry [8] │ blackcurrant [9] │ blood orange [10] │ blueberry your result [1] &quot;Vpple&quot; &quot;Vpricot&quot; &quot;avocVdo&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; Exercise 9.13 Similar to the previous example, Exercise 9.12, also based on the first ten words in fruit, please identify all the vowels that are both followed and preceded by STOP_SOUNDS and replace them with “V”. Vowels are defined as letters including a, e, i, o,and u, dataset fruit[1:10] [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; target matches [1] │ apple [2] │ apricot [3] │ avocado [4] │ banana [5] │ bell p&lt;e&gt;pper [6] │ bilberry [7] │ blackberry [8] │ blackcurrant [9] │ blood orange [10] │ blueberry your result [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pVpper&quot; [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; 9.6 Recap To summarize, when creating a regular expression, you may consider the following questions: For each character in the match, is it a literal character or does it belong to a limited character class? You can use character sets and classes to match specific characters or ranges of characters. Is the pattern you are matching a fixed size or variable size? You can use quantifiers to indicate how many times a character or group of characters can appear in the match. Are there any repeated characters in your match that you want to capture and refer to later? You can use capture groups and back-references to reference a previously matched group in your expression. Therefore, three important steps: Determine if each character in the match is a literal character/string or falls into a limited character class. This will help you decide if you need to use a character set/class. Determine if the potential match is a fixed size or a variable size. This will help you decide if you need to use quantifiers. Determine if there are any repeated characters in the match. This will help you decide if you need to use capture groups and back-references. 9.7 More Practices In this section, we will explore more examples of the practical applications of regular expressions. Specifically, we will focus on how regular expressions can be used to manipulate datasets, taking advantage of their powerful capabilities and potential for text processing. In Chapter 7, we have talked about important ways in which we can manipulate our data as a data.frame. Please refer to the Chapter 7 for a review of the important verbs in dplyr. In the library tidyr, there are three very useful functions for data manipulation: separate(), extract() and unnest(). Please read the documentations of these three functions and run the examples provided in the documentation to make sure that you understand how they work. separate(): This function separates a single column into multiple columns based on a specified separator (which can be specified by regluar expression). extract(): This function extracts values from a single column based on a specified regular expression pattern and creates new columns with those extracted values. unnest(): This function expands a column that contains a list of values into multiple rows. 9.7.1 Case 1 If we have a data frame like dt below, how do we extract only the numbers of the weeks from the y column, and add this information to a new column, z? Original data frame: df &lt;- data.frame( x = 1:4, y = c(&quot;wk 3&quot;, &quot;week-1&quot;, &quot;7&quot;, &quot;w#9&quot;) ) df Expected data frame: Hints: We need to create a regular expression to extract numbers from the strings in y column. We need to create a new column for the data frame df. Exercise 9.14 If we have a data frame like df below, how do we extract all the vowels of the words in the WORD column and create two additional columns: Because a word may have several vowels, create a new column, which shows all the vowels in the word by combining them into a long string with the delimiter “_” Create another column for the number of vowels for each word Vowels are defined as [aeiou]. df &lt;- data.frame( WORD = fruit[1:10] ) df 9.7.2 Case 2 How to separate the English and Chinese strings in x column and create two new columns, EN, CH? Original data frame: df &lt;- data.frame(x = c(&quot;I我&quot;, &quot;love愛&quot;, &quot;you你&quot;)) df Expected data frame: Hints: Please check tidyr::extract(), which is a function to split a column into several columns using regular expression. We need to create regular expression to extract alphanumeric (or non-alphanumeric) characters from the values of x column. In the regular expression, one capture group matches the values for EN column and the other capture group matches the values for the CH column. Exercise 9.15 How to extract the numbers and split the numbers into START and END in the following data frame? Original data frame: df &lt;- data.frame(x = c(&quot;1-12周&quot;, &quot;1-10周&quot;, &quot;5-12周&quot;)) df Expected data frame: 9.7.3 Case 3 How to extract all the individual digits of each token, compute the sum of the numbers, and save the results in a new column SUM? Original data frame: df &lt;- data.frame( x = c(&quot;1234&quot;, &quot;B246&quot;, &quot;217C&quot;, &quot;2357f&quot;, &quot;21WD4&quot;) ) df Expected data frame: Hints: We need to know how to extract all numbers from the values in x. We need to know how to compute the sum of the numbers extracted from each value in x. We need to add these sums to the new column. You may check the use of unnest(). Exercise 9.16 How to extract all the numbers that follow a upper-casing letter? For example, 34 after W; 217 after B? Original data frame: df &lt;- data.frame( x = c(&quot;12W34&quot;, &quot;AB2C46&quot;, &quot;B217C&quot;, &quot;akTs6df&quot;, &quot;21WD4&quot;) ) df Expected data frame: Exercise 9.17 Based on Exercise 9.16, can you add another column to the resulting data frame, which records the upper-casing letter that the numbers follow for each row? 9.8 Case Study: Chinese Four-Character Idioms Many studies have shown that Chinese speakers make use of large proportion of four-character idioms (四字成語) in discourse. Let’s have an exploratory analysis of four-character idioms in Chinese. 9.8.1 Dictionary Entries In our demo_data directory, there is a file demo_data/dict-ch-idiom.txt, which includes a list of four-character idioms in Chinese. These idioms are collected from 搜狗輸入法詞庫 and the original file formats (.scel) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., .txt. Let’s first load the idioms dataset in R. ## Loading file all_idioms &lt;- readLines(con = &quot;demo_data/dict-ch-idiom.txt&quot;,encoding = &quot;UTF-8&quot;) ## Checking first 6 tokens head(all_idioms) [1] &quot;阿保之功&quot; &quot;阿保之勞&quot; &quot;阿鼻地獄&quot; &quot;阿鼻叫喚&quot; &quot;阿斗太子&quot; &quot;阿芙蓉膏&quot; ## Checking last 6 tokens tail(all_idioms) [1] &quot;罪無可逭&quot; &quot;罪人不帑&quot; &quot;作纛旗兒&quot; &quot;坐纛旂兒&quot; &quot;作姦犯科&quot; &quot;作育英才&quot; ## total number of tokens length(all_idioms) [1] 56536 In order to make use of the tidy structure in R, we convert the vector into a data.frame (i.e., each row refers to one particular idiom type): idiom &lt;- data.frame(IDIOMS = all_idioms) idiom %&gt;% head 9.8.2 Case Study: X來Y去 We can create a regular expression pattern to extract all idioms with the format of X來Ｙ去: idiom %&gt;% filter(str_detect(IDIOMS, &quot;.來.去&quot;)) To analyze the meaning of this constructional schema, we can extract the characters in X and Y slots of the schema: ## Version 1 idiom_laiqu &lt;-idiom %&gt;% filter(str_detect(IDIOMS, &quot;.來.去&quot;)) %&gt;% mutate(PATTERN = str_replace(IDIOMS, &quot;(.)來(.)去&quot;, &quot;\\\\1_\\\\2&quot;)) %&gt;% separate(PATTERN, into = c(&quot;W1&quot;, &quot;W2&quot;), sep = &quot;_&quot;) idiom_laiqu # ## version 2 # require(tidyr) # idiom %&gt;% # filter(str_detect(string, &quot;.來.去&quot;)) %&gt;% # mutate(string2 = string) %&gt;% # extract(col=&quot;string2&quot;, # into=c(&quot;w1&quot;,&quot;w2&quot;), # regex = &quot;(.)來(.)去&quot;) One empirical question is how many of these idioms are of the pattern W1 = W2 (e.g., 想來想去, 直來直去) and how many are of the pattern W1 != W2 (e.g., 說來道去, 朝來暮去): # Create `structure` column idiom_laiqu_2 &lt;- idiom_laiqu %&gt;% mutate(STRUCTURE = ifelse(W1==W2, &quot;XX&quot;,&quot;XY&quot;)) idiom_laiqu_2 # Count `structure` frequecnies idiom_laiqu_count &lt;- idiom_laiqu_2 %&gt;% count(STRUCTURE) idiom_laiqu_count # Create barplots idiom_laiqu_count %&gt;% ggplot(aes(STRUCTURE, n, fill = STRUCTURE)) + geom_col() ########################## ### Another alterantive### ########################## # idiom_laiqu %&gt;% # mutate(STRUCTURE = ifelse(W1==W2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% # count(STRUCTURE) %&gt;% # ggplot(aes(STRUCTURE, n, fill = STRUCTURE)) + # geom_col() Exercise 9.18 Please use same dataset idiom (loaded from demo_data/dict-ch-idiom.txt) and extract all the idioms that fall into the schema of 一X一Y. idiom &lt;- data.frame(string = readLines(&quot;demo_data/dict-ch-idiom.txt&quot;)) Exercise 9.19 Also, with the idiom as our data, if we are interested in all the idioms that have duplicated characters in them, with schemas like either _A_A or A_A_, where A is a fixed character. How can we extract all idioms of these two types from idiom? Please visualize the distribution of the two idiom types using a bar plot. Sample answers have been provided below. Idioms with duplicate characters in them Type Distribution Exercise 9.20 Following Exercise 9.19, for each type of the idioms (i.e., “A_A_” or “_A_A”), please provide their respective proportions of W1 = W2 vs. W1 != W2, where W1 and W2 refer to the words filled in the variable slots (“_”) of the idiomatic templates. The following table is a random sample of each idiom type (5 tokens for each type) (Not sure if you can get the exact sample results with set.seed(123)): "],["data-import.html", "Chapter 10 Data Import 10.1 Overview 10.2 Importing Data 10.3 What is a CSV file? (Self-study) 10.4 Character Encoding (Self-study) 10.5 Native R Functions for I/O 10.6 readr Functions 10.7 Directory Operations", " Chapter 10 Data Import Most of the time, we need to work with our own data. In this chapter, we will learn the fundamental concepts and techniques in data I/O (input/output). In particular, we have two main objectives: Learn how to load our data in R from external files Learn how to save our data in R to external files 10.1 Overview There are two major file types that data analysts often work with: txt and csv files. Following the spirit of tidy structure, we will introduce the package, readr, which is also part of the tidyverse. This package provides several useful functions for R users to load their data efficiently from plain-text files. In particular, we will introduce the most effective functions: read_csv() and write_csv() for the loading of CSV files. 10.2 Importing Data Figure 10.1: Data I/O It’s important to note that readLines() and readline() are different functions in R, so please make sure to use the correct one depending on your intended use. readLines() is a function used for reading lines from a text file and storing them as character strings in R. readline() is a function that prompts the user to enter a line of text in the console and stores it as a character string in R. This function is commonly used for interactive input in R scripts or to get user input for a specific task. 10.3 What is a CSV file? (Self-study) A CSV is a comma-separated values file, which allows data to be saved in a tabular format. CSVs look like a spreadsheet but with a .csv extension. A CSV file has a fairly simple structure. It’s a list of data separated by commas. For example, let’s say you have a few contacts in a contact manager, and you export them as a CSV file. You’d get a file containing texts like this: Name,Email,Phone Number,Address Bob Smith,bob@example.com,123-456-7890,123 Fake Street Mike Jones,mike@example.com,098-765-4321,321 Fake Avenue CSV files can be easily viewed with any of the text editors (e.g., Notepad++, TextEdit), or spreadsheet programs, such as Microsoft Excel or Google Spreadsheets. But you can only have one single sheet in a file and all data are kept as normal texts in the file. 10.3.1 Why are .CSV files used? There are several advantages of using CSV files for data exchange: CSV files are plain-text files, making them easier for the developer to create Since they’re plain text, they’re easier to import into a spreadsheet or another storage database, regardless of the specific software you’re using To better organize large amounts of data 10.3.2 How do I save CSV files? Saving CSV files is relatively easy. You just need to know where to change the file type. With a normal spreadsheet application (e.g., MS Excel), under the “File” section in the “Save As” tab, you can select it and change default file extension to “CSV (Comma delimited) (*.csv)“. Data in tabular files is usually separated, or delimited, by commas, but other characters like tabs (\\\\t) can also be used. When tabs are used, the file extension TSV is often used to indicate the delimiter used to separate columns. However, tabular files can also be named with the .txt extension, since the file extension is primarily for the user’s convenience rather than for the R engine. In general, using a more descriptive file extension can make it easier to manage data files. In data analysis, we may also need to deal with some other types of data files in addition to the plain-texts. For the import/export of other types of data, R also has specific packages designed for them: haven - SPSS, Stata, and SAS files readxl - excel files (.xls and .xlsx) DBI - databases jsonlite - json xml2 - XML httr - Web APIs rvest - HTML (Web Scraping) readtext - large text data (corpus) 10.4 Character Encoding (Self-study) Before we move on to the functions for data input/output, I would like to talk about the topic of character encoding. In computing, all characters in languages are represented by binary code, which uses a combination of 1s and 0s to encode each character. Because each character needs to be mapped to a unique number, the more characters a language has, the more encoding numbers are needed. For example, English text is composed of Latin alphabets, a limited set of essential numbers, and punctuation marks. Therefore, a 7-bit encoding scheme, where each character is represented by 7 binary digits, is enough to cover all possible characters in the English language. This encoding scheme provides a maximum of 128 (27 = 128) possible different characters to encode, with each coding number (from 0 to 127) corresponding to a unique character (See below). One of the most well-known encoding schemes for English texts is the ASCII (American Standard Code for Information Interchange) encoding. It is a 7-bit character encoding scheme that was developed from telegraph code. ## ASCII Characters to Code Numbers (Integers) utf8ToInt(&quot;j&quot;) ## code number in decimal [1] 106 utf8ToInt(&quot;J&quot;) ## code number in decimal [1] 74 ## ASCII Code Numbers in Hexadecimal Format as.hexmode(utf8ToInt(&quot;j&quot;)) ## code number in hex [1] &quot;6a&quot; as.hexmode(utf8ToInt(&quot;J&quot;)) ## code number in hex [1] &quot;4a&quot; 10.4.1 Problems with ASCII Many languages have more characters than those found in the ASCII character set, leading to problems with this encoding. German umlauts (über) Accents in Indo-European Languages (déjà) This leads to a process called asciification or romanization ASCII equivalents were defined for foreign characters that have not been included in the original character set Indo-European languages developed extended versions of ASCII to address this issue 10.4.2 From 7-bit to Single-Byte Encoding To address the limitations of the 7-bit encoding, single-byte (8-bit) encoding was introduced. This allows for the first 128 characters to be reserved for ASCII characters, while the remaining 128 can be used for extended characters. This provides a maximum of 256 (28) possibilities. One example of an 8-bit encoding is the ISO-8859-1 encoding, which is widely used for Western European languages. ## A char not in ASCII utf8ToInt(&quot;ü&quot;) ## code number in decimal mode [1] 252 as.hexmode(utf8ToInt(&quot;ü&quot;)) ## code number in hexadecimal mode [1] &quot;fc&quot; 10.4.3 Problems with Single-Byte Encoding Single-byte (8-bit) encoding, while an improvement over ASCII, still has its own set of problems. Inconsistency There are a large number of overlapping character sets used for encoding characters in different (European) languages. There can be cases where more than one symbol maps to the same code point or number, leading to ambiguity. Compatibility While it is an improvement over ASCII, it still cannot deal with writing systems with large character sets, such as the CJK (Chinese, Japanese, Korean) languages. 10.4.4 From Single-Byte to Multi-Byte Encoding Two-byte character sets To address these limitations, two-byte character sets were introduced. These sets can represent up to 65,536 (216 = 65536) distinct characters, making them suitable for languages with larger character sets. Multiple byte encoding Another approach to encoding characters is multiple byte encoding, which is used for languages such as traditional Chinese (Big-5 encoding) and simplified Chinese (GB encoding). In these encoding schemes, multiple bytes are used to represent a single character, allowing for a larger set of characters to be encoded. 10.4.5 Problems with Multi-byte Encoding Different languages still use different encodings, and some are single-byte, while some are multiple-byte. However, digital texts usually have many writing systems at the same time, including single-byte texts, letters, spaces, punctuations, Arabic numerals, interspersed with 2-byte Chinese characters. Ambiguity: The inconsistency in the byte-encoding number creates a problem as different languages may use different numbers of bytes to encode the same character, making it challenging to convert between them accurately. This inconsistency creates confusion and ambiguity in the way that text is displayed across different languages. Efficiency: Multiple-byte encoding requires a fixed number of bytes for each character, which can result in wasted space when representing characters that require fewer bytes (e.g., ASCII characters). The fixed-length encoding scheme of multiple byte encodings reduces efficiency in terms of storage and transmission. Limited character set: Multiple byte encodings often have a limited character set, which means they cannot represent all characters. This limitation makes it difficult to handle text in different languages that utilize non-ASCII characters. It restricts the ability to support a diverse range of languages and writing systems. 10.4.6 Unicode and UTF-8 The objective of modern character encoding is to eliminate the ambiguity of different character sets by specifying a Universal Character Set that includes over 100,000 distinct coded characters, used in all common writing systems today. One of the most widely used character encoding systems is UTF-8. UTF-8 uses a variable-length encoding system, where each character is coded using one to four bytes. ASCII characters require only one byte in UTF-8 (0-127), ISO-8859 characters require two bytes (128-2,047), CKJ characters require three bytes (2,048-65,535), and rare characters require four bytes (65,536-1,112,064). UTF-8 has many strengths, including the ability to encode text in any language, the ability to encode characters in a variable-length character encoding, and the ability to encode characters with no overlap or confusion between conflicting byte ranges. UTF-8 is now the standard encoding used for web pages, XML, and JSON data transmission. 10.4.7 Suggestions It is always recommended to use the UTF-8 encoding when creating a new dataset. It is important to check the encoding of the dataset to avoid ambiguity in character sets. When loading a dataset, it is crucial to specify the encoding of the file to ensure accurate interpretation of characters. Garbled characters are usually an indicator of wrong encoding setting. One should be cautious of the default encoding used in applications like MS-Word, MS-Excel, Praat, SPSS, etc., while creating, collecting, or editing a dataset. It is essential to note that the default encoding for files in Mac/Linux is UTF-8, while in Windows, it is not. Therefore, Windows users need to be extra careful about the encoding of their files. If you don’t know the encoding of the text file, you can use uchardet::detect_file_enc() to check the encoding of the file: uchardet::detect_file_enc(&quot;demo_data/corp-alice.txt&quot;) [1] &quot;ASCII&quot; uchardet::detect_file_enc(&quot;demo_data/data-chinese-poem-utf8.txt&quot;) [1] &quot;UTF-8&quot; uchardet::detect_file_enc(&quot;demo_data/data-chinese-poem-big5.txt&quot;) [1] &quot;BIG5&quot; 10.4.8 Recap: Character, Code Point, and Hexadecimal Mode Character: A minimal unit of text that has semantic value in the language (cf. morpheme vs. grapheme), such as a, 我, é. Code Point: Any legal numeric value in the character encoding set Hexadecimal: A positional system that represents numbers using a base of 16. In R, there are a few base functions that work with these concepts: Encoding(): Read or set the declared encodings for a character vector iconv(): Convert a character vector between encodings utf8ToInt(): Convert a UTF-8 encoded character to integers (code number in decimals) as.hexamode(): Convert numbers into Hexadecimals ## Characters e1 &lt;- &quot;Q&quot; Encoding(e1) [1] &quot;unknown&quot; ## Convert the encoding to UTF-8/Big-5 e2 &lt;- iconv(e1, to = &quot;UTF-8&quot;) Encoding(e2) ## ASCII chars are never marked with a declared encoding [1] &quot;unknown&quot; e2 [1] &quot;Q&quot; ## Convert char to code number e1_decimal &lt;- utf8ToInt(e1) e1_decimal ## code point of `Q` [1] 81 ## Convert code number into hex mode e1_hex &lt;- as.hexmode(e1_decimal) e1_hex ## code point in hexadecimal mode of `Q` [1] &quot;51&quot; We can represent characters in UTF-8 code point (hexadecimals) by using the escape \"\\u....\": print(&quot;\\u51&quot;) [1] &quot;Q&quot; print(&quot;\\u5987&quot;) [1] &quot;妇&quot; The following is an example of a Chinese character. c1 &lt;- &quot;臺&quot; Encoding(c1) ## Non-ASCII char encoding in R. What&#39;s the output in Windows? [1] &quot;UTF-8&quot; c1_decimal&lt;- utf8ToInt(c1) c1_decimal [1] 33274 c1_hex&lt;-as.hexmode(c1_decimal) c1_hex [1] &quot;81fa&quot; print(&quot;\\u81fa&quot;) [1] &quot;臺&quot; Exercise 10.1 What is the character of the Unicode code point (hexadecimal) U+20AC? How do you find out the character in R? Exercise 10.2 What is the Unicode code point (in hexadecimal mode) for the character 我 and 你? Which character is larger in terms of the code points? Hexadecimal numerals are widely used by computer system designers and programmers, as they provide a human-friendly representation of binary-coded values. One single byte can encode 256 different characters, whose values may range from 00000000 to 11111111 in binary form. These binary forms can be represented as 00 to FF in hexadecimal. 10.5 Native R Functions for I/O 10.5.1 readLines() When working with text data, it is common to import text files into R for further data processing. To accomplish this, R provides a base function called readLines(). This function reads a txt file using line breaks as the delimiter and returns the contents of the file as a (character) vector. In other words, each line in the text file is treated as an independent element in the vector. alice &lt;- readLines(con = &quot;demo_data/corp-alice.txt&quot; ) alice[1:10] [1] &quot;[Alice&#39;s Adventures in Wonderland by Lewis Carroll 1865]&quot; [2] &quot;&quot; [3] &quot;CHAPTER I. Down the Rabbit-Hole&quot; [4] &quot;&quot; [5] &quot;Alice was beginning to get very tired of sitting by her sister on the&quot; [6] &quot;bank, and of having nothing to do: once or twice she had peeped into the&quot; [7] &quot;book her sister was reading, but it had no pictures or conversations in&quot; [8] &quot;it, &#39;and what is the use of a book,&#39; thought Alice &#39;without pictures or&quot; [9] &quot;conversation?&#39;&quot; [10] &quot;&quot; class(alice) [1] &quot;character&quot; length(alice) [1] 3331 Depending on how you arrange the contents in the text file, you may sometimes get a vector of different types: If each paragraph in the text file is a word, you get a word-based vector. If each paragraph in the text file is a sentence, you get a sentence-based vector. If each paragraph in the text file is a paragraph, you get a paragraph-based vector. There is another native R function for reading text files–scan(), which is similar to readLines(). However, scan() is a more versatile function that can be used to read in a wider variety of file formats, including numeric data and non-textual data. scan() reads in a text file and returns its contents as a vector of atomic objects (numeric, character, or logical), with the values separated by a specified delimiter (e.g., whitespace or commas). That is, scan() allows for users’ defined delimited (unlike the default line break in readLines()). In summary, readLines() is best used for reading in text files with a consistent line-by-line structure, while scan() is more flexible and can be used for reading in a wider range of data formats. Exercise 10.3 Based on the inspection of the vector alice created above, what is the content of each line in the original txt file (demo_data/corp-alice.txt)? If you need to import text files with specific encoding (e.g., big-5, utf-8, gb, ISO8859-1), the recommended method is as follows: ## Reading a big5 file infile &lt;- file(description = &quot;demo_data/data-chinese-poem-big5.txt&quot;, encoding = &quot;big-5&quot;) ## file as a connection text_ch_big5 &lt;- readLines(infile) ## read texts from the connection close(infile) ## close the connection writeLines(text_ch_big5) 春 眠 不 覺 曉 ， 處 處 聞 啼 鳥 。 夜 來 風 雨 聲 ， 花 落 知 多 少 。 ## Reading a utf-8 file infile &lt;- file(description = &quot;demo_data/data-chinese-poem-utf8.txt&quot;, encoding = &quot;utf-8&quot;) text_ch_big5 &lt;- readLines(infile) close(infile) writeLines(text_ch_big5) 春 眠 不 覺 曉 ， 處 處 聞 啼 鳥 。 夜 來 風 雨 聲 ， 花 落 知 多 少 。 ## Reading a gb18030 file infile &lt;- file(description = &quot;demo_data/data-chinese-poem-gb2312.txt&quot;, encoding = &quot;gb2312&quot;) text_ch_gb &lt;- readLines(infile) close(infile) writeLines(text_ch_gb) 春 眠 不 觉 晓 ， 处 处 闻 啼 鸟 。 夜 来 风 雨 声 ， 花 落 知 多 少 。 You can play with the following methods of loading the text files into R. Like I said, the above method is always recommended. It seems that the encoding settings within the readlines() or other R-native data loading functions (e.g., scan()) do not always work properly. (This is due to the variation of the operation systems and also the default locale of the OS.) x &lt;- readLines(&quot;demo_data/data-chinese-poem-big5.txt&quot;, encoding=&quot;big-5&quot;) y &lt;- scan(&quot;demo_data/data-chinese-poem-big5.txt&quot;, what=&quot;c&quot;,sep=&quot;\\n&quot;, encoding=&quot;big-5&quot;) y1 &lt;- scan(&quot;demo_data/data-chinese-poem-big5.txt&quot;, what=&quot;c&quot;,sep=&quot;\\n&quot;, fileEncoding=&quot;big-5&quot;) # x &lt;- readLines(&quot;demo_data/data-chinese-poem-gb2312.txt&quot;, encoding=&quot;gb2312&quot;) # y &lt;- scan(&quot;demo_data/data-chinese-poem-gb2312.txt&quot;, what=&quot;c&quot;,sep=&quot;\\n&quot;, encoding=&quot;gb2312&quot;) # y1 &lt;- scan(&quot;demo_data/data-chinese-poem-gb2312.txt&quot;, what=&quot;c&quot;,sep=&quot;\\n&quot;, fileEncoding=&quot;gb2312&quot;) ## The input texts do not show up properly? writeLines(x) writeLines(y) writeLines(y1) ## convert the input texts into your system default encoding writeLines(iconv(x, from=&quot;big-5&quot;,to=&quot;utf-8&quot;)) writeLines(iconv(y, from=&quot;big-5&quot;, to=&quot;utf-8&quot;)) 10.5.2 writeLines() After processing the data in R, we often need to save our data in an external file for future reference. For text data, we can use the base function writeLines() to save a character vector. By default, each element will be delimited by a line break after it is exported to the file. output &lt;- sample(alice[nzchar(alice)],10) writeLines(output, con = &quot;corp-alice-2.txt&quot;) Please note that when you writeLines(), you may also need to pay attention to the default encoding of the output file. This is especially important for Windows users. I think R will use the system default encoding as the expected encoding of the output file. For Mac, it’s UTF-8, which is perfect. However, for Windows, it is NOT. It depends on your OS language. Can you try to do the following and check the encoding of the output file? x &lt;- &quot;鳳凰臺上鳳凰遊，鳳去臺空江自流。&quot; Encoding(x) # `iconvlist()` check all encodings [1] &quot;UTF-8&quot; ## For Mac Users, we can output files in different encodings as follows. ## method1 writeLines(x, con = &quot;output_test_1.txt&quot;) ## method2 con &lt;- file(description = &quot;output_test_2.txt&quot;, encoding=&quot;big-5&quot;) writeLines(x, con) close(con) ## method3 con &lt;- file(description = &quot;output_test_3.txt&quot;, encoding=&quot;utf-8&quot;) writeLines(x, con) close(con) ## For Windows Users, please try the following? x_big5 &lt;- iconv(x, from=&quot;utf-8&quot;, to = &quot;big-5&quot;) writeLines(x, con = &quot;output_test_w_1.txt&quot;, useBytes = TRUE) writeLines(x_big5, con = &quot;output_test_w_2.txt&quot;, useBytes = TRUE) ## Check encodings uchardet::detect_file_enc(&quot;output_test_1.txt&quot;) ## mac1 [1] &quot;UTF-8&quot; uchardet::detect_file_enc(&quot;output_test_2.txt&quot;) ## mac2 [1] &quot;BIG5&quot; uchardet::detect_file_enc(&quot;output_test_3.txt&quot;) ## mac3 [1] &quot;UTF-8&quot; uchardet::detect_file_enc(&quot;output_test_w_1.txt&quot;) ## win1 [1] &quot;UTF-8&quot; uchardet::detect_file_enc(&quot;output_test_w_2.txt&quot;) ## win2 [1] &quot;BIG5&quot; It is said that writeLines() will attempt to re-encode the provided text to the native encoding of the system. This works fine when the system default is UTF-8. But in Windows, the default is NOT UTF-8. I can’t say I fully understand how the encoding system works with Windows. If you would like to know more about this, please refer to this article: String Encoding and R. Exercise 10.4 Please describe the meaning of the data processing in the code chunk above: sample(alice[nzchar(alice)],10). What did it do with the vector alice? Exercise 10.5 The above code is repeated below. How do you re-write the code using the %&gt;% pipe-based syntax with a structure provided below? Your output should be exactly the same as the output produced by the original codes. library(tidyverse) alice %&gt;% ... %&gt;% ... ... %&gt;% ... %&gt;% writeLiens(con=&quot;corp-alice-2.txt&quot;) Exercise 10.6 In our demo_data directory, there are three text files in different encodings: demo_data/chinese_gb2312.txt demo_data/chinese_big5.txt demo_data/chinese_utf8.txt Please figure out ways to load these files into R as vectors properly. All three files include the same texts: [1] &quot;這是中文字串。&quot; &quot;文本也有English Characters。&quot; The simplified Chinese version (i.e., chinese_gb2312.txt) [1] &quot;这是中文字串。&quot; &quot;文本也有English Characters。&quot; 10.6 readr Functions 10.6.1 readr::read_csv() Another common source of data is a spreadsheet-like tabular file, which corresponds to the data.frame in R. Usually we save these tabular data in a csv file, i.e., a comma-separated file. Although R has its own base functions for csv-files reading (e.g., read.table(), read.csv() etc.), here we will use the more powerful version read_csv() provided in the library of readr: library(readr) nobel &lt;- read_csv(file = &quot;demo_data/data-nobel-laureates.csv&quot;, locale = locale(encoding=&quot;UTF-8&quot;)) nobel The csv file is in fact a normal plain-text file. Each line consists of a row data, with the columns separated by commas. Sometimes we may receive a data set with other self-defined characters as the delimiter. Another often-seen case is to use the tab as the delimiter. Files with tab as the delimiter are often with the extension tsv. In readr, we can use read_tsv() to read tsv files. gender_freq &lt;- read_tsv(file = &quot;demo_data/data-stats-f1-freq.tsv&quot;, locale = locale(encoding=&quot;UTF-8&quot;)) gender_freq 10.6.2 readr::write_csv() In readr, we can also export our data frames to external files, using write_csv() or write_tsv(). Exercise 10.7 Load the plain-text csv file demo_data/data-bnc-bigram.csv into a data frame and print the top 20 bigrams in the R console arranged by their frequencies (i.e., bi.freq column). Exercise 10.8 Following Exercise 10.7, please export the data frame of the top 20 bigrams to an external file, named data-bnc-bigram-10.csv, and save it under your current working directory. 10.7 Directory Operations When we work with external files, we often need to deal with directories as well. Most importantly, we need to know both the paths and filenames of the external data in order to properly load the data into R. When we work with external files, it is crucial to deal with directories in addition to the files themselves. It is essential to have knowledge of both the path and filename of the external data to ensure that we can accurately load the data into R. 10.7.1 Working Directory When you start RStudio, R will use a directory as the working directory by default. If you start RStudio using the app icon, RStudio will use your system default directory as the working directory. If you start RStudio by opening a specific R script, RStudio will use the location of the source file as the working directory. 10.7.2 Relative vs. Absolute Paths When working with external files in R, we need to specify the path and filename of the external data to load it properly into R. There are two ways to specify the path to an external file, which are the absolute path and the relative path. The absolute path specifies the complete path to the file location on the computer. This can be very inconvenient as we need to specify the full path every time. For instance, in Windows, the absolute path always starts with the drive label (e.g., C:/Users/Alvinchen/...), while in Mac, it starts with the system root (e.g., /Users/Alvinchen/...). &quot;C:/Users/Alvin/Documents/ENC2055/demo_data/data-bnc-bigrams.csv&quot; Alternatively, we can use the relative path, which specifies the path in relation to the current working directory. By default, if we only provide the filename, R will search for the file in the working directory. For example, if we use the read_csv(\"data-bnc-bigram.csv\") function, R will look for the file data-bnc-bigram.csv in the working directory. If the file is not in the working directory, R will return an error message. ## Example of `relative path` x &lt;- read_csv(&quot;data-bnc-bigram.csv&quot;) If the file is located in a sub-directory (of the working directory), we can specify the path to the sub-directory from the current working directory. For instance, read_csv(\"demo_data/data-bnc-bigram.csv\") specifies a sub-directory called demo_data, which is located under the current working directory, and within the sub-directory, R will look for the file data-bnc-bigram.csv. ## Example of `relative path` with a sub-directory x &lt;- read_csv(&quot;demo_data/data-bnc-bigram.csv&quot;) We can also use .. to specify the parent directory of the current working directory. For example, read_csv(\"../data-bnc-bigram.csv\") specifies that R should look for the file data-bnc-bigram.csv in the parent directory of the current working directory. It is worth noting that people sometimes use . to refer to the current working directory. ## Example of `relative path` from the parent directory of the working dir x &lt;- read_csv(&quot;../data-bnc-bigram.csv&quot;) 10.7.3 Directory Operations There are a few important directory operations that we often need when working with files (input/output): getwd(): check the working directory of the current R session setwd(): set the working directory for the current R session getwd() setwd() By default, R looks for the filename or the path under the working directory unless the absolute/relative path to the files/directories is particularly specified. ## get all filenames contained in a directory dir(path = &quot;demo_data/&quot;, full.names = FALSE, ## whether to get both filenames &amp; full relative paths recursive = FALSE) ## check whether a file/directory exists file.exists(&quot;demo_data/data-bnc-bigram.csv&quot;) ## check all filenames contained in the directory ## that goes up two levels from the working dir dir(path = &quot;../../&quot;) The path dir(path = \"../../\") specifies a directory path that goes up two levels in the file hierarchy from the current working directory. The double dots .. are used to navigate to the parent directory, so ../ moves up one level in the file hierarchy, and ../../ moves up two levels. For example, if the current working directory is C:/Users/username/Documents/project/data, then ../../ will take you to C:/Users/username/. If there is a directory named demo at that level, you can access it by appending demo/ to the end of the path: ../../demo/. 10.7.4 Loading files from a directory When working with external files, it is often necessary to load all the files from a particular directory into R. This task typically involves two steps: First, we need to obtain a list of the filenames contained in the directory. Next, we need to load the data from each file into R using its corresponding filename. For example, suppose we want to load all the text files from the directory demo_data/shakespeare. To do this, we need to first obtain a list of the filenames in the directory, and then use a loop to read each file into R based on its filename. ## Corpus root directory corpus_root_dir &lt;- &quot;demo_data/shakespeare&quot; ## Get the filenames from the directory flist &lt;- dir(path = corpus_root_dir, full.names = TRUE) flist [1] &quot;demo_data/shakespeare/Hamlet, Prince of Denmark.txt&quot; [2] &quot;demo_data/shakespeare/King Lear.txt&quot; [3] &quot;demo_data/shakespeare/Macbeth.txt&quot; [4] &quot;demo_data/shakespeare/Othello, the Moor of Venice.txt&quot; ## Holder for all data flist_texts &lt;- list() ## Traverse each file for (i in 1:length(flist)) { flist_texts[[i]] &lt;- readLines(flist[i]) } ## peek at first file content head(flist_texts[[1]]) ## first 6 [1] &quot;&lt; Shakespeare -- HAMLET, PRINCE OF DENMARK &gt;&quot; [2] &quot;&lt; from Online Library of Liberty (http://oll.libertyfund.org) &gt;&quot; [3] &quot;&lt; Unicode .txt version by Mike Scott (http://www.lexically.net) &gt;&quot; [4] &quot;&lt; from \\&quot;The Complete Works of William Shakespeare\\&quot; &gt;&quot; [5] &quot;&lt; ed. with a glossary by W.J. Craig M.A. &gt;&quot; [6] &quot;&lt; (London: Oxford University Press, 1916) &gt;&quot; tail(flist_texts[[1]]) ## last 6 [1] &quot;&lt;/FORTINBRAS&gt;&quot; [2] &quot;&lt;STAGE DIR&gt;&quot; [3] &quot;&lt;A dead march. Exeunt, bearing off the bodies; after which a peal of ordnance is shot off.&gt;&quot; [4] &quot;&lt;/STAGE DIR&gt;&quot; [5] &quot;&lt;/SCENE 2&gt;&quot; [6] &quot;&lt;/ACT 5&gt;&quot; In R, we can apply a function to all the elements in a vector using sapply(). We can simplify the code above, which uses a for-loop structure, into one line of code: ## Load data from all files flist_text &lt;- sapply(flist, readLines) ## peek at first file content head(flist_texts[[1]]) ## first 6 [1] &quot;&lt; Shakespeare -- HAMLET, PRINCE OF DENMARK &gt;&quot; [2] &quot;&lt; from Online Library of Liberty (http://oll.libertyfund.org) &gt;&quot; [3] &quot;&lt; Unicode .txt version by Mike Scott (http://www.lexically.net) &gt;&quot; [4] &quot;&lt; from \\&quot;The Complete Works of William Shakespeare\\&quot; &gt;&quot; [5] &quot;&lt; ed. with a glossary by W.J. Craig M.A. &gt;&quot; [6] &quot;&lt; (London: Oxford University Press, 1916) &gt;&quot; tail(flist_texts[[1]]) ## last 6 [1] &quot;&lt;/FORTINBRAS&gt;&quot; [2] &quot;&lt;STAGE DIR&gt;&quot; [3] &quot;&lt;A dead march. Exeunt, bearing off the bodies; after which a peal of ordnance is shot off.&gt;&quot; [4] &quot;&lt;/STAGE DIR&gt;&quot; [5] &quot;&lt;/SCENE 2&gt;&quot; [6] &quot;&lt;/ACT 5&gt;&quot; Exercise 10.9 Please make yourself familar with the following commands: file.create(), dir.create(), unlink(), basename(),file.info(), save(), and load(). Exercise 10.10 Please create a sub-directory in your working directory, named temp. Load the dataset demo_data/data-bnc-bigram.csv and subset bigrams whose bigram frequencies (bi.freq column) are larger than 200. Order the sub data frame according to the bigram frequencies in a descending order and save the sub data frame into a csv file named data-bnc-bigram-freq200.csv in the temp directory. bnc_bigram_freq200 &lt;- read_csv(&quot;temp/data-bnc-bigram-freq200.csv&quot;) bnc_bigram_freq200 "],["iteration.html", "Chapter 11 Iteration 11.1 Code Duplication 11.2 Vectorized Functions: Vector vs. List 11.3 Iteration 11.4 purr 11.5 purr + dplyr 11.6 map() with self-defined functions", " Chapter 11 Iteration 11.1 Code Duplication Code duplication is a significant problem in programming that should be avoided whenever possible. When you notice repetitive code within your script, it is generally not an ideal situation. Repeating the same code in multiple places can lead to difficulties in maintaining and debugging your script. When the same code chunk is repeated in multiple places, it becomes challenging to track and update. If a bug is discovered or an improvement is needed, you have to make changes in every instance of the duplicated code, which is time-consuming and error-prone. It also makes the script harder to understand for other developers, as they have to navigate through redundant code segments. To mitigate these issues, it is advisable to refactor the duplicated code into reusable functions or modules. By encapsulating the common functionality in a single place, you can reduce redundancy and improve code maintainability. In short, there are generally two major ways to reduce duplication in coding: Wrap the duplicate procedures into a function Use iteration structure in script In this chapter, we will talk about code efficiency. In particular, we will work with the library purr. According to Wickham &amp; Grolemund (2017) Chapter 21 Iteration, there are three main advantages of reducing code duplication: It’s easier to see the intent/objective of your code, because your eyes are drawn to what’s different, not what stays the same. It’s easier to respond to changes required for code maintenance. Without much code duplication, you only need to make changes in one place, rather than remembering to change every place that you have copied-and-pasted the code. You’re likely to have fewer bugs because each line of code is used in a limited place. 11.2 Vectorized Functions: Vector vs. List Most of the R-internal functions are vectorized. By default, if we apply a function to a multi-element vector, R will automatically apply the same procedure to all elements of the vector all at once, and return the results of the same vector’s length. a.vec &lt;- c(1:10) ## `sqrt()` is applied to all elements of the vector one by one sqrt(a.vec) [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 [9] 3.000000 3.162278 ## `round()` is applied to all elements of the vector one by one round(sqrt(a.vec), 2) [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 The concept of a vectorized function in R is important here. A vectorized function is designed to operate on an entire vector (or multiple vectors) as a whole, performing the same operation on each element simultaneously. It leverages the internal optimizations in R to process the vector efficiently, resulting in faster and more concise code. But this is NOT something we can do with a list: a.list &lt;- list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) sqrt(a.list) Error in sqrt(a.list): non-numeric argument to mathematical function You may now recall that in Chapter 5 , we have introduced the control structure of for-loop, which allows us to perform a specific procedure to every element of a list. a.list &lt;- list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) results &lt;- vector() # empty list holder for(i in 1:length(a.list)){ results[i] &lt;- sqrt(a.list[[i]]) } results [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 [9] 3.000000 3.162278 In this chapter, we will explore more efficient ways of iterating over non-vector objects such as lists and data frames. 11.3 Iteration Because we often work with list and data.frame (tibble) objects in R, it would be great if we can have a straightforward approach for performing the same procedure on: Each element in the list Each row in the data.frame Each column in the data.frame These three scenarios are the most-often used contexts for iteration (i.e., iterating over data structures). Let’s start with a example. We first create a pseudo data set, i.e., a list with students’ grades from five different classes. exams.list &lt;- list( class1 = round(runif(30, 0, 100)), # 30 tokens of random numbers in the range &lt;0, 100&gt; class2 = round(runif(30, 0, 100)), class3 = round(runif(30, 0, 100)), class4 = round(runif(30, 0, 100)), class5 = round(runif(30, 0, 100)) ) exams.list $class1 [1] 90 69 80 2 48 76 22 32 23 14 41 41 37 15 14 23 47 27 86 5 44 80 12 56 21 [26] 13 75 90 37 67 $class2 [1] 9 38 27 81 45 81 81 79 44 75 63 71 0 48 22 38 61 35 11 24 67 42 79 10 43 [26] 98 89 89 18 13 $class3 [1] 65 34 66 32 19 78 9 47 51 60 33 49 95 48 89 91 61 41 15 94 30 6 95 72 14 [26] 55 95 59 40 65 $class4 [1] 32 31 22 37 98 15 9 14 69 62 89 67 74 52 66 82 79 98 44 31 41 1 18 84 23 [26] 24 8 25 73 85 $class5 [1] 50 39 25 11 39 57 22 44 22 50 35 65 37 36 53 74 22 41 27 63 18 86 75 67 62 [26] 37 53 87 58 84 If we want to compute the mean scores of each cluster, you probably want to use mean(): mean(exams.list) [1] NA It should be clear now that mean() expects a numeric vector, on which the mean score is computed. So you might wonder, why not take a simple approach? We can calculate the mean scores for each class and store all five scores in a list: set.seed(123) # Make sure we get the same results exams.list.means &lt;- list( class1mean = mean(exams.list$class1), class2mean = mean(exams.list$class2), class3mean = mean(exams.list$class3), class4mean = mean(exams.list$class4), class5mean = mean(exams.list$class5) ) exams.list.means $class1mean [1] 42.9 $class2mean [1] 49.36667 $class3mean [1] 53.6 $class4mean [1] 48.43333 $class5mean [1] 47.96667 The disadvantage is obvious: (a) what if you have 10 classes? 100 classes? (b) what if now you decide to compute standard deviation? The rule-of-thumb is that the more you find code duplication in your script, the more likely you need to restructure your codes with iterations. 11.4 purr library(tidyverse) Now let’s take a look at how iteration structures can help us with repeated procedures. map(exams.list, mean) $class1 [1] 42.9 $class2 [1] 49.36667 $class3 [1] 53.6 $class4 [1] 48.43333 $class5 [1] 47.96667 ## Or, alternatively: # exams.list %&gt;% map(mean) With only one-line code, you can apply a function to each element of a data structure and obtain the results as a new data structure of the same type. It simplifies the process of applying a function iteratively and collecting the output at the same time. map() is a very powerful function to do iteration. Its usage is as follows: Let’s break down the code map(exams.list, mean) and explain it concisely: The map() function is used to apply the mean function to each element in the exams.list. The result of applying mean to the first element is stored as the first element in a new list. Similarly, the result of applying mean to the second element is stored as the second element in the new list. This process continues for each element in the exams.list. Finally, the function returns a list containing all the computed mean values. In the purrr package, the map() function defaults to returning the result as a list. However, you can use other variants of the mapping function, like map_dbl() for a double vector, map_int() for an integer vector, or map_chr() for a character vector, to specify a different output data structure. These variants allow you to tailor the output format based on your specific requirements. ## apply `mean()` to all elements of `exam.list` ## and return the output as data frame map_df(exams.list, mean) ## apply `mean()` to all elements of `exam.list` ## and return the output as a double/numeric vector map_dbl(exams.list, mean) class1 class2 class3 class4 class5 42.90000 49.36667 53.60000 48.43333 47.96667 In R, the apply() function, along with its variants like lapply() and sapply(), is a native R function used for applying a function to a specific data structure. These native R functions are very similar to map() in purrr library. Here’s a brief comparison between apply() and map() from the purrr library: apply(): It is primarily used with matrices or multidimensional arrays. It applies a function to either rows or columns of the matrix or array, aggregating the results. It is efficient for handling structured data and performing calculations across dimensions. lapply(): It applies a function to each element of a list or vector and returns a list of results. It is commonly used for iterative operations on lists and allows you to process each element individually. sapply(): It is similar to lapply() but simplifies the result by attempting to convert it into a more convenient format, such as a vector or matrix. It is useful when you want a simplified output and don’t necessarily need a list structure. map() (from purrr): It is part of the purrr library, which is an extension of the apply family of functions. map() provides a consistent and powerful approach to iterate over elements in various data structures, including lists, data frames, and vectors. It returns a list as the output, allowing you to work with the results in a flexible manner. In summary, while the native R functions like apply(), lapply(), and sapply() provide useful ways to iterate over data structures, map() from the purrr library offers additional flexibility and consistency when working with lists, data frames, and vectors. It is particularly beneficial when you need to apply a function to multiple elements and handle the results in a list format. Exercise 11.1 Use the same dataset, exam.list, and compute the median and standard deviation values for each class. Have these values returned as vectors. Median class1 class2 class3 class4 class5 39.0 44.5 53.0 42.5 47.0 Standard Deviation class1 class2 class3 class4 class5 27.67714 28.71919 27.34782 29.94096 21.19285 Exercise 11.2 Similar to the previous exercise, how can you use the same dataset, exam.list, to compute the median and standard deviation values for each class, and have both of these values returned as a data.frame (The first row refers to the median values and the second row refers to the standard deviation values.) Please use map_df() to produce the following expected result. 11.5 purr + dplyr When working with data frames in R, the map() function from the purrr library can be a powerful and efficient tool for data manipulation. When used in combination with the mutate() function from the dplyr package, it allows for seamless transformation of columns in a data frame. Specifically, in the context of data frame, map() can be used to apply a function to a column or a selected set of columns. Therefore, by combining map() with mutate(), we can efficiently apply a function to each column of a data frame and generate new columns based on the results. To illustrate this usage, let’s look at an example of the four-character idioms dataset from the previous chapter. ## reading utf8 file con &lt;- file(description = &quot;demo_data/dict-ch-idiom.txt&quot;, encoding = &quot;utf-8&quot;) texts &lt;- readLines(con) close(con) ## convert into data frame idiom &lt;- data.frame(string = texts) idiom Now if we would like to find out whether each idiom has duplicate characters in it, we can make use of regular expressions: ## Take a look at the first ten idioms x &lt;- idiom$string[1:10] ## Check whether they have repeated characters str_detect(x, &quot;.*(.).*\\\\1.*&quot;) [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE What if we would like to include this information as an independent (new) column of data frame, idiom? Two important things should pop up in your mind: We need mutate() to help us create a new column We need to apply the above procedure, str_detect(), to each element in the column idiom$string. idiom %&gt;% mutate(duplicate = str_detect(string, &quot;.*(.).*\\\\1.*&quot;)) %&gt;% filter(duplicate) So far, we do not have to use map() because str_detect() is a vectorized function, i.e., when it takes a vector as its input, it performs the pattern matching for all the elements of the input string vector, and outputs a vector, which can be a perfect candidate for a new column. Now, what if we would like to identify idioms including animal characters and create a new column that shows the number of animal characters for these idioms? It is clear that we need to use str_extract_all() to extract all possible matches from each idiom. And it should also be clear that the output of str_extract_all() is a list. ## Regex patter for animal characters pat &lt;- &quot;[鼠牛虎兔龍蛇馬羊猴雞狗豬]&quot; ## Output of str_extract_all output &lt;- str_extract_all(idiom$string, pat) ## Example idiom[895,] [1] &quot;白馬非馬&quot; output[895] [[1]] [1] &quot;馬&quot; &quot;馬&quot; So now the question is: how can we get the information (i.e., the number of animal characters) from the output of str_extract_all() and add this information to the existing data frame as a new column? And the hint is clear: you need to make use of the iteration function: map(). See if you are able to produce an output as shown below using the code template provided: ## Code Template idiom %&gt;% mutate(animals = str_extract_all(......)) %&gt;% ## extract matches mutate(num_animal = map(......)) %&gt;% ## compute number of matches filter(......) %&gt;% ## remove idioms with no matches select(-animals) ## remove irrelevant columns Exercise 11.3 This exercise will use the subset of idiom, which include only four-word idioms with at least one duplicate character in them. Please create a new column, showing how many types of duplications there are in each idiom? For example, in 阿狗阿貓, there is only one duplicate character 阿; but in 矮矮胖胖, there are two duplicate characters, 矮 and 胖. Exercise 11.4 Continuing the previous exercise, please create another new column, showing all the duplicate characters in each idiom. For example, in 阿狗阿貓, the duplicate character is 阿; but in 矮矮胖胖, the duplicate character is矮_胖. That is, if the idiom has more than one duplicate character, please use the _ as the delimiter and concatenate all duplicate characters into a long string. Exercise 11.5 Based on the previous exercise, please analyze the distribution of all the duplicate characters in the four-character idioms included in the dictionary (i.e., the duplicate_char column from the previous exercise) and identify the top 20 duplicate characters. Please visualize your results in a bar plot with both the top 20 duplicate characters as well as the number of the character’s duplications in the idiom. 11.6 map() with self-defined functions With the power and flexibility of purrr::map(), we can basically do everything iteratively. More attractively, we can apply a self-defined function as well! (Please see Chapter 6 for how to create a self-defined function in R.) A function object is defined using the following template: FUNCTION_NAME &lt;- function(ARG1, ARG2) { THINGS TO BE DONE WITHIN THE FUNCTION return(...) } A function object usually include: Self-defined name Arguments Return Let’s consider a simple example. First we define a custom function called my_center(): This function takes a vector object x as its input argument; It subtracts each element of x by the mean score of x, and divides the difference by the standard deviation of x; The resulting vector is returned as the output of the function. ## define function my_center &lt;- function(x) { (x - mean(x))/sd(x) } Now we can apply our my_center function to each element (e.g., the scores of each class) in exams.list: ## apply function via map() map(exams.list, my_center) $class1 [1] 1.70176536 0.94301647 1.34045637 -1.47775379 0.18426759 1.19593277 [7] -0.75513580 -0.39382680 -0.71900490 -1.04418299 -0.06864871 -0.06864871 [13] -0.21317231 -1.00805209 -1.04418299 -0.71900490 0.14813669 -0.57448130 [19] 1.55724176 -1.36936109 0.03974399 1.34045637 -1.11644479 0.47331478 [25] -0.79126670 -1.08031389 1.15980187 1.70176536 -0.21317231 0.87075468 $class2 [1] -1.40556418 -0.39578645 -0.77880559 1.10147019 -0.15204699 1.10147019 [7] 1.10147019 1.03183035 -0.18686691 0.89255066 0.47471160 0.75327098 [13] -1.71894348 -0.04758723 -0.95290519 -0.39578645 0.40507176 -0.50024621 [19] -1.33592434 -0.88326535 0.61399129 -0.25650676 1.03183035 -1.37074426 [25] -0.22168684 1.69340887 1.38002957 1.38002957 -1.09218488 -1.26628449 $class3 [1] 0.41685219 -0.71669323 0.45341817 -0.78982519 -1.26518295 0.89220994 [7] -1.63084276 -0.24133548 -0.09507155 0.23402228 -0.75325921 -0.16820351 [13] 1.51383162 -0.20476949 1.29443574 1.36756770 0.27058826 -0.46073136 [19] -1.41144688 1.47726564 -0.86295716 -1.74054071 1.51383162 0.67281405 [25] -1.44801286 0.05119237 1.51383162 0.19745630 -0.49729734 0.41685219 $class4 [1] -0.5488580 -0.5822570 -0.8828486 -0.3818627 1.6554804 -1.1166421 [7] -1.3170365 -1.1500412 0.6869075 0.4531140 1.3548888 0.6201093 [13] 0.8539028 0.1191233 0.5867103 1.1210953 1.0208981 1.6554804 [19] -0.1480692 -0.5822570 -0.2482664 -1.5842290 -1.0164449 1.1878934 [25] -0.8494496 -0.8160505 -1.3504356 -0.7826514 0.8205037 1.2212925 $class5 [1] 0.09594432 -0.42309872 -1.08369896 -1.74429919 -0.42309872 0.42624444 [7] -1.22525615 -0.18717007 -1.22525615 0.09594432 -0.61184165 0.80373029 [13] -0.51747018 -0.56465592 0.23750151 1.22840187 -1.22525615 -0.32872726 [19] -0.98932750 0.70935882 -1.41399908 1.79463064 1.27558760 0.89810175 [25] 0.66217309 -0.51747018 0.23750151 1.84181637 0.47343017 1.70025918 ## try different return structures map_df(exams.list, my_center) After centering, we can quickly identify subjects whose scores are one standard deviation above their class average. Based on the skills you’ve learned, any idea how to do this? Exercise 11.6 Use the built-in the mtcars dataset (?mtcars for more detail). How do you get the class type of each column in the mtcars by using map()? Exercise 11.7 Create a self-defined function to convert each number of a numeric vector to a “z” score. y &lt;- c(1, 4, 6, 10, 20) my_z(y) [1] -0.9779865 -0.5704921 -0.2988292 0.2444966 1.6028112 Exercise 11.8 Use the earlier dataset exams.list. For each element in exams.list, please convert the student’s score to a z-score by applying your self-defined function in an iterative structure (e.g., map). Please present the result as a data frame. References Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. "],["data-scientist-first-step.html", "Chapter 12 Data Scientist First Step 12.1 Loading Nobel Laureates Dataset 12.2 Workflow 12.3 Column Names 12.4 Missing Data (NA values) 12.5 Data Preprocessing 12.6 Exploratory Analysis 12.7 Exercises", " Chapter 12 Data Scientist First Step Finally this chapter will demonstrate how you can make use of what you have learned from the previous chapters to perform an exploratory data analysis on the dataset you are interested in. Here we will look at a dataset of Nobel Laureates. Before you start, always remember to load necessary R libraries first. ## Loading libraries library(tidyverse) 12.1 Loading Nobel Laureates Dataset ## Data Import my_df &lt;- read_csv(file = &quot;demo_data/data-nobel-laureates.csv&quot;, locale = locale(encoding=&quot;UTF-8&quot;)) ## Peek my_df ## data overview str(my_df) spc_tbl_ [969 × 18] (S3: spec_tbl_df/tbl_df/tbl/data.frame) $ Year : num [1:969] 1901 1901 1901 1901 1901 ... $ Category : chr [1:969] &quot;Chemistry&quot; &quot;Literature&quot; &quot;Medicine&quot; &quot;Peace&quot; ... $ Prize : chr [1:969] &quot;The Nobel Prize in Chemistry 1901&quot; &quot;The Nobel Prize in Literature 1901&quot; &quot;The Nobel Prize in Physiology or Medicine 1901&quot; &quot;The Nobel Peace Prize 1901&quot; ... $ Motivation : chr [1:969] &quot;\\&quot;in recognition of the extraordinary services he has rendered by the discovery of the laws of chemical dynamic&quot;| __truncated__ &quot;\\&quot;in special recognition of his poetic composition, which gives evidence of lofty idealism, artistic perfection&quot;| __truncated__ &quot;\\&quot;for his work on serum therapy, especially its application against diphtheria, by which he has opened a new ro&quot;| __truncated__ NA ... $ Prize Share : chr [1:969] &quot;1/1&quot; &quot;1/1&quot; &quot;1/1&quot; &quot;1/2&quot; ... $ Laureate ID : num [1:969] 160 569 293 462 463 1 161 571 294 464 ... $ Laureate Type : chr [1:969] &quot;Individual&quot; &quot;Individual&quot; &quot;Individual&quot; &quot;Individual&quot; ... $ Full Name : chr [1:969] &quot;Jacobus Henricus van &#39;t Hoff&quot; &quot;Sully Prudhomme&quot; &quot;Emil Adolf von Behring&quot; &quot;Jean Henry Dunant&quot; ... $ Birth Date : Date[1:969], format: &quot;1852-08-30&quot; &quot;1839-03-16&quot; ... $ Birth City : chr [1:969] &quot;Rotterdam&quot; &quot;Paris&quot; &quot;Hansdorf (Lawice)&quot; &quot;Geneva&quot; ... $ Birth Country : chr [1:969] &quot;Netherlands&quot; &quot;France&quot; &quot;Prussia (Poland)&quot; &quot;Switzerland&quot; ... $ Sex : chr [1:969] &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... $ Organization Name : chr [1:969] &quot;Berlin University&quot; NA &quot;Marburg University&quot; NA ... $ Organization City : chr [1:969] &quot;Berlin&quot; NA &quot;Marburg&quot; NA ... $ Organization Country: chr [1:969] &quot;Germany&quot; NA &quot;Germany&quot; NA ... $ Death Date : Date[1:969], format: &quot;1911-03-01&quot; &quot;1907-09-07&quot; ... $ Death City : chr [1:969] &quot;Berlin&quot; &quot;Châtenay&quot; &quot;Marburg&quot; &quot;Heiden&quot; ... $ Death Country : chr [1:969] &quot;Germany&quot; &quot;France&quot; &quot;Germany&quot; &quot;Switzerland&quot; ... - attr(*, &quot;spec&quot;)= .. cols( .. Year = col_double(), .. Category = col_character(), .. Prize = col_character(), .. Motivation = col_character(), .. `Prize Share` = col_character(), .. `Laureate ID` = col_double(), .. `Laureate Type` = col_character(), .. `Full Name` = col_character(), .. `Birth Date` = col_date(format = &quot;&quot;), .. `Birth City` = col_character(), .. `Birth Country` = col_character(), .. Sex = col_character(), .. `Organization Name` = col_character(), .. `Organization City` = col_character(), .. `Organization Country` = col_character(), .. `Death Date` = col_date(format = &quot;&quot;), .. `Death City` = col_character(), .. `Death Country` = col_character() .. ) - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ## data summary summary(my_df) Year Category Prize Motivation Min. :1901 Length:969 Length:969 Length:969 1st Qu.:1947 Class :character Class :character Class :character Median :1976 Mode :character Mode :character Mode :character Mean :1970 3rd Qu.:1999 Max. :2016 Prize Share Laureate ID Laureate Type Full Name Length:969 Min. : 1.0 Length:969 Length:969 Class :character 1st Qu.:230.0 Class :character Class :character Mode :character Median :462.0 Mode :character Mode :character Mean :470.2 3rd Qu.:718.0 Max. :937.0 Birth Date Birth City Birth Country Sex Min. :1817-11-30 Length:969 Length:969 Length:969 1st Qu.:1891-06-02 Class :character Class :character Class :character Median :1916-09-14 Mode :character Mode :character Mode :character Mean :1911-02-02 3rd Qu.:1935-07-13 Max. :1997-07-12 NA&#39;s :31 Organization Name Organization City Organization Country Length:969 Length:969 Length:969 Class :character Class :character Class :character Mode :character Mode :character Mode :character Death Date Death City Death Country Min. :1903-11-01 Length:969 Length:969 1st Qu.:1954-07-14 Class :character Class :character Median :1980-04-15 Mode :character Mode :character Mean :1975-03-04 3rd Qu.:1999-07-26 Max. :2017-02-08 NA&#39;s :352 When dealing with files that contain Chinese or non-English characters, or files encoded in UTF-8, I recommend explicitly specifying the encoding of the file to ensure proper data import. 12.2 Workflow 12.3 Column Names Before we begin, it’s evident that the column names contain spaces that we want to remove. Our objective is to eliminate the spaces and replace them with underscores (_). ## Original column names names(my_df) [1] &quot;Year&quot; &quot;Category&quot; &quot;Prize&quot; [4] &quot;Motivation&quot; &quot;Prize Share&quot; &quot;Laureate ID&quot; [7] &quot;Laureate Type&quot; &quot;Full Name&quot; &quot;Birth Date&quot; [10] &quot;Birth City&quot; &quot;Birth Country&quot; &quot;Sex&quot; [13] &quot;Organization Name&quot; &quot;Organization City&quot; &quot;Organization Country&quot; [16] &quot;Death Date&quot; &quot;Death City&quot; &quot;Death Country&quot; ## Overwrite the original with new ones names(my_df) &lt;- str_replace_all(names(my_df), &quot;\\\\s&quot;,&quot;_&quot;) ## Autoprint updated my_df my_df ## Check the names again names(my_df) [1] &quot;Year&quot; &quot;Category&quot; &quot;Prize&quot; [4] &quot;Motivation&quot; &quot;Prize_Share&quot; &quot;Laureate_ID&quot; [7] &quot;Laureate_Type&quot; &quot;Full_Name&quot; &quot;Birth_Date&quot; [10] &quot;Birth_City&quot; &quot;Birth_Country&quot; &quot;Sex&quot; [13] &quot;Organization_Name&quot; &quot;Organization_City&quot; &quot;Organization_Country&quot; [16] &quot;Death_Date&quot; &quot;Death_City&quot; &quot;Death_Country&quot; By completing these steps, we will have transformed the column names by removing spaces and replacing them with underscores, ensuring consistency and ease of use in further data processing. 12.4 Missing Data (NA values) It is always important to check if the information of each row is complete. We first define a function check_num_NA(), which checks the number of NA values of the input x object We then map() this self-defined function to each column of the my_df ## Define a custom function check_num_NA &lt;- function(x){ sum(is.na(x)) } ## Map the function to the data frame my_df %&gt;% map_df(check_num_NA) my_df %&gt;% map_int(check_num_NA) Year Category Prize 0 0 0 Motivation Prize_Share Laureate_ID 88 0 0 Laureate_Type Full_Name Birth_Date 0 0 31 Birth_City Birth_Country Sex 28 26 26 Organization_Name Organization_City Organization_Country 247 253 253 Death_Date Death_City Death_Country 352 370 364 ## Alternatively, you can write this way: # map_df(my_df, check_num_NA) # map_int(my_df, check_num_NA) From the above results, we see that Sex column has 26 cases of NA values. We can examine the rows with NA’s in more detail by filtering out these cases: my_df %&gt;% filter(is.na(Sex)) After inspection of these 26 cases, can you make any generalization to account for their missing values in Sex column? Another method is to examine cases/rows where there is at least one NA value, using complete.cases(): ## Filter incomplete cases my_df %&gt;% filter(!complete.cases(.)) -&gt; my_df_missing my_df_missing ## Check the percentage of incomplete cases nrow(my_df_missing)/nrow(my_df) [1] 0.5614035 When dealing with NA (missing) values in data analysis, here are a few suggestions to consider: Identify the presence of NA values: Begin by checking your data to identify the presence of NA values. This can be done using functions like is.na() or complete.cases(). Understand the nature of missingness: It’s essential to understand why the NA values exist in your data. Are they at random or systematic? This understanding can help inform the appropriate handling strategy. Analyze missing data patterns: It can be insightful to analyze the patterns and relationships between missing values and other variables. This analysis can provide additional insights into the reasons for missingness and potential biases in your data. Remove NA values: In some cases, you may choose to remove rows or columns with NA values if they are not critical for your analysis. This can be done using functions like na.omit() or complete.cases(). Impute missing values: If the NA values are significant and removing them would result in a loss of valuable information, you can impute (replace) them with estimated values. Common imputation methods include mean imputation, median imputation, or using more advanced techniques like regression imputation or multiple imputation. Document and report missing data handling: Whichever approach you choose, it’s important to document and report how you handled missing data. This ensures transparency and allows others to understand the impact of missing data on your analysis. 12.5 Data Preprocessing Before proceeding with data analysis, it is essential to preprocess the data to ensure it is appropriately prepared for quantitative (statistical) analysis. In our dataset, it is possible to encounter duplicate data points (e.g., Laureate_ID = 838): ## Check laureates from China my_df %&gt;% filter(Birth_Country == &quot;China&quot;) Alternatively, the factor of interest may be implicit and embedded within the dataset. For example, if we would like to examine the Nobel Prize winners by “the age of winning” or “the decades”, these factors of interest may not be straightforward from the existing columns in the dataset: ## Check `Year` and `Bird_Date` ## Information implicit in these columns my_df %&gt;% select(Laureate_ID, Year, Full_Name, Birth_Date, Category) Some common considerations for data preprocessing include: Cleaning the column names to ensure they are consistent and easy to work with. Handling missing data Creating unique indices for all data points Lowercasing all character vectors (i.e., text columns) to normalize the letter casing Identifying duplicate tokens within the dataset Creating new columns that better reflect the factors of interest, such as: Decade Prize_Age ## Data preprocessing nobel_winners &lt;- my_df %&gt;% mutate(id = row_number()) %&gt;% ## unique row indices mutate_if(is.character, tolower) %&gt;% ## lower all character vectors distinct_at(vars(Full_Name, Year, Category), .keep_all = TRUE) %&gt;% ## duplicates mutate(Decade = 10 * (Year %/% 10), ## create new factors Prize_Age = Year - lubridate::year(Birth_Date)) ## check nobel_winners Please note that the necessity of the aforementioned preprocessing steps may vary depending on the characteristics of your dataset and the specific research questions you are addressing. It is important to carefully consider which preprocessing steps are relevant and applicable to your particular case. Furthermore, the order in which these preprocessing steps are performed is not arbitrary and can have an impact on the outcomes of your analysis. The optimal ordering of preprocessing steps may vary depending on the nature of your data and the objectives of your research. It is advisable to take into account the specific requirements and considerations of your study when determining the most appropriate order for performing these steps. Given two positive integers, normally, we define the division as follows: a / b in R. There are two variants of the integer division: a %% b (a modulo b): This modulo operation will return the remainder of the division (The expression 5 %% 3 would evaluate to 2). a %/% b: This integer division operation will return the quotient of the division (The expression 5 %/% 3 would evaluate to 1). We often use this modulo operation to check if an output value is a multiple of a specific integer. We can check if our new Prize_Age has any NA values? nobel_winners %&gt;% filter(is.na(Prize_Age)) %&gt;% select(Laureate_ID, Year, Full_Name, Birth_Date, Prize_Age) Check Chinese laureates again: nobel_winners %&gt;% filter(Birth_Country == &quot;china&quot;) %&gt;% select(Laureate_ID, Full_Name, Year, Category) 12.6 Exploratory Analysis After performing data preprocessing, the next step is to conduct exploratory data analysis. As mentioned in Chapter 7, data visualization is often a useful starting point for exploratory analysis. In this section, we will explore how to generate relevant statistics and graphs based on different research questions. By utilizing these tools, we can gain insights and uncover patterns in the data, helping us better understand the underlying relationships and trends. 12.6.1 Discipline Distribution RQ: What is the distribution of laureates across different disciplines? ## statistics nobel_winners %&gt;% count(Category) %&gt;% mutate(percent = round(n/sum(n),2)) ## barplots nobel_winners %&gt;% count(Category) %&gt;% ggplot(aes(x = Category, y = n, fill = Category)) + geom_col() + geom_text(aes(label = n), vjust = -0.25) + labs(title = &quot;No. of Laureates in Different Disciplines&quot;, x = &quot;Category&quot;, y = &quot;N&quot;) + theme(legend.position = &quot;none&quot;) ## barplots (ordered) nobel_winners %&gt;% count(Category) %&gt;% ggplot(aes(x = fct_reorder(Category, -n), y = n, fill = Category)) + geom_col() + geom_text(aes(label = n), vjust = -0.25) + labs(title = &quot;No. of Laureates in Different Disciplines&quot;, x = &quot;Category&quot;, y = &quot;N&quot;) + theme(legend.position = &quot;none&quot;) An even more dynamic graph: ## barplot (dynamic) ## Uncomment if you have not installed this library # install.packages(&quot;gganimate&quot;, dependencies = T) library(gganimate) my_df %&gt;% count(Category) %&gt;% mutate(Category = fct_reorder(Category, -n)) %&gt;% ggplot(aes(x = Category, y = n, fill = Category)) + geom_text(aes(label = n), vjust = -0.25) + geom_col()+ labs(title = &quot;No. of Laureates in Different Disciplines&quot;, x = &quot;Category&quot;, y = &quot;N&quot;) + theme(legend.position = &quot;none&quot;) + transition_states(Category) + shadow_mark(past = TRUE) 12.6.2 Age Distribution RQ: At what age did laureates normally win their Nobel Prizes? ## statistics summary(nobel_winners$Prize_Age) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 17.00 50.00 60.00 59.45 69.00 90.00 30 psych::describe(nobel_winners$Prize_Age) %&gt;% t X1 vars 1.00000000 n 881.00000000 mean 59.45175936 sd 12.41297890 median 60.00000000 trimmed 59.50496454 mad 13.34340000 min 17.00000000 max 90.00000000 range 73.00000000 skew -0.04907038 kurtosis -0.44875709 se 0.41820389 ## histogram nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Prize_Age)) + geom_histogram(color=&quot;white&quot;) ## boxplot nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(y = Prize_Age)) + geom_boxplot() 12.6.3 Category x Age Interaction RQ: Is there variation in the age of prize winners across different prize categories? ## Statistics nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% group_by(Category) %&gt;% summarize(mean_age = mean(Prize_Age), sd_age = sd(Prize_Age)) %&gt;% ungroup %&gt;% arrange(mean_age) ## Histogram nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Prize_Age, fill = Category, color = Category)) + geom_histogram(color=&quot;white&quot;) + facet_wrap(~Category) + theme(legend.position = &quot;none&quot;) ## Density graphs nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Prize_Age, fill = Category, color = Category)) + geom_density() + facet_wrap(~Category) + theme(legend.position = &quot;none&quot;) ## boxplot nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Category, y = Prize_Age, fill = Category))+ geom_boxplot(notch=T) ## mean and CI plots nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(Category, Prize_Age, fill = Category)) + stat_summary(fun = mean, geom = &quot;bar&quot;, fill=&quot;white&quot;, color=&quot;black&quot;) + stat_summary(fun.data = function(x) mean_se(x, mult = 1.96), geom = &quot;errorbar&quot;, width = 0.1, color=&quot;grey40&quot;) geom_density_ridges() is a visualization function from the ggridges package in R. It allows you to create ridgeline plots, also known as density ridgeline plots or joyplots. These plots display the density distribution of a variable across different categories or groups. The function geom_density_ridges() takes your data and maps a continuous variable to the y-axis to represent the density. The x-axis represents the categories or groups you want to compare. Each category is represented by a ridgeline, and the density of the variable is visualized as the height or thickness of the ridgeline. This visualization is useful for comparing the distributions of a continuous variable across different groups, as it provides a clear and compact representation of the density curves for each group. It allows you to identify variations in the density and explore patterns or differences between groups in a visually appealing manner. library(ggridges) nobel_winners %&gt;% filter(!is.na(Prize_Age)) %&gt;% ggplot(aes(x = Prize_Age, y = Category, fill = Category)) + geom_density_ridges() 12.6.4 Gender Distribution RQ: What is the gender distribution of Nobel winners? ## statistics nobel_winners %&gt;% filter(!is.na(Sex)) %&gt;% count(Sex) %&gt;% mutate(percent = round(n/sum(n),2)) ## graphs nobel_winners %&gt;% filter(!is.na(Sex)) %&gt;% ggplot(aes(Sex)) + geom_bar(fill=&quot;white&quot;,color=&quot;black&quot;) 12.6.5 Age x Gender Interaction RQ: Did the ages differ greatly for male and female winners? ## statistics nobel_winners %&gt;% filter(!is.na(Sex) &amp; !is.na(Prize_Age)) %&gt;% group_by(Sex) %&gt;% summarize(mean_prize_age = mean(Prize_Age), sd_prize_age = sd(Prize_Age), min_prize_age = min(Prize_Age), max_prize_age = max(Prize_Age), N = n()) -&gt; sum_sex_age ## Check sum_sex_age As for visualization, we can create boxplots for male and female winners. ## boxplot nobel_winners %&gt;% filter(!is.na(Sex) &amp; !is.na(Prize_Age)) %&gt;% ggplot(aes(Sex, Prize_Age, fill=Sex)) + geom_boxplot(notch=T, color=&quot;grey30&quot;) + scale_fill_manual(values = c(&quot;lightpink&quot;,&quot;lightblue&quot;)) Or alternatively, we can create the mean and confidence interval plots for winner’s age distribution for each gender. ## Mean and CI Plot ## you have to have this for `stat_summary()` # require(Hmisc) nobel_winners %&gt;% filter(!is.na(Sex) &amp; !is.na(Prize_Age)) %&gt;% ggplot(aes(Sex, Prize_Age, color=Sex)) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 2) + stat_summary(fun.data = function(x) mean_se(x, mult=1.96), geom = &quot;errorbar&quot;, width = 0.1) We can create an informative graph showing not only the mean ages of male and female winners, but also their respective minimum and maximum ages. ## Barplot sum_sex_age %&gt;% pivot_longer(cols = c(&quot;mean_prize_age&quot;,&quot;min_prize_age&quot;, &quot;max_prize_age&quot;), names_to = &quot;Prize_Age&quot;, values_to = &quot;Age&quot;) %&gt;% mutate(Prize_Age = str_replace_all(Prize_Age, &quot;_prize_age$&quot;,&quot;&quot;)) %&gt;% ggplot(aes(Sex, Age, fill=Prize_Age)) + geom_bar(stat=&quot;identity&quot;, width = 0.8, color=&quot;white&quot;, position = position_dodge(0.8)) 12.6.6 State Distribution RQ: What is the distribution of Nobel Prize winners’ states for American laureates? We can explore the state distribution of the US Nobel Winners. This would give us an idea which state in the United States has the most Nobel winners. To start with, for US winners, we can extract their birth states from their birth cities (using regular expressions of course): ## Explore the data nobel_winners %&gt;% filter(Birth_Country == &quot;united states of america&quot;) %&gt;% select(Year, Category, Full_Name, Birth_City) Now we can utilize regular expression to extract the state from the birth cities: nobel_winners %&gt;% filter(Birth_Country == &quot;united states of america&quot;) %&gt;% mutate(Birth_State = str_replace(Birth_City, &quot;([^,]+), ([a-z]+)$&quot;, &quot;\\\\2&quot;)) %&gt;% group_by(Birth_State) %&gt;% summarize(N = n()) %&gt;% arrange(desc(N), Birth_State) -&gt; sum_state sum_state And then we can plot the US winner’s state distribution: sum_state %&gt;% mutate(Birth_State = fct_reorder(str_to_upper(Birth_State), N)) %&gt;% ggplot(aes(Birth_State, N, fill = N)) + geom_col() + coord_flip() + scale_fill_viridis_c(guide=F) + labs(y = &quot;Number of Winners&quot;, x = &quot;Winner Birth States&quot;) In the above bar plot, we can see that there is one false positive (e.g., “JAMAICA PLAIN, MA (BOSTON)”) in the matches of the regular expression. How can we adjust the regular expression to fix this issue? In the demo-data/US-states-csv, you can find a csv with the mapping between states abbreviations and their full names. We can include the full names of states in the above result by adding another column. The US-states.csv dataset ## Loading mapping table US_states &lt;- read_csv(&quot;demo_data/US-states.csv&quot;) US_states ## Joining tables sum_state %&gt;% mutate(Birth_State = str_to_upper(Birth_State)) %&gt;% left_join(US_states, by = c(&quot;Birth_State&quot; = &quot;Code&quot;)) 12.7 Exercises Exercise 12.1 Create a subset of nobel_winners, which includes only winners who won the prizes more than once and in more than one category. Exercise 12.2 Please create a data frame of summary statistics, which shows us the distributions of male and female winners in different categories as shown below. Also, please show the number of males and females as well as the proportions for each prize category. (i.e., frequencies and normalized frequencies) Exercise 12.3 Create a line plot that illustrates the average ages of prize winners for different categories over different decades. "],["text-analytics-a-start.html", "Chapter 13 Text Analytics: A Start 13.1 Installing quanteda 13.2 Building a corpus from character vector 13.3 Tokenization 13.4 Keyword-in-Context (KWIC) 13.5 KWIC with Regular Expressions 13.6 Lexical Density Plot 13.7 Document-Feature Matrix 13.8 Feature Selection 13.9 Top Features 13.10 Wordclouds 13.11 Keyness Analysis 13.12 Flowchart 13.13 Exercises", " Chapter 13 Text Analytics: A Start In this chapter, I will present a quick overview of computational text analytics with R. The most important package for exploratory text analysis is quanteda. As computational text analytics itself is an interesting topic, I would recommend other more advanced courses for those who are interested in this field (e.g., Corpus Linguistics, Computational Linguistics) This chapter only provides you a very general overview of the common procedures/tasks in text processing. 13.1 Installing quanteda There are many packages that are made for computational text analytics in R. You may consult the CRAN Task View: Natural Language Processing for a lot more alternatives. To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. The libraryquanteda is not included in the default base R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you need to install the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. The library quanteda contains all of the core natural language processing and textual data management functions.The following libraries work with quanteda, providing more advanced functions for computational text analytics. quanteda.textmodels: includes the text models and supporting functions (i.e., textmodel_*() functions). quanteda.textstats: includes statistic processing for textual data (i.e., textstat_*() functions). quanteda.textplots: includes plotting functions for textual data (i.e., textplot_*() functions). library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(tidyverse) packageVersion(&quot;quanteda&quot;) [1] &#39;3.3.1&#39; 13.2 Building a corpus from character vector To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US Presidential Inaugural Address texts and metadata for the corpus from 1789 to present. data_corpus_inaugural Corpus consisting of 59 documents and 4 docvars. 1789-Washington : &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot; 1793-Washington : &quot;Fellow citizens, I am again called upon by the voice of my c...&quot; 1797-Adams : &quot;When it was first perceived, in early times, that no middle ...&quot; 1801-Jefferson : &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot; 1805-Jefferson : &quot;Proceeding, fellow citizens, to that qualification which the...&quot; 1809-Madison : &quot;Unwilling to depart from examples of the most revered author...&quot; [ reached max_ndoc ... 53 more documents ] class(data_corpus_inaugural) [1] &quot;corpus&quot; &quot;character&quot; We create a corpus() object with the pre-loaded corpus in quanteda– data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) # save the `corpus` to a short obj name summary(corp_us) After the corpus is loaded, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addresses made by all presidents. In quanteda, it has implemented a default tokenization method for English texts. I think it has also implemented a default tokenization method for Chinese texts as well. For more control on the word segmentation, you may need to consult other segmentation packages (e.g., jiebaR, ckiptagger etc.). More details are usually discussed in ENC2045 or ENC2036. corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() Exercise 13.1 Could you reproduce the above line plot and add information of President to the plot as labels of the dots? Hints: Please check ggplot2::geom_text() or more advanced one, ggrepel::geom_text_repel() 13.3 Tokenization The first step for most textual analyses is usually tokenization, i.e., breaking each long text into word tokens for linguistic analysis. corp_us_tokens &lt;- tokens(corp_us) corp_us_tokens[1] Tokens consisting of 1 document and 4 docvars. 1789-Washington : [1] &quot;Fellow-Citizens&quot; &quot;of&quot; &quot;the&quot; &quot;Senate&quot; [5] &quot;and&quot; &quot;of&quot; &quot;the&quot; &quot;House&quot; [9] &quot;of&quot; &quot;Representatives&quot; &quot;:&quot; &quot;Among&quot; [ ... and 1,525 more ] 13.4 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or concordances, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word (or any other linguistic unit) by examining how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us_tokens, &quot;terror&quot;) kwic() returns a data frame, which can be easily exported as a CSV file for later use. Please note that kwic(), when taking a tokens object as the input argument. That is, please tokenize the corpus object with tokens() first before you perform more advanced textual analysis, e.g., kwic(). Also, with kwic(), the pattern you look for cannot be a multi-word linguistic pattern. For Chinese, quanteda can take care of Chinese word segmentation but with rather limited capacity. texts &lt;- c(&quot;舞台正中間擺著一張「空著的導演椅」，影片一下全場鼻酸。&quot;, &quot;第58屆金馬獎頒獎典禮今（27）日在國父紀念館盛大登場，星光大道紅毯於下午5點30分登場。&quot;) corpus_ch &lt;- corpus(texts) corpus_ch Corpus consisting of 2 documents. text1 : &quot;舞台正中間擺著一張「空著的導演椅」，影片一下全場鼻酸。&quot; text2 : &quot;第58屆金馬獎頒獎典禮今（27）日在國父紀念館盛大登場，星光大道紅毯於下午5點30分登場。&quot; corpus_ch_tokens &lt;- tokens(corpus_ch) corpus_ch_tokens[[1]] [1] &quot;舞台&quot; &quot;正&quot; &quot;中間&quot; &quot;擺著&quot; &quot;一張&quot; &quot;「&quot; &quot;空&quot; &quot;著&quot; &quot;的&quot; &quot;導演&quot; [11] &quot;椅&quot; &quot;」&quot; &quot;，&quot; &quot;影片&quot; &quot;一下&quot; &quot;全&quot; &quot;場&quot; &quot;鼻酸&quot; &quot;。&quot; corpus_ch_tokens[[2]] [1] &quot;第&quot; &quot;58&quot; &quot;屆&quot; &quot;金馬獎&quot; &quot;頒獎典禮&quot; [6] &quot;今&quot; &quot;（&quot; &quot;27&quot; &quot;）&quot; &quot;日&quot; [11] &quot;在&quot; &quot;國父紀念館&quot; &quot;盛大&quot; &quot;登場&quot; &quot;，&quot; [16] &quot;星光&quot; &quot;大道&quot; &quot;紅&quot; &quot;毯&quot; &quot;於&quot; [21] &quot;下午&quot; &quot;5&quot; &quot;點&quot; &quot;30&quot; &quot;分&quot; [26] &quot;登場&quot; &quot;。&quot; 13.5 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. corp_us_tokens &lt;- tokens(corp_us) kwic(corp_us_tokens, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us_tokens, phrase(&quot;our country&quot;)) kwic(corp_us_tokens, phrase(&quot;[a-zA-Z]+ against&quot;), valuetype=&quot;regex&quot;) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. Exercise 13.2 Please create a bar plot, showing the number of uses of the word country in each president’s address. Please include different variants of the word, e.g., countries, Countries, Country, in your kwic() search. 13.6 Lexical Density Plot Plotting a kwic object produces a lexical dispersion plot, which allows us to visualize the occurrences of particular terms throughout the texts. corp_us_tokens %&gt;% tokens_subset(Year &gt; 1949) %&gt;% kwic(pattern= &quot;american&quot;) %&gt;% textplot_xray() corp_us_subset &lt;- corp_us_tokens %&gt;% tokens_subset(Year &gt; 1949) textplot_xray( kwic(corp_us_subset, pattern = &quot;american&quot;), kwic(corp_us_subset, pattern = &quot;people&quot;) ) 13.7 Document-Feature Matrix Another important object class is defined in quanteda: the dfm. It stands for Document-Feature-Matrix. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterize the documents. The cells in the matrix often refer to the co-occurrence statistics between each document and the feature. Usually, we first create the tokens version of the corpus (using tokens()) and then use dfm() to create the dfm version of the corpus from tokens. That is, it is recommened to use the tokens as the input for dfm. corp_us_dfm &lt;- corp_us_tokens %&gt;% dfm class(corp_us_dfm) [1] &quot;dfm&quot; attr(,&quot;package&quot;) [1] &quot;quanteda&quot; corp_us_dfm Document-feature matrix of: 59 documents, 9,441 features (91.84% sparse) and 4 docvars. features docs fellow-citizens of the senate and house representatives : 1789-Washington 1 71 116 1 48 2 2 1 1793-Washington 0 11 13 0 2 0 0 1 1797-Adams 3 140 163 1 130 0 2 0 1801-Jefferson 2 104 130 0 81 0 0 1 1805-Jefferson 0 101 143 0 93 0 0 0 1809-Madison 1 69 104 0 43 0 0 0 features docs among vicissitudes 1789-Washington 1 1 1793-Washington 0 0 1797-Adams 4 0 1801-Jefferson 1 0 1805-Jefferson 7 0 1809-Madison 0 0 [ reached max_ndoc ... 53 more documents, reached max_nfeat ... 9,431 more features ] We can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and 13.8 Feature Selection A dfm may not be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered with respect to the features of the dfm: The granularity of the features The informativeness of the features The distributional properties of the features Not only does dfm() provide many arguments for users to specify conditions for features selection; in quanteda, we can also apply dfm_trim() to select important features for later analysis. corp_dfm_trimmed &lt;- corp_us %&gt;% tokens( remove_punct = T, remove_numbers= T, remove_symbols = T) %&gt;% dfm %&gt;% dfm_remove(stopwords(&quot;en&quot;)) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us)-1, docfreq_type = &quot;count&quot;) dim(corp_us_dfm) [1] 59 9441 dim(corp_dfm_trimmed) [1] 59 1401 13.9 Top Features With a dfm, we can check important features from the corpus. topfeatures(corp_dfm_trimmed,10) people government us can must upon great 584 564 505 487 376 371 344 may states world 343 334 319 13.10 Wordclouds With a dfm, we can visualize important words in the corpus with a Word Cloud. It is a novel but intuitive visual representation of text data. It allows us to quickly perceive the most prominent words from a large collection of texts. corp_dfm_trimmed %&gt;% textplot_wordcloud(min_count = 50, random_order = FALSE, rotation = .25, color = RColorBrewer::brewer.pal(8, &quot;Dark2&quot;)) We can also compare word clouds for different subsets of the corpus: corpus_subset(corp_us, President %in% c(&quot;Obama&quot;, &quot;Trump&quot;, &quot;Clinton&quot;)) %&gt;% tokens(remove_punct = T, remove_numbers= T, remove_symbols = T) %&gt;% tokens_group(groups = President) %&gt;% dfm() %&gt;% dfm_remove(stopwords(&quot;en&quot;)) %&gt;% dfm_trim(min_termfreq = 5, termfreq_type = &quot;count&quot;) %&gt;% textplot_wordcloud(comparison = TRUE) 13.11 Keyness Analysis When you have two collections of texts, we can use quantitative methods to identify which words are more strongly associated with one of the two sub-corpora. This is the idea of keyword analysis. 13.12 Flowchart Finally, Figure 13.1 below provides a summary flowchart for computatutional text analytics in R. Figure 13.1: Computational Text Processing Flowchart in R 13.13 Exercises In the following exercise, please use the dataset demo_data/TW_President.tar.gz, which is a text collection of the inaugural speeches of Taiwan Presidents (till 2016). You may load the entire text collection as a corpus object using the following code: require(readtext) require(quanteda) corp_tw &lt;- readtext(&quot;demo_data/TW_President.tar.gz&quot;) %&gt;% corpus Exercise 13.3 Please create a data frame, which includes the metadata information for each text. You may start with a data frame you get from summary(corp_tw) and then create two additional columns—President and Year, which can be extracted from the text filenames in the Text column. Hint: tidyr::extract() summary(corp_tw) %&gt;% as_tibble After you create the metadata DF, please assign it to the docvars(corp_tw) for later analysis. Exercise 13.4 Please create a lexical density plot for the use of “台灣” in all presidents’ texts. Exercise 13.5 Please create a word cloud of the entire corpus corp_tw. In the word cloud, please remove punctuations, numbers, and symbols. The word cloud has to only include words whose frequency &gt;= 20. Exercise 13.6 Create word clouds showing the comparison of President Tsai (CAYANGWEN), Ma (MAYANGJIU), and Shuibian Chen (CHENSHUIBIAN). Exercise 13.7 Please create a keyness plot, showing the preferred words used by President Tsai (CAYANGWEN) vs. President Ma (MAYANGJIU). "],["python-fundamentals.html", "Chapter 14 Python Fundamentals 14.1 Set up Environment 14.2 Conda Environment 14.3 Data Type 14.4 Data Structure 14.5 String 14.6 Control Structure 14.7 Function 14.8 List Comprehension 14.9 Python Scripts 14.10 Modules 14.11 Input/Output", " Chapter 14 Python Fundamentals This unit covers Python fundamentals. All the codes here are Python codes. 14.1 Set up Environment Install Anaconda Install Jupyter Notebook/Lab (See Jupyter Notebook installation documentation) Run the codes in notebooks 14.2 Conda Environment We can create a conda environment using: $ conda create --name XXX Please specify the conda environment name XXX on your own. Similarly, when you add the self-defined conda environment to the notebook kernel list: $ python -m ipykernel install --user --name=XXX You need to specify the conda environment name XXX. There are several important things here: You need to install the relevant modules AFTER you activate the conda environment in the terminal. You need to add the kernel name with python -m ipykernel install --user --name=XXX within the conda enviroment as well. In other words, you need to install the module ipykernel in the target conda environment as well. After a few trial-and-errors, I think the best environment setting is that you only add the kernel name (conda environment) to ipykernel within the conda environment. Do not add the conda environment again in your base python environment. What’s even better is to install jupyter in your conda environment (python-notes) and run your notebook from this python-notes as well. 14.3 Data Type String Numbers (Integers and Floats) Data Type Conversion Input a = &#39;cats&#39; b = &#39;dogs&#39; print(a + b) catsdogs a = 12 b = 13 print(a+b) 25 14.4 Data Structure List Tuple Dictionary vocab = [&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;] word_pos = ((&#39;cat&#39;,&#39;n&#39;),(&#39;dog&#39;,&#39;n&#39;),(&#39;bark&#39;,&#39;v&#39;)) word_freq = {&#39;cat&#39;: 3, &#39;dog&#39;: 1, &#39;bird&#39;: 5} print(vocab) [&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;] print(type(vocab)) &lt;class &#39;list&#39;&gt; print(type(word_pos)) &lt;class &#39;tuple&#39;&gt; print(type(word_freq)) &lt;class &#39;dict&#39;&gt; List and Tuple look similar but they differ in one important aspect: List is mutable while Tuple is Immutable. That is, when a List is created, particular elements of it can be reassigned. Along with this, the entire list can be reassigned. Elements and slices of elements can be deleted from the list. But these changes will not be possible for a Tuple. 14.5 String vocab1 = [&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;] vocab2 = (&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;) vocab1[0] = &#39;human&#39; print(vocab1) [&#39;human&#39;, &#39;dog&#39;, &#39;bird&#39;] vocab2[0] = &#39;human&#39; &#39;tuple&#39; object does not support item assignment w = &#39;wonderful&#39; type(w) &lt;class &#39;str&#39;&gt; w[:3] &#39;won&#39; In Python, a String functions as a List: &#39;o&#39; in w True w2 = &#39;book&#39; w + w2 &#39;wonderfulbook&#39; &#39; &#39;.join(w) &#39;w o n d e r f u l&#39; Useful functions for String: sent = &#39; This is a sentence example with leading/trailing spaces. &#39; sent.capitalize() &#39; this is a sentence example with leading/trailing spaces. &#39; sent.title() &#39; This Is A Sentence Example With Leading/Trailing Spaces. &#39; sent.upper() &#39; THIS IS A SENTENCE EXAMPLE WITH LEADING/TRAILING SPACES. &#39; sent.lower() &#39; this is a sentence example with leading/trailing spaces. &#39; sent.rstrip() &#39; This is a sentence example with leading/trailing spaces.&#39; sent.lstrip() &#39;This is a sentence example with leading/trailing spaces. &#39; sent.strip() &#39;This is a sentence example with leading/trailing spaces.&#39; sent.find(&#39;is&#39;) 5 sent.replace(&#39;is&#39;,&#39;was&#39;) &#39; Thwas was a sentence example with leading/trailing spaces. &#39; String formatting nwords = 50 textid = &#39;diary&#39; &#39;%s has %d words&#39; % (textid.upper(), nwords) &#39;DIARY has 50 words&#39; 14.6 Control Structure Iteration (for-loop) for w in vocab: print(&quot;_&quot; + w + &quot;_&quot;) _cat_ _dog_ _bird_ Condition (if-else) for w in vocab: if(w[0]==&quot;c&quot;): print(w) cat 14.7 Function def greet(name): print(&#39;Hello, &#39; + name + &#39;, how are you doing!&#39;) greet(name=&#39;Alvin&#39;) Hello, Alvin, how are you doing! greet(name=&#39;Charlie&#39;) Hello, Charlie, how are you doing! 14.8 List Comprehension [len(w) for w in vocab] [3, 3, 4] 14.9 Python Scripts Depending on the editor you use, you may have two types of Python script files: *.py: A simple python script file *.ipynb: A Jupyter Notebook file which needs to be run in Jupyter Lab/Notebook 14.10 Modules $ pip install PACKAGE_NAME Import packages/libraries import re import os 14.11 Input/Output with open(&#39;temp.txt&#39;, &#39;w&#39;) as f: f.write(&#39;hello world!\\n&#39;+&#39;This is a sentence.&#39;) 32 with open(&#39;temp.txt&#39;, &#39;r&#39;) as f: texts = [l for l in f.readlines()] print(texts) [&#39;hello world!\\n&#39;, &#39;This is a sentence.&#39;] rm temp.txt File and Directory Operation import os os.getcwdb() b&#39;/Users/alvinchen/Library/CloudStorage/Dropbox/NTNU/Programming_Linguistics/Programming_Linguistics_bookdown&#39; os.unlink() os.rename() os.chdir() os.listdir() os.getwd() os.mkdir() os.rmdir os.path.exists() os.path.isfile() os.path.isdir() "],["organizing-files.html", "Chapter 15 Organizing Files 15.1 shutil", " Chapter 15 Organizing Files This unit shows how we can utilize Python to organize files on the hard drive, e.g., traversing the directory, copying, renaming, moving, or compressing files automatically. 15.1 shutil The shutil (shell utility) module helps us copy, move, rename, and delete files in Python. To copy file and directory: shutil.copy(): to copy a single file shutil.copytree(): to copy an entire folder and every folder and file contained in it To remove file and directory: os.unlink(): to delete the file os.rmdir(): to delete an empty folder shutil.rmtree(): to delete a non-empty folder and all files and folders it contains To move file and directory: shutil.move(): to move the file or folder at the path source to the path destination All the shutil file/directory operation functions will return a string of absolute path of the new files/directories locations. import shutil Ways to get the path root or the current working directory: from pathlib import Path print(Path.home()) /Users/alvinchen import os print(os.getcwd()) /Users/alvinchen/Library/CloudStorage/Dropbox/NTNU/Programming_Linguistics/Programming_Linguistics_bookdown t = shutil.copy(&#39;demo_data/corp-alice.txt&#39;, Path.home()) os.unlink(t) # clean up t = shutil.copytree(&#39;demo_data&#39;, Path.home()/&#39;demo_data&#39;) t PosixPath(&#39;/Users/alvinchen/demo_data&#39;) os.rmdir(t) [Errno 66] Directory not empty: &#39;/Users/alvinchen/demo_data&#39; #shutil.rmtree(t) Be very careful when using these “removing” functions. It is often a good idea to run these data-removing functions with these calls commented out and with print() calls added to double check the file/directory names to be deleted. for f in Path(Path.home()/&#39;demo_data&#39;).glob(&#39;*.txt&#39;): print(f) /Users/alvinchen/demo_data/data-chinese-poem-big5.txt /Users/alvinchen/demo_data/corp-alice.txt /Users/alvinchen/demo_data/chinese_big5.txt /Users/alvinchen/demo_data/data-chinese-poem-utf8.txt /Users/alvinchen/demo_data/dict-ch-idiom.txt /Users/alvinchen/demo_data/chinese_utf8.txt /Users/alvinchen/demo_data/data-sentences.txt /Users/alvinchen/demo_data/chinese_gb2312.txt /Users/alvinchen/demo_data/data-chinese-poem-gb2312.txt # os.unlink(f) ## clean up the earlier copied folder shutil.rmtree(Path(Path.home()/&#39;demo_data&#39;)) Because the data-removing functions in shutil irreversibly delete files and folders, they can be dangerous to use. Another third-party module, send2trash, can be much safer because it will send files and folders to the computer’s trash or recycle bin instead of permanently deleting them. For beginners, this can be very helpful for life-saving files/folders. Exercise 15.1 Combine the first page of each PDF in a directory into one new PDF. "],["object-orientation.html", "Chapter 16 Object Orientation 16.1 What is Object Orientation? 16.2 Class Defition 16.3 Types of Methods 16.4 Class Inheritance 16.5 Special Methods 16.6 Property Decorator 16.7 Checking Functions 16.8 Name Mangling 16.9 Reference", " Chapter 16 Object Orientation 16.1 What is Object Orientation? An effective program often utilizes an object-oriented approach in its design. Therefore, most coding languages evolve as an object-oriented language. In OO design, systems consist of collaborating sets of well-defined objects. One object sends a message to another object to achieve some goal. The message is implemented as a function call, often referred to as a method. Therefore, methods are associated with specific objects and depend on the nature of the object as well. ## Examples of String object&#39;s methods newstring = text.upper() newstring = text.lower() newstring = text.split() An object’s blueprint determines its structure. This blueprint is referred to as the class definition. When we create a new object from the object’s class definition, the object is an instance of the class and the process is called instantiation. In addition to methods, an object can also have attributes or properties that we can get and set. We can also determine whether to hide these attributes and make them available only through the methods (private attributes) or expose them to the outside world (public attributes). 16.2 Class Defition A Class definition is a template for a new object instance. While there are many pre-defined classes available in default Python modules, we can define our own class depending on the tasks at hand. In a class definition, usually we include: the mechanism and data required to create an new instance of the class (constructor method(s)), its attributes (both private and public attributes), the methods that can be used to set and get attributes or change the state ob the object (class methods). # Define a class class Employee(object): &quot;Employee Class&quot; # Constructor method def __init__(self, first, last, pay): self.first = first self.last = last self.email = first + &#39;.&#39; + last + &#39;@email.com&#39; self.pay = pay # instance method def fullname(self): return &#39;{} {}&#39;.format(self.first, self.last) emp_1 = Employee(&#39;Alvin&#39;, &#39;Chen&#39;, 50000) emp_2 = Employee(&#39;John&#39;, &#39;Doe&#39;, 60000) emp_1.fullname() &#39;Alvin Chen&#39; emp_2.fullname() &#39;John Doe&#39; 16.3 Types of Methods Class Method: a method bound to the class and with cls as the first default argument Static Method: a self-contained method bound to the class Instance Method: a method bound to the object instance of the class with self as the first default argument class Employee(object): num_of_emps = 0 raise_amt = 1.04 def __init__(self, first, last, pay): self.first = first self.last = last self.email = first + &#39;.&#39; + last + &#39;@email.com&#39; self.pay = pay Employee.num_of_emps += 1 def fullname(self): return &#39;{} {}&#39;.format(self.first, self.last) def apply_raise(self): self.pay = int(self.pay * self.raise_amt) @classmethod def set_raise_amt(cls, amount): cls.raise_amt = amount @classmethod def from_string(cls, emp_str): first, last, pay = emp_str.split(&#39;-&#39;) return cls(first, last, pay) @staticmethod def is_workday(day): if day.weekday() == 5 or day.weekday() == 6: return False return True emp_1 = Employee(&#39;Corey&#39;, &#39;Schafer&#39;, 50000) emp_2 = Employee(&#39;Test&#39;, &#39;Employee&#39;, 60000) Employee.set_raise_amt(1.05) print(Employee.raise_amt) 1.05 print(emp_1.raise_amt) 1.05 print(emp_2.raise_amt) 1.05 emp_str_1 = &#39;John-Doe-70000&#39; emp_str_2 = &#39;Steve-Smith-30000&#39; emp_str_3 = &#39;Jane-Doe-90000&#39; first, last, pay = emp_str_1.split(&#39;-&#39;) #new_emp_1 = Employee(first, last, pay) new_emp_1 = Employee.from_string(emp_str_1) print(new_emp_1.email) John.Doe@email.com print(new_emp_1.pay) 70000 import datetime my_date = datetime.date(2016, 7, 11) print(Employee.is_workday(my_date)) True 16.4 Class Inheritance With the class definition, we can create as many instances of the class as needed. Moreover, with the class definition, we can create suborindate or superordinate classess based on the original class. These sub-classess have taxonomic relationships with the original super class. Extend the original class constructor methods using super() class Employee: raise_amt = 1.04 def __init__(self, first, last, pay): self.first = first self.last = last self.email = first + &#39;.&#39; + last + &#39;@email.com&#39; self.pay = pay def fullname(self): return &#39;{} {}&#39;.format(self.first, self.last) def apply_raise(self): self.pay = int(self.pay * self.raise_amt) class Developer(Employee): raise_amt = 1.10 def __init__(self, first, last, pay, prog_lang): super().__init__(first, last, pay) self.prog_lang = prog_lang class Manager(Employee): def __init__(self, first, last, pay, employees=None): super().__init__(first, last, pay) if employees is None: self.employees = [] else: self.employees = employees def add_emp(self, emp): if emp not in self.employees: self.employees.append(emp) def remove_emp(self, emp): if emp in self.employees: self.employees.remove(emp) def print_emps(self): for emp in self.employees: print(&#39;--&gt;&#39;, emp.fullname()) dev_1 = Developer(&#39;Alvin&#39;, &#39;Chen&#39;, 50000, &#39;Python&#39;) dev_2 = Developer(&#39;John&#39;, &#39;Doe&#39;, 60000, &#39;R&#39;) mgr_1 = Manager(&#39;Anne&#39;, &#39;Mary&#39;, 90000, [dev_1]) print(mgr_1.email) Anne.Mary@email.com mgr_1.add_emp(dev_2) mgr_1.remove_emp(dev_2) mgr_1.print_emps() --&gt; Alvin Chen 16.5 Special Methods Dunder methods (double-underscore) To avoid overloading the expressions in coding class Employee: raise_amt = 1.04 def __init__(self, first, last, pay): self.first = first self.last = last self.email = first + &#39;.&#39; + last + &#39;@email.com&#39; self.pay = pay def fullname(self): return &#39;{} {}&#39;.format(self.first, self.last) def apply_raise(self): self.pay = int(self.pay * self.raise_amt) def __repr__(self): return &quot;Employee(&#39;{}&#39;, &#39;{}&#39;, {})&quot;.format(self.first, self.last, self.pay) def __str__(self): return &#39;{} - {}&#39;.format(self.fullname(), self.email) def __add__(self, other): return self.pay + other.pay def __len__(self): return len(self.fullname()) emp_1 = Employee(&#39;Corey&#39;, &#39;Schafer&#39;, 50000) emp_2 = Employee(&#39;Test&#39;, &#39;Employee&#39;, 60000) # print(emp_1 + emp_2) print(len(emp_1)) 13 16.6 Property Decorator @property: make a method function like attribute-accessing @NAME.setter: make a method function like class-attribute assigning @NAME.deleter: make a method function like class-attribute deleting class Employee: def __init__(self, first, last): self.first = first self.last = last @property def email(self): return &#39;{}.{}@email.com&#39;.format(self.first, self.last) @property def fullname(self): return &#39;{} {}&#39;.format(self.first, self.last) @fullname.setter def fullname(self, name): first, last = name.split(&#39; &#39;) self.first = first self.last = last @fullname.deleter def fullname(self): print(&#39;Delete Name!&#39;) self.first = None self.last = None emp_1 = Employee(&#39;John&#39;, &#39;Smith&#39;) emp_1.fullname = &quot;Corey Schafer&quot; print(emp_1.first) Corey print(emp_1.email) Corey.Schafer@email.com print(emp_1.fullname) Corey Schafer del emp_1.fullname Delete Name! 16.7 Checking Functions isinstance(): Check an instance’s type issubclass(): Check class inheritance 16.8 Name Mangling __NAME: Any identifier of this form within the class is textually replaced with _classname__NAME, where classname is the current class name, with leading underscore(s) stripped. This is for the purpose of creating private variables to the class. 16.9 Reference This notebook is based on Corey Schafer’s OOP Tutorial "],["regular-expression.html", "Chapter 17 Regular Expression 17.1 Comparison of R and Python 17.2 Structure of Regular Expression Usage 17.3 Special Falgs/Settings for Regular Expressions 17.4 Regular Expression in Python 17.5 Text Munging 17.6 References", " Chapter 17 Regular Expression 17.1 Comparison of R and Python R Python str_extract() re.search() str_extract_all() re.findall() str_match_all() re.finditer() str_replace_all() re.sub() str_split() re.split() ? re.subn() ? re.match() str_detect() ? str_subset() ? The above table shows the similarities and differences in terms of the regular expression functions in Python and R. They are more or less similar. These mappings can be helpful for R users to understand the re in Python. 17.2 Structure of Regular Expression Usage 17.2.1 re.search() Import the regex module with import re Create a Regex object by compiling a regular expression pattern (re.compile()). Remember to use a raw string. Use the pattern for search (re.search()) by passing the string you want to search into the Regex object’s search method. This returns a Match object. Call the Match object’s group() method to return a string of the actual matched text. import re text_to_search = &#39;&#39;&#39; abcdefghijklmnopqurtuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ 1234567890 Ha HaHa MetaCharacters (Need to be escaped): . ^ $ * + ? { } [ ] \\ | ( ) coreyms.com 321-555-4321 123.555.1234 123*555*1234 800-555-1234 900-555-1234 Mr. Schafer Mr Smith Ms Davis Mrs. Robinson Mr. T &#39;&#39;&#39; sentence = &#39;Start a sentence and then bring it to an end&#39; pattern = re.compile(r&#39;\\d{3}-\\d{3}-\\d{4}&#39;, re.I) 17.2.2 re.findall() While search() will return a Match object of the first matched text in the searched string, the findall() method will return the strings of every match in the searched string. (re.findall() will not return a Match object but a list of strings–*as long as there are no groups in the regular expression.) ## perform a search matches= pattern.findall(text_to_search) matches [&#39;321-555-4321&#39;, &#39;800-555-1234&#39;, &#39;900-555-1234&#39;] If there are groups in the regular expressions, then re.findall() will return a list of tuples. pattern2 = re.compile(r&#39;(\\d{3})-(\\d{3})-(\\d{4})&#39;, re.I) pattern2.findall(text_to_search) [(&#39;321&#39;, &#39;555&#39;, &#39;4321&#39;), (&#39;800&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;900&#39;, &#39;555&#39;, &#39;1234&#39;)] 17.2.3 re.finditer() ## find all matches matches = pattern.finditer(text_to_search) if matches: for m in matches: print(&quot;%02d-%02d: %s&quot; % (m.start(), m.end(), m.group())) 151-163: 321-555-4321 190-202: 800-555-1234 203-215: 900-555-1234 17.3 Special Falgs/Settings for Regular Expressions re.IGNORECASE: case-insensitive for pattern matching re.DOTALL: to allow the wildcard * to match linebreaks re.VERBOSE: to create complex regular expressions with multilines and comments (#) pattern3 = re.compile(r&#39;&#39;&#39; (\\d{3}) # area code - # delimiter (\\d{3}) # first 3 digits - # delimiter (\\d{4}) # last 4 digits &#39;&#39;&#39;, re.VERBOSE) pattern3.findall(text_to_search) [(&#39;321&#39;, &#39;555&#39;, &#39;4321&#39;), (&#39;800&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;900&#39;, &#39;555&#39;, &#39;1234&#39;)] Exercise 17.1 With the text_to_search, how to create a more complete regular expression to extract all the phone numbers, including those numbers that have other delimiters (e.g., *) [(&#39;321&#39;, &#39;555&#39;, &#39;4321&#39;), (&#39;123&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;123&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;800&#39;, &#39;555&#39;, &#39;1234&#39;), (&#39;900&#39;, &#39;555&#39;, &#39;1234&#39;)] 17.4 Regular Expression in Python 17.4.1 Raw String Notation Raw string notation (r'text') keeps regular expressions sane. Without it, every backslash ('') in a regular expression would have to be prefixed with another one to escape it. For example, the two following lines of code are functionally identical: 17.4.2 Find all matches re.findall(): matches all occurrences of a pattern, not just the first one as re.search() does. re.finditer(): If one wants more information about all matches of a pattern than the matched text, re.finditer() is useful as it provides match objects instead of strings. 17.4.3 group() vs. groups() group(): by default, returns the whole match of the pattern groups(): by default, returns all capturing groups m = re.match(&quot;a(.)(.)&quot;,&quot;abcedf&quot;) print(m.group(0)) # return the whole match abc print(m.group()) # return the whole match, same as above abc print(m.groups()) # return each capturing group match (&#39;b&#39;, &#39;c&#39;) print(m.group(1)) # return first capturing gorup match b 17.4.4 string format validation valid = re.compile(r&quot;^[a-z]+@[a-z]+\\.[a-z]{3}$&quot;) print(valid.match(&#39;alvin@ntnu.edu&#39;)) &lt;re.Match object; span=(0, 14), match=&#39;alvin@ntnu.edu&#39;&gt; print(valid.match(&#39;alvin123@ntnu.edu&#39;)) None print(valid.match(&#39;alvin@ntnu.homeschool&#39;)) None 17.4.5 re.match() vs. re.search() Python offers two different primitive operations based on regular expressions: re.match() checks for a match only at the beginning of the string re.search() checks for a match anywhere in the string (this is what Perl does by default). print(re.match(&quot;c&quot;, &quot;abcdef&quot;)) # No match None print(re.search(&quot;^c&quot;, &quot;abcdef&quot;)) # No match, same as above None print(re.search(&quot;c&quot;, &quot;abcdef&quot;)) # Match &lt;re.Match object; span=(2, 3), match=&#39;c&#39;&gt; re.match always matches at the beginning of the input string even if it is in the MULTILINE mode. re.search however, when in MULTILINE mode, is able to search at the beginning of every line if used in combination with ^. print(re.match(&#39;X&#39;, &#39;A\\nB\\nX&#39;, re.MULTILINE)) # No match None print(re.search(&#39;^X&#39;, &#39;A\\nB\\nX&#39;, re.MULTILINE)) # Match &lt;re.Match object; span=(4, 5), match=&#39;X&#39;&gt; print(re.search(&#39;^X&#39;, &#39;A\\nB\\nX&#39;)) # No match None 17.4.6 re.split() text = &quot;&quot;&quot;Ross McFluff: 834.345.1254 155 Elm Street Ronald Heathmore: 892.345.3428 436 Finley Avenue Frank Burger: 925.541.7625 662 South Dogwood Way Heather Albrecht: 548.326.4584 919 Park Place&quot;&quot;&quot; # split text into lines re.split(r&#39;\\n&#39;,text) [&#39;Ross McFluff: 834.345.1254 155 Elm Street&#39;, &#39;&#39;, &#39;Ronald Heathmore: 892.345.3428 436 Finley Avenue&#39;, &#39;Frank Burger: 925.541.7625 662 South Dogwood Way&#39;, &#39;&#39;, &#39;&#39;, &#39;Heather Albrecht: 548.326.4584 919 Park Place&#39;] re.split(r&#39;\\n+&#39;, text) [&#39;Ross McFluff: 834.345.1254 155 Elm Street&#39;, &#39;Ronald Heathmore: 892.345.3428 436 Finley Avenue&#39;, &#39;Frank Burger: 925.541.7625 662 South Dogwood Way&#39;, &#39;Heather Albrecht: 548.326.4584 919 Park Place&#39;] entries = re.split(r&#39;\\n+&#39;, text) [re.split(r&#39;\\s&#39;, entry) for entry in entries] [[&#39;Ross&#39;, &#39;McFluff:&#39;, &#39;834.345.1254&#39;, &#39;155&#39;, &#39;Elm&#39;, &#39;Street&#39;], [&#39;Ronald&#39;, &#39;Heathmore:&#39;, &#39;892.345.3428&#39;, &#39;436&#39;, &#39;Finley&#39;, &#39;Avenue&#39;], [&#39;Frank&#39;, &#39;Burger:&#39;, &#39;925.541.7625&#39;, &#39;662&#39;, &#39;South&#39;, &#39;Dogwood&#39;, &#39;Way&#39;], [&#39;Heather&#39;, &#39;Albrecht:&#39;, &#39;548.326.4584&#39;, &#39;919&#39;, &#39;Park&#39;, &#39;Place&#39;]] [re.split(r&#39;:?\\s&#39;, entry, maxsplit=3) for entry in entries] [[&#39;Ross&#39;, &#39;McFluff&#39;, &#39;834.345.1254&#39;, &#39;155 Elm Street&#39;], [&#39;Ronald&#39;, &#39;Heathmore&#39;, &#39;892.345.3428&#39;, &#39;436 Finley Avenue&#39;], [&#39;Frank&#39;, &#39;Burger&#39;, &#39;925.541.7625&#39;, &#39;662 South Dogwood Way&#39;], [&#39;Heather&#39;, &#39;Albrecht&#39;, &#39;548.326.4584&#39;, &#39;919 Park Place&#39;]] 17.5 Text Munging re.sub() text = &#39;&#39;&#39;Peter Piper picked a peck of pickled peppers A peck of pickled peppers Peter Piper picked If Peter Piper picked a peck of pickled peppers Where’s the peck of pickled peppers Peter Piper picked?&#39;&#39;&#39; print(re.sub(r&#39;[aeiou]&#39;,&#39;_&#39;, text)) P_t_r P_p_r p_ck_d _ p_ck _f p_ckl_d p_pp_rs A p_ck _f p_ckl_d p_pp_rs P_t_r P_p_r p_ck_d If P_t_r P_p_r p_ck_d _ p_ck _f p_ckl_d p_pp_rs Wh_r_’s th_ p_ck _f p_ckl_d p_pp_rs P_t_r P_p_r p_ck_d? print(re.sub(r&#39;([aeiou])&#39;,r&#39;[\\1]&#39;, text)) P[e]t[e]r P[i]p[e]r p[i]ck[e]d [a] p[e]ck [o]f p[i]ckl[e]d p[e]pp[e]rs A p[e]ck [o]f p[i]ckl[e]d p[e]pp[e]rs P[e]t[e]r P[i]p[e]r p[i]ck[e]d If P[e]t[e]r P[i]p[e]r p[i]ck[e]d [a] p[e]ck [o]f p[i]ckl[e]d p[e]pp[e]rs Wh[e]r[e]’s th[e] p[e]ck [o]f p[i]ckl[e]d p[e]pp[e]rs P[e]t[e]r P[i]p[e]r p[i]ck[e]d? American_dates = [&quot;7/31/1976&quot;, &quot;02.15.1970&quot;, &quot;11-31-1986&quot;, &quot;04/01.2020&quot;] print(American_dates) [&#39;7/31/1976&#39;, &#39;02.15.1970&#39;, &#39;11-31-1986&#39;, &#39;04/01.2020&#39;] print([re.sub(r&#39;(\\d+)(\\D)(\\d+)(\\D)(\\d+)&#39;, r&#39;\\3\\2\\1\\4\\5&#39;, date) for date in American_dates]) [&#39;31/7/1976&#39;, &#39;15.02.1970&#39;, &#39;31-11-1986&#39;, &#39;01/04.2020&#39;] In re.sub(repl, string), the repl argument can be a function. If repl is a function, it is called for every non-overlapping occurrence of pattern. The function takes a single match object argument, and returns the replacement string. s = &quot;This is a simple sentence.&quot; pat_vowels = re.compile(r&#39;[aeiou]&#39;) def replaceVowels(m): c = m.group(0) c2 = &quot;&quot; if c in &quot;ie&quot;: c2 = &quot;F&quot; else: c2 = &quot;B&quot; return c2 pat_vowels.sub(replaceVowels, s) &#39;ThFs Fs B sFmplF sFntFncF.&#39; Exercise 17.2 Create a small program to extract both emails and phone numbers from the texts on this faculty page: Department o English, NTNU. All Phone Numbers: [&#39;02-7749-1801&#39;, &#39;02-7749-1772&#39;, &#39;02-7749-1775&#39;, &#39;02-7749-1783&#39;, &#39;02-7749-1767&#39;, &#39;02-7749-1757&#39;, &#39;02-7749-1781&#39;, &#39;02-7749-1777&#39;, &#39;02-7749-1773&#39;, &#39;02-7749-1759&#39;, &#39;02-7749-1768&#39;, &#39;02-7749-1822&#39;, &#39;02-7749-1760&#39;, &#39;02-7749-1764&#39;, &#39;02-7749-1769&#39;, &#39;02-7749-1817&#39;, &#39;02-7749-1756&#39;, &#39;02-7749-1788&#39;, &#39;02-7749-1758&#39;, &#39;02-7749-1754&#39;, &#39;02-7749-1821&#39;, &#39;02-7749-1819&#39;, &#39;02-7749-1790&#39;, &#39;02-7749-1761&#39;, &#39;02-7749-1761&#39;, &#39;02-7749-1774&#39;, &#39;02-7749-1763&#39;, &#39;02-7749-1776&#39;, &#39;02-7749-1770&#39;, &#39;02-7749-1786&#39;, &#39;02-7749-1816&#39;, &#39;02-7749-1765&#39;, &#39;02-7749-1778&#39;, &#39;02-7749-1541&#39;, &#39;02-7749-1762&#39;, &#39;02-7749-1821&#39;, &#39;02-7749-1811&#39;, &#39;02-7749-1779&#39;, &#39;02-7749-1820&#39;, &#39;02-7749-1766&#39;, &#39;02-7749-1782&#39;, &#39;02-7749-1762&#39;, &#39;02-7749-1800&#39;, &#39;02-2363-4793&#39;] All Emails: [&#39;chunyin@ntnu.edu.tw&#39;, &#39;mhchang@ntnu.edu.tw&#39;, &#39;clchern@ntnu.edu.tw&#39;, &#39;joanchang@ntnu.edu.tw&#39;, &#39;hjchen@ntnu.edu.tw&#39;, &#39;tcsu@ntnu.edu.tw&#39;, &#39;t22028@ntnu.edu.tw&#39;, &#39;lip@ntnu.edu.tw&#39;, &#39;ting@ntnu.edu.tw&#39;, &#39;hclee@ntnu.edu.tw&#39;, &#39;hslin@ntnu.edu.tw&#39;, &#39;chyhuang@ntnu.edu.tw&#39;, &#39;profgood@ntnu.edu.tw&#39;, &#39;cclin@ntnu.edu.tw&#39;, &#39;iriswu@ntnu.edu.tw&#39;, &#39;yeutingliu@ntnu.edu.tw&#39;, &#39;ioana.luca@ntnu.edu.tw&#39;, &#39;lindsey@ntnu.edu.tw&#39;, &#39;jprystash@ntnu.edu.tw&#39;, &#39;hsysu@ntnu.edu.tw&#39;, &#39;hannes.bergthaller@ntnu.edu.tw&#39;, &#39;peichinchang@ntnu.edu.tw&#39;, &#39;shiaohui@ntnu.edu.tw&#39;, &#39;mlhsieh@ntnu.edu.tw&#39;, &#39;lihsin@ntnu.edu.tw&#39;, &#39;lijeni@ntnu.edu.tw&#39;, &#39;ykhsu@ntnu.edu.tw&#39;, &#39;ycshao@ntnu.edu.tw&#39;, &#39;jjwu@ntnu.edu.tw&#39;, &#39;jungsu@ntnu.edu.tw&#39;, &#39;t22001@ntnu.edu.tw&#39;, &#39;jjtseng@ntnu.edu.tw&#39;, &#39;wanghc@ntnu.edu.tw&#39;, &#39;alvinchen@ntnu.edu.tw&#39;, &#39;gfsayang@ntnu.edu.tw&#39;, &#39;yuchentai@ntnu.edu.tw&#39;, &#39;jiaqiwu8@ntnu.edu.tw&#39;, &#39;angelawu@ntnu.edu.tw&#39;, &#39;yichien@ntnu.edu.tw&#39;, &#39;fwkung@ntnu.edu.tw&#39;, &#39;english@ntnu.edu.tw&#39;] 17.6 References Python regular expression cheatsheet Python official regular expression documentation Friedl, Jeffrey. Mastering Regular Expressions. 3rd ed., O’Reilly Media, 2009. A good graphic interface to try out regular expressions: pythex.org "],["pandas.html", "Chapter 18 Pandas 18.1 Libraries 18.2 Importing/Exporting Data 18.3 Inspecting Data Frame 18.4 Basic Functions 18.5 Subsetting Data Frame 18.6 Exploration 18.7 Join/Combine Data Frames 18.8 Statistics 18.9 Generic Functions 18.10 References", " Chapter 18 Pandas Methods to deal with tabular data These methods are to replicate what dplyr in R is capable of To handle tabular data like data frames, I would still recommend using R instead of Python for beginners. pandas can be intimidating for a lot of beginners. The statsmodels can download R datasets from https://vincentarelbundock.github.io/Rdatasets/datasets.html 18.1 Libraries import pandas as pd import numpy as np import statsmodels.api as sm import matplotlib 18.2 Importing/Exporting Data Importing: pd.read_csv(filename): From a CSV file pd.read_table(filename): From a delimited text file (like TSV) pd.read_excel(filename): From an Excel file pd.read_sql(query, connection_object): Read from a SQL table/database pd.read_json(json_string): Read from a JSON formatted string, URL or file. pd.read_html(url): Parses an html URL, string or file and extracts tables to a list of dataframes pd.read_clipboard(): Takes the contents of your clipboard and passes it to read_table() pd.DataFrame(dict): From a dict, keys for columns names, values for data as lists pd.DataFrame(list of tuples): From a list, which includes the records of each row Exporting: df.to_csv(filename) df.to_excel(filename) df.to_sql(table_name, connection_object) df.to_json(filename) DEMO_DATA_DIR = &#39;demo_data/titanic/&#39; iris = sm.datasets.get_rdataset(&#39;iris&#39;).data titanic = pd.read_csv(DEMO_DATA_DIR+&#39;train.csv&#39;) iris.head() Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa titanic.head() PassengerId Survived Pclass ... Fare Cabin Embarked 0 1 0 3 ... 7.2500 NaN S 1 2 1 1 ... 71.2833 C85 C 2 3 1 3 ... 7.9250 NaN S 3 4 1 1 ... 53.1000 C123 S 4 5 0 3 ... 8.0500 NaN S [5 rows x 12 columns] x= [(1,2,3,4), (5,6,7,8), (9,10,11,12)] pd.DataFrame(x,columns=[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;]) A B C D 0 1 2 3 4 1 5 6 7 8 2 9 10 11 12 x = {&quot;A&quot;:[1,2,3,4], &quot;B&quot;:[5,6,7,8], &quot;C&quot;:[9,10,11,12]} pd.DataFrame(x) A B C 0 1 5 9 1 2 6 10 2 3 7 11 3 4 8 12 When you have data of the columns, use dict; when you have the data of the rows, use list as the source data structures of a data frame. 18.3 Inspecting Data Frame df.head(n): First n rows of the DataFrame df.tail(n): Last n rows of the DataFrame df.shape: Number of rows and columns df.info(): Index, Datatype and Memory information df.describe(): Summary statistics for numerical columns s.value_counts(dropna=False): View unique values and counts df.apply(pd.Series.value_counts): Unique values and counts for all columns df.columns df.index df.dtypes df.set_index('column_name'): Set a column as the index iris.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Sepal.Length 150 non-null float64 1 Sepal.Width 150 non-null float64 2 Petal.Length 150 non-null float64 3 Petal.Width 150 non-null float64 4 Species 150 non-null object dtypes: float64(4), object(1) memory usage: 6.0+ KB iris.describe() Sepal.Length Sepal.Width Petal.Length Petal.Width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 print(iris.shape) (150, 5) iris.head(3) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa iris.tail(3) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica iris[&#39;Species&#39;].value_counts() Species setosa 50 versicolor 50 virginica 50 Name: count, dtype: int64 print(iris.columns) Index([&#39;Sepal.Length&#39;, &#39;Sepal.Width&#39;, &#39;Petal.Length&#39;, &#39;Petal.Width&#39;, &#39;Species&#39;], dtype=&#39;object&#39;) print(iris.index) RangeIndex(start=0, stop=150, step=1) print(iris.dtypes) Sepal.Length float64 Sepal.Width float64 Petal.Length float64 Petal.Width float64 Species object dtype: object 18.4 Basic Functions ## DataFrame attributes iris.shape (150, 5) iris.columns Index([&#39;Sepal.Length&#39;, &#39;Sepal.Width&#39;, &#39;Petal.Length&#39;, &#39;Petal.Width&#39;, &#39;Species&#39;], dtype=&#39;object&#39;) iris.index RangeIndex(start=0, stop=150, step=1) iris.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Sepal.Length 150 non-null float64 1 Sepal.Width 150 non-null float64 2 Petal.Length 150 non-null float64 3 Petal.Width 150 non-null float64 4 Species 150 non-null object dtypes: float64(4), object(1) memory usage: 6.0+ KB iris.describe() Sepal.Length Sepal.Width Petal.Length Petal.Width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 iris.dtypes # check column data types Sepal.Length float64 Sepal.Width float64 Petal.Length float64 Petal.Width float64 Species object dtype: object 18.5 Subsetting Data Frame df[col]: Returns column with label col as Series df[[col1, col2]]: Returns columns as a new DataFrame s.iloc[0]: Selection by position s.loc['index_one']: Selection by index df.iloc[0,:]: First row df.iloc[0,0]: First element of first column iris.loc[:5, &#39;Species&#39;] # first six rows of &#39;Species&#39; column 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa 5 setosa Name: Species, dtype: object iris.iloc[:5, 4] # same as above 0 setosa 1 setosa 2 setosa 3 setosa 4 setosa Name: Species, dtype: object 18.6 Exploration How to perform the key functions provided in R dplyr? dplyr Key Verbs filter() select() mutate() arrange() summarize() group_by() 18.6.1 NA Values Functions to take care of NA values: df.isnull() df.notnull() df.dropna(): Drop rows with null values df.dropna(axis=1): Drop columns with null values df.dropna(axis=1, thresh=n): Drop all columns have less than n non-values df.fillna(x): Replaces all null values with x s.fillna(s.mean()): Replace the null values of a Series with its mean score Quick check of the null values in each column titanic.isnull().sum() PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 titanic.dropna(axis=1, thresh=600) PassengerId Survived Pclass ... Ticket Fare Embarked 0 1 0 3 ... A/5 21171 7.2500 S 1 2 1 1 ... PC 17599 71.2833 C 2 3 1 3 ... STON/O2. 3101282 7.9250 S 3 4 1 1 ... 113803 53.1000 S 4 5 0 3 ... 373450 8.0500 S .. ... ... ... ... ... ... ... 886 887 0 2 ... 211536 13.0000 S 887 888 1 1 ... 112053 30.0000 S 888 889 0 3 ... W./C. 6607 23.4500 S 889 890 1 1 ... 111369 30.0000 C 890 891 0 3 ... 370376 7.7500 Q [891 rows x 11 columns] titanic.notnull().sum() PassengerId 891 Survived 891 Pclass 891 Name 891 Sex 891 Age 714 SibSp 891 Parch 891 Ticket 891 Fare 891 Cabin 204 Embarked 889 dtype: int64 18.6.2 Converting Data Types s.astype(float): Convert a Series into a float type iris.dtypes Sepal.Length float64 Sepal.Width float64 Petal.Length float64 Petal.Width float64 Species object dtype: object iris[&#39;Species&#39;]=iris[&#39;Species&#39;].astype(&#39;category&#39;) iris.dtypes Sepal.Length float64 Sepal.Width float64 Petal.Length float64 Petal.Width float64 Species category dtype: object #iris.value_counts(iris[&#39;Species&#39;]).plot.bar() 18.6.3 Pandas-supported Data Types pandas-dtypes (source) 18.6.4 Transformation s.replace(X, Y) titanic.head() PassengerId Survived Pclass ... Fare Cabin Embarked 0 1 0 3 ... 7.2500 NaN S 1 2 1 1 ... 71.2833 C85 C 2 3 1 3 ... 7.9250 NaN S 3 4 1 1 ... 53.1000 C123 S 4 5 0 3 ... 8.0500 NaN S [5 rows x 12 columns] titanic.value_counts(titanic[&#39;Survived&#39;]).plot.bar() titanic.columns Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Name&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;], dtype=&#39;object&#39;) titanic.groupby([&#39;Sex&#39;,&#39;Pclass&#39;]).mean(numeric_only=True) PassengerId Survived Age SibSp Parch Fare Sex Pclass female 1 469.212766 0.968085 34.611765 0.553191 0.457447 106.125798 2 443.105263 0.921053 28.722973 0.486842 0.605263 21.970121 3 399.729167 0.500000 21.750000 0.895833 0.798611 16.118810 male 1 455.729508 0.368852 41.281386 0.311475 0.278689 67.226127 2 447.962963 0.157407 30.740707 0.342593 0.222222 19.741782 3 455.515850 0.135447 26.507589 0.498559 0.224784 12.661633 titanic[titanic[&#39;Age&#39;] &lt; 18].groupby([&#39;Sex&#39;,&#39;Pclass&#39;]).mean(numeric_only=True) PassengerId Survived Age SibSp Parch Fare Sex Pclass female 1 525.375000 0.875000 14.125000 0.500000 0.875000 104.083337 2 369.250000 1.000000 8.333333 0.583333 1.083333 26.241667 3 374.942857 0.542857 8.428571 1.571429 1.057143 18.727977 male 1 526.500000 1.000000 8.230000 0.500000 2.000000 116.072900 2 527.818182 0.818182 4.757273 0.727273 1.000000 25.659473 3 437.953488 0.232558 9.963256 2.069767 1.000000 22.752523 18.6.5 filter() ## filter iris[iris[&#39;Sepal.Length&#39;]&gt;5] Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 5 5.4 3.9 1.7 0.4 setosa 10 5.4 3.7 1.5 0.2 setosa 14 5.8 4.0 1.2 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica [118 rows x 5 columns] When there are more than one filtering condition, put the conditions in parentheses. iris[(iris[&#39;Sepal.Length&#39;]&gt;4) &amp; (iris[&#39;Sepal.Width&#39;]&gt;5)] Empty DataFrame Columns: [Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species] Index: [] iris.query(&#39;`Sepal.Length`&gt;5&#39;) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 5 5.4 3.9 1.7 0.4 setosa 10 5.4 3.7 1.5 0.2 setosa 14 5.8 4.0 1.2 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica [118 rows x 5 columns] iris[(iris[&#39;Sepal.Length&#39;]&gt;5) &amp; (iris[&#39;Sepal.Width&#39;]&gt;4)] Sepal.Length Sepal.Width Petal.Length Petal.Width Species 15 5.7 4.4 1.5 0.4 setosa 32 5.2 4.1 1.5 0.1 setosa 33 5.5 4.2 1.4 0.2 setosa 18.6.6 arrange() iris.sort_values([&#39;Species&#39;,&#39;Sepal.Length&#39;], ascending=[False,True]) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 106 4.9 2.5 4.5 1.7 virginica 121 5.6 2.8 4.9 2.0 virginica 113 5.7 2.5 5.0 2.0 virginica 101 5.8 2.7 5.1 1.9 virginica 114 5.8 2.8 5.1 2.4 virginica .. ... ... ... ... ... 33 5.5 4.2 1.4 0.2 setosa 36 5.5 3.5 1.3 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa 18 5.7 3.8 1.7 0.3 setosa 14 5.8 4.0 1.2 0.2 setosa [150 rows x 5 columns] 18.6.7 select() ## select iris[[&#39;Sepal.Length&#39;, &#39;Species&#39;]] Sepal.Length Species 0 5.1 setosa 1 4.9 setosa 2 4.7 setosa 3 4.6 setosa 4 5.0 setosa .. ... ... 145 6.7 virginica 146 6.3 virginica 147 6.5 virginica 148 6.2 virginica 149 5.9 virginica [150 rows x 2 columns] ## deselect columns iris.drop([&#39;Sepal.Length&#39;], axis=1).head() Sepal.Width Petal.Length Petal.Width Species 0 3.5 1.4 0.2 setosa 1 3.0 1.4 0.2 setosa 2 3.2 1.3 0.2 setosa 3 3.1 1.5 0.2 setosa 4 3.6 1.4 0.2 setosa iris.filter([&#39;Species&#39;,&#39;Sepal.Length&#39;]) Species Sepal.Length 0 setosa 5.1 1 setosa 4.9 2 setosa 4.7 3 setosa 4.6 4 setosa 5.0 .. ... ... 145 virginica 6.7 146 virginica 6.3 147 virginica 6.5 148 virginica 6.2 149 virginica 5.9 [150 rows x 2 columns] iris[[&#39;Species&#39;,&#39;Sepal.Length&#39;]] Species Sepal.Length 0 setosa 5.1 1 setosa 4.9 2 setosa 4.7 3 setosa 4.6 4 setosa 5.0 .. ... ... 145 virginica 6.7 146 virginica 6.3 147 virginica 6.5 148 virginica 6.2 149 virginica 5.9 [150 rows x 2 columns] ## extract one particular column sepal_length = iris[&#39;Sepal.Length&#39;] type(sepal_length) &lt;class &#39;pandas.core.series.Series&#39;&gt; 18.6.8 mutate() ## mutate iris[&#39;Species_new&#39;] = iris[&#39;Species&#39;].apply(lambda x: len(x)) iris[&#39;Species_initial&#39;] = iris[&#39;Species&#39;].apply(lambda x: x[:2].upper()) iris Sepal.Length Sepal.Width ... Species_new Species_initial 0 5.1 3.5 ... 6 SE 1 4.9 3.0 ... 6 SE 2 4.7 3.2 ... 6 SE 3 4.6 3.1 ... 6 SE 4 5.0 3.6 ... 6 SE .. ... ... ... ... ... 145 6.7 3.0 ... 9 VI 146 6.3 2.5 ... 9 VI 147 6.5 3.0 ... 9 VI 148 6.2 3.4 ... 9 VI 149 5.9 3.0 ... 9 VI [150 rows x 7 columns] ## mutate alternative 2 iris.assign(Specias_initial2 = iris[&#39;Species&#39;].apply(lambda x: x.upper())) Sepal.Length Sepal.Width ... Species_initial Specias_initial2 0 5.1 3.5 ... SE SETOSA 1 4.9 3.0 ... SE SETOSA 2 4.7 3.2 ... SE SETOSA 3 4.6 3.1 ... SE SETOSA 4 5.0 3.6 ... SE SETOSA .. ... ... ... ... ... 145 6.7 3.0 ... VI VIRGINICA 146 6.3 2.5 ... VI VIRGINICA 147 6.5 3.0 ... VI VIRGINICA 148 6.2 3.4 ... VI VIRGINICA 149 5.9 3.0 ... VI VIRGINICA [150 rows x 8 columns] 18.6.9 apply(), mutate_if() df.apply(np.mean): Apply a function to all columns df.apply(np.max,axis=1): Apply a function to each row When apply() functions to the data frame, the axis=1 refers to row mutation and axis=0 refers to column mutation. This is very counter-intuitive for R users. iris.head(10) Sepal.Length Sepal.Width Petal.Length ... Species Species_new Species_initial 0 5.1 3.5 1.4 ... setosa 6 SE 1 4.9 3.0 1.4 ... setosa 6 SE 2 4.7 3.2 1.3 ... setosa 6 SE 3 4.6 3.1 1.5 ... setosa 6 SE 4 5.0 3.6 1.4 ... setosa 6 SE 5 5.4 3.9 1.7 ... setosa 6 SE 6 4.6 3.4 1.4 ... setosa 6 SE 7 5.0 3.4 1.5 ... setosa 6 SE 8 4.4 2.9 1.4 ... setosa 6 SE 9 4.9 3.1 1.5 ... setosa 6 SE [10 rows x 7 columns] iris[[&#39;Sepal.Width&#39;,&#39;Petal.Width&#39;]].apply(np.sum, axis=1).head(10) 0 3.7 1 3.2 2 3.4 3 3.3 4 3.8 5 4.3 6 3.7 7 3.6 8 3.1 9 3.2 dtype: float64 18.6.10 group_by() and summarize() iris.groupby(by=&#39;Species&#39;).mean(numeric_only=True) Sepal.Length Sepal.Width Petal.Length Petal.Width Species setosa 5.006 3.428 1.462 0.246 versicolor 5.936 2.770 4.260 1.326 virginica 6.588 2.974 5.552 2.026 iris.filter([&#39;Species&#39;,&#39;Sepal.Length&#39;]).groupby(&#39;Species&#39;).agg({&#39;Sepal.Length&#39;:[&#39;mean&#39;,&#39;count&#39;,&#39;std&#39;]}) Sepal.Length mean count std Species setosa 5.006 50 0.352490 versicolor 5.936 50 0.516171 virginica 6.588 50 0.635880 titanic.head() PassengerId Survived Pclass ... Fare Cabin Embarked 0 1 0 3 ... 7.2500 NaN S 1 2 1 1 ... 71.2833 C85 C 2 3 1 3 ... 7.9250 NaN S 3 4 1 1 ... 53.1000 C123 S 4 5 0 3 ... 8.0500 NaN S [5 rows x 12 columns] titanic.groupby([&#39;Pclass&#39;,&#39;Sex&#39;]).agg(np.sum) PassengerId ... Embarked Pclass Sex ... 1 female 44106 ... CSSCCSSSSCCCCCSCSCSSSSCSCCCCCCSCCCSCSSCCCCSCQS... male 55599 ... SSSCCSCSSCSSCCSSCSSCCSSCSSCSQSSSSSCSCSSSSSSSCC... 2 female 33676 ... CSSCSSSSSSSSSSSSSSSSQSSQSSSSSSCSSSSSSSSSSSSCSS... male 48380 ... SSSSSSSSSSCSCSSSSSSCSSSSSSSSSSSSSSSSSSSCCSSSSS... 3 female 57561 ... SSSSSCQSSQQSCSQQSSSSQSSSQCSCSCSCSSSQSSSSQSQSQS... male 158064 ... SSQSSSQCSCSCSQCSSCSCSCSSCSSSSSSSSSSSSSSSSSSSSS... [6 rows x 10 columns] titanic.pivot_table(index=[&#39;Pclass&#39;,&#39;Sex&#39;], values=[&#39;Survived&#39;], aggfunc=np.sum) Survived Pclass Sex 1 female 91 male 45 2 female 70 male 17 3 female 72 male 47 18.6.11 rename() iris Sepal.Length Sepal.Width ... Species_new Species_initial 0 5.1 3.5 ... 6 SE 1 4.9 3.0 ... 6 SE 2 4.7 3.2 ... 6 SE 3 4.6 3.1 ... 6 SE 4 5.0 3.6 ... 6 SE .. ... ... ... ... ... 145 6.7 3.0 ... 9 VI 146 6.3 2.5 ... 9 VI 147 6.5 3.0 ... 9 VI 148 6.2 3.4 ... 9 VI 149 5.9 3.0 ... 9 VI [150 rows x 7 columns] iris.columns Index([&#39;Sepal.Length&#39;, &#39;Sepal.Width&#39;, &#39;Petal.Length&#39;, &#39;Petal.Width&#39;, &#39;Species&#39;, &#39;Species_new&#39;, &#39;Species_initial&#39;], dtype=&#39;object&#39;) Selective renaming column names iris = iris.rename(columns={&#39;Sepal.Length&#39;:&#39;SLen&#39;}) iris SLen Sepal.Width Petal.Length ... Species Species_new Species_initial 0 5.1 3.5 1.4 ... setosa 6 SE 1 4.9 3.0 1.4 ... setosa 6 SE 2 4.7 3.2 1.3 ... setosa 6 SE 3 4.6 3.1 1.5 ... setosa 6 SE 4 5.0 3.6 1.4 ... setosa 6 SE .. ... ... ... ... ... ... ... 145 6.7 3.0 5.2 ... virginica 9 VI 146 6.3 2.5 5.0 ... virginica 9 VI 147 6.5 3.0 5.2 ... virginica 9 VI 148 6.2 3.4 5.4 ... virginica 9 VI 149 5.9 3.0 5.1 ... virginica 9 VI [150 rows x 7 columns] Massive renaming column names iris.rename(columns=lambda x: &#39;XX&#39;+x) XXSLen XXSepal.Width ... XXSpecies_new XXSpecies_initial 0 5.1 3.5 ... 6 SE 1 4.9 3.0 ... 6 SE 2 4.7 3.2 ... 6 SE 3 4.6 3.1 ... 6 SE 4 5.0 3.6 ... 6 SE .. ... ... ... ... ... 145 6.7 3.0 ... 9 VI 146 6.3 2.5 ... 9 VI 147 6.5 3.0 ... 9 VI 148 6.2 3.4 ... 9 VI 149 5.9 3.0 ... 9 VI [150 rows x 7 columns] titanic.head(10) PassengerId Survived Pclass ... Fare Cabin Embarked 0 1 0 3 ... 7.2500 NaN S 1 2 1 1 ... 71.2833 C85 C 2 3 1 3 ... 7.9250 NaN S 3 4 1 1 ... 53.1000 C123 S 4 5 0 3 ... 8.0500 NaN S 5 6 0 3 ... 8.4583 NaN Q 6 7 0 1 ... 51.8625 E46 S 7 8 0 3 ... 21.0750 NaN S 8 9 1 3 ... 11.1333 NaN S 9 10 1 2 ... 30.0708 NaN C [10 rows x 12 columns] titanic.set_index(&#39;Name&#39;).rename(index=lambda x:x.replace(&#39; &#39;,&quot;_&quot;).upper()) PassengerId ... Embarked Name ... BRAUND,_MR._OWEN_HARRIS 1 ... S CUMINGS,_MRS._JOHN_BRADLEY_(FLORENCE_BRIGGS_THA... 2 ... C HEIKKINEN,_MISS._LAINA 3 ... S FUTRELLE,_MRS._JACQUES_HEATH_(LILY_MAY_PEEL) 4 ... S ALLEN,_MR._WILLIAM_HENRY 5 ... S ... ... ... ... MONTVILA,_REV._JUOZAS 887 ... S GRAHAM,_MISS._MARGARET_EDITH 888 ... S JOHNSTON,_MISS._CATHERINE_HELEN_&quot;CARRIE&quot; 889 ... S BEHR,_MR._KARL_HOWELL 890 ... C DOOLEY,_MR._PATRICK 891 ... Q [891 rows x 11 columns] 18.7 Join/Combine Data Frames df1.append(df2): Add the rows in df1 to the end of df2 (columns should be identical) (rbind() in R) pd.concat([df1, df2],axis=1): Add the columns in df1 to the end of df2 (rows should be identical) (cbind() in R) df1.join(df2,on=col1,how='inner'): SQL-style join the columns in df1 with the columns on df2 where the rows for col have identical values. ‘how’ can be one of ‘left’, ‘right’, ‘outer’, ‘inner’ 18.8 Statistics df.describe(): Summary statistics for numerical columns df.mean(): Returns the mean of all columns df.corr(): Returns the correlation between columns in a DataFrame df.count(): Returns the number of non-null values in each DataFrame column df.max(): Returns the highest value in each column df.min(): Returns the lowest value in each column df.median(): Returns the median of each column df.std(): Returns the standard deviation of each column titanic.count() PassengerId 891 Survived 891 Pclass 891 Name 891 Sex 891 Age 714 SibSp 891 Parch 891 Ticket 891 Fare 891 Cabin 204 Embarked 889 dtype: int64 titanic.median(numeric_only=True) PassengerId 446.0000 Survived 0.0000 Pclass 3.0000 Age 28.0000 SibSp 0.0000 Parch 0.0000 Fare 14.4542 dtype: float64 18.9 Generic Functions pandas.pivot_table() pandas.crosstab() pandas.cut() pandas.qcut() pandas.merge() pandas.get_dummies() 18.10 References Python for Data Analysis Python for Data Analysis GitHub How to get sample datasets in Python "],["nltk.html", "Chapter 19 NLTK 19.1 Installation 19.2 Corpora Data 19.3 WordNet 19.4 Discovering Word Collocations 19.5 Tokenization 19.6 Chinese Word Segmentation 19.7 Afterwords", " Chapter 19 NLTK The almighty nltk package! 19.1 Installation Install package in the terminal !pip install nltk Download nltk data in python import nltk nltk.download(&#39;all&#39;, halt_on_error=False) import nltk # nltk.download() The complete collection of the nltk.corpus is huge. You probably don’t need all of the corpora data. You can use nltk.download() to initialize a User Window for installation of specific datasets. 19.2 Corpora Data The package includes a lot of pre-loaded corpora datasets The default nltk_data directory is in /Users/YOUT_NAME/nltk_data/ Selective Examples Brown Corpus Reuters Corpus WordNet from nltk.corpus import gutenberg, brown, reuters # brown corpus ## Categories (topics?) print(&#39;Brown Corpus Total Categories: &#39;, len(brown.categories())) Brown Corpus Total Categories: 15 print(&#39;Categories List: &#39;, brown.categories()) Categories List: [&#39;adventure&#39;, &#39;belles_lettres&#39;, &#39;editorial&#39;, &#39;fiction&#39;, &#39;government&#39;, &#39;hobbies&#39;, &#39;humor&#39;, &#39;learned&#39;, &#39;lore&#39;, &#39;mystery&#39;, &#39;news&#39;, &#39;religion&#39;, &#39;reviews&#39;, &#39;romance&#39;, &#39;science_fiction&#39;] # Sentences print(brown.sents()[0]) ## first sentence [&#39;The&#39;, &#39;Fulton&#39;, &#39;County&#39;, &#39;Grand&#39;, &#39;Jury&#39;, &#39;said&#39;, &#39;Friday&#39;, &#39;an&#39;, &#39;investigation&#39;, &#39;of&#39;, &quot;Atlanta&#39;s&quot;, &#39;recent&#39;, &#39;primary&#39;, &#39;election&#39;, &#39;produced&#39;, &#39;``&#39;, &#39;no&#39;, &#39;evidence&#39;, &quot;&#39;&#39;&quot;, &#39;that&#39;, &#39;any&#39;, &#39;irregularities&#39;, &#39;took&#39;, &#39;place&#39;, &#39;.&#39;] print(brown.sents(categories=&#39;fiction&#39;)) ## first sentence for fiction texts [[&#39;Thirty-three&#39;], [&#39;Scotty&#39;, &#39;did&#39;, &#39;not&#39;, &#39;go&#39;, &#39;back&#39;, &#39;to&#39;, &#39;school&#39;, &#39;.&#39;], ...] ## Tagged Sentences print(brown.tagged_sents()[0]) [(&#39;The&#39;, &#39;AT&#39;), (&#39;Fulton&#39;, &#39;NP-TL&#39;), (&#39;County&#39;, &#39;NN-TL&#39;), (&#39;Grand&#39;, &#39;JJ-TL&#39;), (&#39;Jury&#39;, &#39;NN-TL&#39;), (&#39;said&#39;, &#39;VBD&#39;), (&#39;Friday&#39;, &#39;NR&#39;), (&#39;an&#39;, &#39;AT&#39;), (&#39;investigation&#39;, &#39;NN&#39;), (&#39;of&#39;, &#39;IN&#39;), (&quot;Atlanta&#39;s&quot;, &#39;NP$&#39;), (&#39;recent&#39;, &#39;JJ&#39;), (&#39;primary&#39;, &#39;NN&#39;), (&#39;election&#39;, &#39;NN&#39;), (&#39;produced&#39;, &#39;VBD&#39;), (&#39;``&#39;, &#39;``&#39;), (&#39;no&#39;, &#39;AT&#39;), (&#39;evidence&#39;, &#39;NN&#39;), (&quot;&#39;&#39;&quot;, &quot;&#39;&#39;&quot;), (&#39;that&#39;, &#39;CS&#39;), (&#39;any&#39;, &#39;DTI&#39;), (&#39;irregularities&#39;, &#39;NNS&#39;), (&#39;took&#39;, &#39;VBD&#39;), (&#39;place&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)] ## Sentence in natural forms sents = brown.sents(categories=&#39;fiction&#39;) [&#39; &#39;.join(sent) for sent in sents[1:5]] [&#39;Scotty did not go back to school .&#39;, &#39;His parents talked seriously and lengthily to their own doctor and to a specialist at the University Hospital -- Mr. McKinley was entitled to a discount for members of his family -- and it was decided it would be best for him to take the remainder of the term off , spend a lot of time in bed and , for the rest , do pretty much as he chose -- provided , of course , he chose to do nothing too exciting or too debilitating .&#39;, &#39;His teacher and his school principal were conferred with and everyone agreed that , if he kept up with a certain amount of work at home , there was little danger of his losing a term .&#39;, &#39;Scotty accepted the decision with indifference and did not enter the arguments .&#39;] ## Get tagged words tagged_words = brown.tagged_words(categories=&#39;fiction&#39;) #print(tagged_words[1]) ## a tuple ## Get all nouns nouns = [(word, tag) for word, tag in tagged_words if any (noun_tag in tag for noun_tag in [&#39;NP&#39;,&#39;NN&#39;])] ## Check first ten nouns nouns[:10] [(&#39;Scotty&#39;, &#39;NP&#39;), (&#39;school&#39;, &#39;NN&#39;), (&#39;parents&#39;, &#39;NNS&#39;), (&#39;doctor&#39;, &#39;NN&#39;), (&#39;specialist&#39;, &#39;NN&#39;), (&#39;University&#39;, &#39;NN-TL&#39;), (&#39;Hospital&#39;, &#39;NN-TL&#39;), (&#39;Mr.&#39;, &#39;NP&#39;), (&#39;McKinley&#39;, &#39;NP&#39;), (&#39;discount&#39;, &#39;NN&#39;)] ## Creating Freq list nouns_freq = nltk.FreqDist([w for w, t in nouns]) sorted(nouns_freq.items(),key=lambda x:x[1], reverse=True)[:20] [(&#39;man&#39;, 111), (&#39;time&#39;, 99), (&#39;men&#39;, 72), (&#39;room&#39;, 63), (&#39;way&#39;, 62), (&#39;eyes&#39;, 60), (&#39;face&#39;, 55), (&#39;house&#39;, 54), (&#39;head&#39;, 54), (&#39;night&#39;, 53), (&#39;day&#39;, 52), (&#39;hand&#39;, 50), (&#39;door&#39;, 47), (&#39;life&#39;, 44), (&#39;years&#39;, 44), (&#39;Mrs.&#39;, 41), (&#39;God&#39;, 41), (&#39;Kate&#39;, 40), (&#39;Mr.&#39;, 39), (&#39;people&#39;, 39)] sorted(nouns_freq.items(),key=lambda x:x[0], reverse=True)[:20] [(&#39;zoo&#39;, 2), (&#39;zlotys&#39;, 1), (&#39;zenith&#39;, 1), (&#39;youth&#39;, 5), (&#39;yelling&#39;, 1), (&#39;years&#39;, 44), (&#39;yearning&#39;, 1), (&quot;year&#39;s&quot;, 1), (&#39;year&#39;, 9), (&#39;yards&#39;, 4), (&#39;yard&#39;, 7), (&#39;yachts&#39;, 1), (&#39;writing&#39;, 2), (&#39;writers&#39;, 1), (&#39;writer&#39;, 4), (&#39;wrists&#39;, 1), (&#39;wrist&#39;, 2), (&#39;wrinkles&#39;, 1), (&#39;wrinkle&#39;, 1), (&#39;wretch&#39;, 1)] nouns_freq.most_common(10) [(&#39;man&#39;, 111), (&#39;time&#39;, 99), (&#39;men&#39;, 72), (&#39;room&#39;, 63), (&#39;way&#39;, 62), (&#39;eyes&#39;, 60), (&#39;face&#39;, 55), (&#39;house&#39;, 54), (&#39;head&#39;, 54), (&#39;night&#39;, 53)] ## Accsess data via fileid brown.fileids(categories=&#39;fiction&#39;)[0] &#39;ck01&#39; brown.sents(fileids=&#39;ck01&#39;) [[&#39;Thirty-three&#39;], [&#39;Scotty&#39;, &#39;did&#39;, &#39;not&#39;, &#39;go&#39;, &#39;back&#39;, &#39;to&#39;, &#39;school&#39;, &#39;.&#39;], ...] 19.3 WordNet 19.3.1 A Dictionary Resource from nltk.corpus import wordnet as wn word = &#39;walk&#39; # get synsets word_synsets = wn.synsets(word, pos=&#39;v&#39;) word_synsets [Synset(&#39;walk.v.01&#39;), Synset(&#39;walk.v.02&#39;), Synset(&#39;walk.v.03&#39;), Synset(&#39;walk.v.04&#39;), Synset(&#39;walk.v.05&#39;), Synset(&#39;walk.v.06&#39;), Synset(&#39;walk.v.07&#39;), Synset(&#39;walk.v.08&#39;), Synset(&#39;walk.v.09&#39;), Synset(&#39;walk.v.10&#39;)] Word sense is closely connected to its parts-of-speech. Therefore, in WordNet it is crucial to specify the POS tag of the word to obtain the correct synset of the word. There are four common part-of-speech tags in WordNet, as shown below: Part of Speech Tag Noun n Adjective a Adverb r Verb v Check the definition of a synset (i.e., a specific sense of the word): word_synsets[0].definition() &quot;use one&#39;s feet to advance; advance by steps&quot; Check the examples of a synset: word_synsets[0].examples() [&quot;Walk, don&#39;t run!&quot;, &#39;We walked instead of driving&#39;, &#39;She walks with a slight limp&#39;, &#39;The patient cannot walk yet&#39;, &#39;Walk over to the cabinet&#39;] ## Get details of each synset for s in word_synsets: if str(s.name()).startswith(&#39;walk.v&#39;): print( &#39;Syset ID: %s \\n&#39; &#39;POS Tag: %s \\n&#39; &#39;Definition: %s \\n&#39; &#39;Examples: %s \\n&#39; % (s.name(), s.pos(), s.definition(),s.examples()) ) Syset ID: walk.v.01 POS Tag: v Definition: use one&#39;s feet to advance; advance by steps Examples: [&quot;Walk, don&#39;t run!&quot;, &#39;We walked instead of driving&#39;, &#39;She walks with a slight limp&#39;, &#39;The patient cannot walk yet&#39;, &#39;Walk over to the cabinet&#39;] Syset ID: walk.v.02 POS Tag: v Definition: accompany or escort Examples: [&quot;I&#39;ll walk you to your car&quot;] Syset ID: walk.v.03 POS Tag: v Definition: obtain a base on balls Examples: [] Syset ID: walk.v.04 POS Tag: v Definition: traverse or cover by walking Examples: [&#39;Walk the tightrope&#39;, &#39;Paul walked the streets of Damascus&#39;, &#39;She walks 3 miles every day&#39;] Syset ID: walk.v.05 POS Tag: v Definition: give a base on balls to Examples: [] Syset ID: walk.v.06 POS Tag: v Definition: live or behave in a specified manner Examples: [&#39;walk in sadness&#39;] Syset ID: walk.v.07 POS Tag: v Definition: be or act in association with Examples: [&#39;We must walk with our dispossessed brothers and sisters&#39;, &#39;Walk with God&#39;] Syset ID: walk.v.08 POS Tag: v Definition: walk at a pace Examples: [&#39;The horses walked across the meadow&#39;] Syset ID: walk.v.09 POS Tag: v Definition: make walk Examples: [&#39;He walks the horse up the mountain&#39;, &#39;Walk the dog twice a day&#39;] Syset ID: walk.v.10 POS Tag: v Definition: take a walk; go for a walk; walk for pleasure Examples: [&#39;The lovers held hands while walking&#39;, &#39;We like to walk every Sunday&#39;] 19.3.2 Lexical Relations Extract words that maintain some lexical relations with the synset: word_synsets[0].hypernyms() # hypernym [Synset(&#39;travel.v.01&#39;)] word_synsets[0].hypernyms()[0].hyponyms() # similar words [Synset(&#39;accompany.v.02&#39;), Synset(&#39;advance.v.01&#39;), Synset(&#39;angle.v.01&#39;), Synset(&#39;ascend.v.01&#39;), Synset(&#39;automobile.v.01&#39;), Synset(&#39;back.v.02&#39;), Synset(&#39;bang.v.04&#39;), Synset(&#39;beetle.v.02&#39;), Synset(&#39;betake_oneself.v.01&#39;), Synset(&#39;billow.v.02&#39;), Synset(&#39;bounce.v.03&#39;), Synset(&#39;breeze.v.02&#39;), Synset(&#39;caravan.v.01&#39;), Synset(&#39;career.v.01&#39;), Synset(&#39;carry.v.36&#39;), Synset(&#39;circle.v.01&#39;), Synset(&#39;circle.v.02&#39;), Synset(&#39;circuit.v.01&#39;), Synset(&#39;circulate.v.07&#39;), Synset(&#39;come.v.01&#39;), Synset(&#39;come.v.11&#39;), Synset(&#39;crawl.v.01&#39;), Synset(&#39;cruise.v.02&#39;), Synset(&#39;derail.v.02&#39;), Synset(&#39;descend.v.01&#39;), Synset(&#39;do.v.13&#39;), Synset(&#39;drag.v.04&#39;), Synset(&#39;draw.v.12&#39;), Synset(&#39;drive.v.02&#39;), Synset(&#39;drive.v.14&#39;), Synset(&#39;ease.v.01&#39;), Synset(&#39;fall.v.01&#39;), Synset(&#39;fall.v.15&#39;), Synset(&#39;ferry.v.03&#39;), Synset(&#39;float.v.01&#39;), Synset(&#39;float.v.02&#39;), Synset(&#39;float.v.05&#39;), Synset(&#39;flock.v.01&#39;), Synset(&#39;fly.v.01&#39;), Synset(&#39;fly.v.06&#39;), Synset(&#39;follow.v.01&#39;), Synset(&#39;follow.v.04&#39;), Synset(&#39;forge.v.05&#39;), Synset(&#39;get_around.v.04&#39;), Synset(&#39;ghost.v.01&#39;), Synset(&#39;glide.v.01&#39;), Synset(&#39;go_around.v.02&#39;), Synset(&#39;hiss.v.02&#39;), Synset(&#39;hurtle.v.01&#39;), Synset(&#39;island_hop.v.01&#39;), Synset(&#39;lance.v.01&#39;), Synset(&#39;lurch.v.03&#39;), Synset(&#39;outflank.v.01&#39;), Synset(&#39;pace.v.02&#39;), Synset(&#39;pan.v.01&#39;), Synset(&#39;pass.v.01&#39;), Synset(&#39;pass_over.v.04&#39;), Synset(&#39;play.v.09&#39;), Synset(&#39;plow.v.03&#39;), Synset(&#39;prance.v.02&#39;), Synset(&#39;precede.v.04&#39;), Synset(&#39;precess.v.01&#39;), Synset(&#39;proceed.v.02&#39;), Synset(&#39;propagate.v.02&#39;), Synset(&#39;pursue.v.02&#39;), Synset(&#39;push.v.09&#39;), Synset(&#39;raft.v.02&#39;), Synset(&#39;repair.v.03&#39;), Synset(&#39;retreat.v.02&#39;), Synset(&#39;retrograde.v.02&#39;), Synset(&#39;return.v.01&#39;), Synset(&#39;ride.v.01&#39;), Synset(&#39;ride.v.04&#39;), Synset(&#39;ride.v.10&#39;), Synset(&#39;rise.v.01&#39;), Synset(&#39;roll.v.12&#39;), Synset(&#39;round.v.01&#39;), Synset(&#39;run.v.11&#39;), Synset(&#39;run.v.34&#39;), Synset(&#39;rush.v.01&#39;), Synset(&#39;scramble.v.01&#39;), Synset(&#39;seek.v.04&#39;), Synset(&#39;shuttle.v.01&#39;), Synset(&#39;sift.v.01&#39;), Synset(&#39;ski.v.01&#39;), Synset(&#39;slice_into.v.01&#39;), Synset(&#39;slither.v.01&#39;), Synset(&#39;snowshoe.v.01&#39;), Synset(&#39;speed.v.04&#39;), Synset(&#39;steamer.v.01&#39;), Synset(&#39;step.v.01&#39;), Synset(&#39;step.v.02&#39;), Synset(&#39;step.v.06&#39;), Synset(&#39;stray.v.02&#39;), Synset(&#39;swap.v.02&#39;), Synset(&#39;swash.v.01&#39;), Synset(&#39;swim.v.01&#39;), Synset(&#39;swim.v.05&#39;), Synset(&#39;swing.v.03&#39;), Synset(&#39;taxi.v.01&#39;), Synset(&#39;trail.v.03&#39;), Synset(&#39;tram.v.01&#39;), Synset(&#39;transfer.v.06&#39;), Synset(&#39;travel.v.04&#39;), Synset(&#39;travel.v.05&#39;), Synset(&#39;travel.v.06&#39;), Synset(&#39;travel_by.v.01&#39;), Synset(&#39;travel_purposefully.v.01&#39;), Synset(&#39;travel_rapidly.v.01&#39;), Synset(&#39;trundle.v.01&#39;), Synset(&#39;turn.v.06&#39;), Synset(&#39;walk.v.01&#39;), Synset(&#39;walk.v.10&#39;), Synset(&#39;weave.v.04&#39;), Synset(&#39;wend.v.01&#39;), Synset(&#39;wheel.v.03&#39;), Synset(&#39;whine.v.01&#39;), Synset(&#39;whish.v.02&#39;), Synset(&#39;whisk.v.02&#39;), Synset(&#39;whistle.v.02&#39;), Synset(&#39;withdraw.v.01&#39;), Synset(&#39;zigzag.v.01&#39;), Synset(&#39;zoom.v.02&#39;)] word_synsets[0].root_hypernyms() # root [Synset(&#39;travel.v.01&#39;)] word_synsets[0].hypernym_paths() # from root to this synset in WordNet [[Synset(&#39;travel.v.01&#39;), Synset(&#39;walk.v.01&#39;)]] Synonyms # Collect synonyms of all synsets synonyms = [] for syn in wn.synsets(&#39;book&#39;, pos=&#39;v&#39;): for lemma in syn.lemmas(): synonyms.append(lemma.name()) len(synonyms) 6 len(set(synonyms)) 3 print(set(synonyms)) {&#39;reserve&#39;, &#39;hold&#39;, &#39;book&#39;} Antonyms # First Synset Lemma String word_synsets[0].lemmas()[0].name() &#39;walk&#39; # Antonyms of the First Synset word_synsets[0].lemmas()[0].antonyms()[0] Lemma(&#39;ride.v.02.ride&#39;) While previous taxonomic relations (e.g., hypernymy and hyponymy) are in-between synsets, the synonymy and antonymy relations are in-between lemmas. We need to be very clear about the use of the three variants in WordNet: Word Form Lemma Synset 19.3.3 Semantic Similarity Computation Synsets are organized in a hypernym tree, which can be used for reasoning about the semantic similarity of two Synsets. The closer the two Synsets are in the tree, the more similar they are. # syn1 = wn.synsets(&#39;walk&#39;, pos=&#39;v&#39;)[0] syn1 = wn.synset(&#39;walk.v.01&#39;) syn2 = wn.synset(&#39;toddle.v.01&#39;) syn3 = wn.synset(&#39;think.v.01&#39;) syn1.wup_similarity(syn2) 0.8 syn1.wup_similarity(syn3) 0.2857142857142857 syn1.path_similarity(syn2) 0.5 syn1.path_similarity(syn3) 0.16666666666666666 ref = syn1.hypernyms()[0] syn1.shortest_path_distance(ref) 1 syn2.shortest_path_distance(ref) 2 syn1.shortest_path_distance(syn2) 1 print(ref.definition()) change location; move, travel, or proceed, also metaphorically print([l.name() for l in ref.lemmas()]) [&#39;travel&#39;, &#39;go&#39;, &#39;move&#39;, &#39;locomote&#39;] syn1.hypernym_paths() [[Synset(&#39;travel.v.01&#39;), Synset(&#39;walk.v.01&#39;)]] syn2.hypernym_paths() [[Synset(&#39;travel.v.01&#39;), Synset(&#39;walk.v.01&#39;), Synset(&#39;toddle.v.01&#39;)]] syn3.hypernym_paths() [[Synset(&#39;think.v.03&#39;), Synset(&#39;evaluate.v.02&#39;), Synset(&#39;think.v.01&#39;)]] The wup_similarity method is short for Wu-Pamler Similarity. It is a scoring method for how similar the word senses are based on where the Synsets occur relative to each other in the hypernym tree. For more information or other scoring methods, please check NLTK WordNet documentation. 19.4 Discovering Word Collocations from nltk.corpus import brown from nltk.collocations import BigramCollocationFinder from nltk.metrics import BigramAssocMeasures words = [w.lower() for w in brown.words()] bcf = BigramCollocationFinder.from_words(words) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4) [(&#39;;&#39;, &#39;;&#39;), (&#39;?&#39;, &#39;?&#39;), (&#39;of&#39;, &#39;the&#39;), (&#39;.&#39;, &#39;``&#39;)] # deal with stopwords from nltk.corpus import stopwords stopset = set(stopwords.words(&#39;english&#39;)) ## Fitler critera: ## remove words whose length &lt; 3 or which are on the stop word list filter_stops = lambda w: len(w) &lt; 3 or w in stopset bcf.apply_word_filter(filter_stops) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10) [(&#39;united&#39;, &#39;states&#39;), (&#39;new&#39;, &#39;york&#39;), (&#39;per&#39;, &#39;cent&#39;), (&#39;years&#39;, &#39;ago&#39;), (&#39;rhode&#39;, &#39;island&#39;), (&#39;los&#39;, &#39;angeles&#39;), (&#39;peace&#39;, &#39;corps&#39;), (&#39;san&#39;, &#39;francisco&#39;), (&#39;high&#39;, &#39;school&#39;), (&#39;fiscal&#39;, &#39;year&#39;)] ## apply freq-based filter bcf.apply_freq_filter(3) bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10) [(&#39;united&#39;, &#39;states&#39;), (&#39;new&#39;, &#39;york&#39;), (&#39;per&#39;, &#39;cent&#39;), (&#39;years&#39;, &#39;ago&#39;), (&#39;rhode&#39;, &#39;island&#39;), (&#39;los&#39;, &#39;angeles&#39;), (&#39;peace&#39;, &#39;corps&#39;), (&#39;san&#39;, &#39;francisco&#39;), (&#39;high&#39;, &#39;school&#39;), (&#39;fiscal&#39;, &#39;year&#39;)] Exercise 19.1 Try to identify bigram collocations in the corpus, Alice in the Wonderland. The texts are available in nltk.corpus.gutenburg. Exercise 19.2 Following the same strategy of bigram collocation extraction, please try to extract trigrams from the brown corpus. Remove stop words and short words as we did in the lecture. Include only trigrams whose frequency &gt; 5. 19.5 Tokenization import nltk from nltk.tokenize import word_tokenize, sent_tokenize text = &quot;&quot;&quot;Three blind mice! See how they run! They all ran after the farmer&#39;s wife, Who cut off their tails with a carving knife. Did you ever see such a thing in your life As three blind mice? &quot;&quot;&quot; text_sent = sent_tokenize(text) text_word = word_tokenize(text) text_pos = nltk.pos_tag(text_word) With words, we can create a frequency list: import pprint as pp text_fd= nltk.FreqDist([w.lower() for w in text_word]) pp.pprint(text_fd.most_common(10)) [(&#39;three&#39;, 2), (&#39;blind&#39;, 2), (&#39;mice&#39;, 2), (&#39;!&#39;, 2), (&#39;see&#39;, 2), (&#39;they&#39;, 2), (&#39;a&#39;, 2), (&#39;how&#39;, 1), (&#39;run&#39;, 1), (&#39;all&#39;, 1)] Exercise 19.3 Provide the word frequency list of the top 30 nouns in Alice in the Wonderland. The raw texts of the novel is available in NTLK (see below). alice = nltk.corpus.gutenberg.raw(fileids=&#39;carroll-alice.txt&#39;) [(&#39;Alice&#39;, 392), (&#39;Queen&#39;, 71), (&#39;time&#39;, 65), (&#39;King&#39;, 60), (&#39;Turtle&#39;, 58), (&#39;Mock&#39;, 56), (&#39;Hatter&#39;, 55), (&#39;*&#39;, 54), (&#39;Gryphon&#39;, 54), (&#39;way&#39;, 53), (&#39;head&#39;, 50), (&#39;thing&#39;, 49), (&#39;voice&#39;, 47), (&#39;Rabbit&#39;, 44), (&#39;Duchess&#39;, 42), (&#39;tone&#39;, 40), (&#39;Dormouse&#39;, 40), (&#39;March&#39;, 34), (&#39;moment&#39;, 31), (&#39;Hare&#39;, 31), (&#39;nothing&#39;, 30), (&#39;things&#39;, 30), (&#39;door&#39;, 30), (&#39;Mouse&#39;, 29), (&#39;eyes&#39;, 28), (&#39;Caterpillar&#39;, 27), (&#39;day&#39;, 25), (&#39;course&#39;, 25), (&#39;Cat&#39;, 25), (&#39;round&#39;, 23)] 19.6 Chinese Word Segmentation import jieba text = &quot;&quot;&quot; 高速公路局說，目前在國道3號北向水上系統至中埔路段車多壅塞，已回堵約3公里。另外，國道1號北向仁德至永康路段路段，已回堵約有7公里。建議駕駛人提前避開壅塞路段改道行駛，行經車多路段請保持行車安全距離，小心行駛。 國道車多壅塞路段還有國1內湖-五堵北向路段、楊梅-新竹南向路段；國3三鶯-關西服務區南向路段、快官-霧峰南向路段、水上系統-中埔北向路段；國6霧峰系統-東草屯東向路段、國10燕巢-燕巢系統東向路段。 &quot;&quot;&quot; text_jb = jieba.lcut(text) Building prefix dict from the default dictionary ... Loading model from cache /var/folders/70/qfdgs0k52qj24jtjcz7d0dkm0000gn/T/jieba.cache Loading model cost 0.228 seconds. Prefix dict has been built successfully. print(&#39; | &#39;.join(text_jb)) | 高速公路 | 局說 | ， | 目前 | 在 | 國道 | 3 | 號 | 北向 | 水上 | 系統 | 至 | 中埔 | 路段 | 車多 | 壅塞 | ， | 已回 | 堵約 | 3 | 公里 | 。 | 另外 | ， | 國道 | 1 | 號 | 北向 | 仁德 | 至 | 永康 | 路段 | 路段 | ， | 已回 | 堵 | 約 | 有 | 7 | 公里 | 。 | 建議 | 駕駛人 | 提前 | 避開 | 壅塞 | 路段 | 改道 | 行駛 | ， | 行經車 | 多 | 路段 | 請 | 保持 | 行車 | 安全 | 距離 | ， | 小心 | 行駛 | 。 | | 國道 | 車多 | 壅塞 | 路段 | 還有國 | 1 | 內湖 | - | 五堵 | 北向 | 路段 | 、 | 楊梅 | - | 新竹 | 南向 | 路段 | ； | 國 | 3 | 三鶯 | - | 關 | 西服 | 務區 | 南向 | 路段 | 、 | 快官 | - | 霧峰 | 南向 | 路段 | 、 | 水上 | 系統 | - | 中埔 | 北向 | 路段 | ； | 國 | 6 | 霧峰 | 系統 | - | 東 | 草屯 | 東向 | 路段 | 、 | 國 | 10 | 燕巢 | - | 燕巢 | 系統 | 東向 | 路段 | 。 | 19.7 Afterwords There are many other aspects that we need to take into account when processing texts. Text segmentation is a non-trivial task in natural language processing. The base units that we work with will turn out to be connected to the specific research questions we aim to asnwer. If you would like to know more about computational text analytics, I would highly recommend you to move on to two more advanced courses: ENC2036 Corpus Linguistics ENC2045 Computational Linguistics "],["web-scraping.html", "Chapter 20 Web Scraping 20.1 webbrowswer module 20.2 requests Module 20.3 bs4 Module (Beautiful Soup) 20.4 Reference", " Chapter 20 Web Scraping Web scraping is the term for using a program to download and process content from the web. webbrowser: A default Python module to open a browser to specific page. requests: A module to download files and web pages from the Internet. bs4: A modile to parse HTML, i.e., the format that web pages are written in. selenium: A module to launch and control a web browser (e.g., filling in forms, simulating mouse clicks.) 20.1 webbrowswer module Create a python script with the following codes, named py-checkword.py #! python3 import webbrowser, sys, pyperclip if len(sys.argv) &gt; 1: # Get input from the command line target = &#39; &#39;.join(sys.argv[1:]) else: # Get input from the clipboard target = pyperclip.paste() webbrowser.open(&#39;https://www.dictionary.com/browse/&#39;+ target) Run the python script in the terminal python py-checkword.py beauty Exercise 20.1 How to modify the py-checkword.py so that the user can attach a list of words separated by spaces for checking? For example, the modified script will be able to open three web browsers for beauty, internet, and national. python py-checkword2.py beauty internet national 20.2 requests Module The requests modules allow us to easily download files from the web without having to worry about complicated issues such as network errors, connection problems, and data compression. import requests res = requests.get(&#39;https://www.gutenberg.org/files/2591/2591-0.txt&#39;) type(res) &lt;class &#39;requests.models.Response&#39;&gt; ## Check status code to see if the download is successful res.status_code == requests.codes.ok ## `requests.codes.ok` == 200 True len(res.text) 560045 print(res.text[:250]) ï»¿The Project Gutenberg eBook of Grimmsâ Fairy Tales, by Jacob Grimm and Wilhelm Grimm This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. Prepare potential errors during the file download import requests res = requests.get(&#39;https://www.gutenberg.org/file-that-does-not-exist.txt&#39;) ## Check status code to see if the download is successful res.status_code == requests.codes.ok ## `requests.codes.ok` == 200 False len(res.text) 6414 print(res.text[:250]) &lt;!DOCTYPE html&gt; &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;/&gt; &lt;title&gt;404 | Project Gutenberg&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;/gutenberg/style.css?v=1.1&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;/gutenberg/collapsible.css res.raise_for_status() 404 Client Error: Not Found for url: https://www.gutenberg.org/file-that-does-not-exist.txt A better way to modify the codes is to make sure that the program stops as soon as some unexpected error happens. Always call raise_for_status() after calling requests.get() because we need to make sure the file has been successfully downloaded before the program continues. import requests res = requests.get(&#39;https://www.gutenberg.org/file-that-does-not-exist.txt&#39;) try: res.raise_for_status() except Exception as exc: print(&#39;There was a problem with the link: %s&#39; % (exc)) There was a problem with the link: 404 Client Error: Not Found for url: https://www.gutenberg.org/file-that-does-not-exist.txt Usually we may want to scrape the texts from the web and save them on the Hard Drive. import requests res = requests.get(&#39;https://www.gutenberg.org/files/2591/2591-0.txt&#39;) try: res.raise_for_status() except Exception as exc: print(&#39;There was a problem with the link: %s&#39; % (exc)) with open(&#39;grimms.txt&#39;, &#39;w&#39;) as f: f.write(res.text) 560045 20.3 bs4 Module (Beautiful Soup) Beautiful Soup is a module for extracting information from an HTML page. The package name is pip install -U beatifulsoup4 but in use, it is import bs4. Each Word is a dict: “headword”: head word string “pronunciation”: IPA “parts-of-speech”: A list of senses {“definition”: ’‘, “example”:’’} import requests, bs4 target=&#39;individual&#39; res = requests.get(&#39;https://www.dictionary.com/browse/&#39; + target) res.raise_for_status() soup = bs4.BeautifulSoup(res.text, &#39;lxml&#39;) entries = soup.select(&#39;.css-1avshm7&#39;) # entries ## Define the word structure (dict) cur_word = {} # for each entry for i, entry in enumerate(entries): ## Include only the main entry of the page if len(entry.select(&#39;h1&#39;)) &gt; 0: #print(&#39;Entry Number: &#39;, i) ## headword and pronunciations cur_headword = entry.select(&#39;h1&#39;)[0].getText() cur_spell = entry.select(&#39;.pron-spell-content&#39;)[0].getText() cur_ipa = entry.select(&#39;.pron-ipa-content&#39;)[0].getText().encode(&#39;utf-8&#39;).decode(&#39;utf-8&#39;) #print(&#39;Headword: &#39;, cur_headword) #print(&#39;Pronunciation: &#39;, cur_ipa) cur_word[&#39;headword&#39;] = cur_headword cur_word[&#39;pronunciation&#39;] = cur_ipa # for each POS type in the current entry for pos in entry.select(&#39;.css-pnw38j&#39;): cur_pos = pos.select(&#39;.luna-pos&#39;)[0].getText() #print(&#39;=&#39;*10) #print(&#39;POS: &#39;, cur_pos.upper()) cur_definitions = pos.select(&#39;div[value]&#39;) cur_sense_list =[] # for each definition in the current POS for sense in cur_definitions: #print(&#39;DEF: &#39; + sense.find(text=True, recursive=True)) ## check if there&#39;s any example ex = sense.find(attrs={&#39;class&#39;:&#39;luna-example&#39;}) if ex is not None: cur_ex = ex.getText() _ = ex.extract() else: cur_ex = &#39;&#39; cur_def = sense.getText() #print(&#39;-&#39;*10) #print(&#39;Definition: &#39; + cur_def) #print(&#39;Example: &#39;+ cur_ex) cur_sense = {&#39;definition&#39;: cur_def, &#39;example&#39;: cur_ex} cur_sense_list.append(cur_sense) cur_word[cur_pos] = cur_sense_list import json with open(target+&#39;.json&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: json.dump(cur_word, f, ensure_ascii=False) print(json.dumps(cur_word, sort_keys=False, indent=4, ensure_ascii=False)) import json with open(&#39;produce.json&#39;,&#39;r&#39;, encoding=&#39;utf-8&#39;) as f: cur_word = json.load(f) cur_word.keys() dict_keys([&#39;headword&#39;, &#39;pronunciation&#39;, &#39;verb (used with object),&#39;, &#39;verb (used without object),&#39;, &#39;noun&#39;]) print(json.dumps(cur_word, sort_keys=False, indent=4, ensure_ascii=False)) { &quot;headword&quot;: &quot;produce&quot;, &quot;pronunciation&quot;: &quot;/ verb prəˈdus, -ˈdyus; noun ˈprɒd us, -yus, ˈproʊ dus, -dyus /&quot;, &quot;verb (used with object),&quot;: [ { &quot;definition&quot;: &quot;to bring into existence; give rise to; cause: &quot;, &quot;example&quot;: &quot;to produce steam.&quot; }, { &quot;definition&quot;: &quot;to bring into existence by intellectual or creative ability: &quot;, &quot;example&quot;: &quot;to produce a great painting.&quot; }, { &quot;definition&quot;: &quot;to make or manufacture: &quot;, &quot;example&quot;: &quot;to produce automobiles for export.&quot; }, { &quot;definition&quot;: &quot;to bring forth; give birth to; bear: &quot;, &quot;example&quot;: &quot;to produce a litter of puppies.&quot; }, { &quot;definition&quot;: &quot;to provide, furnish, or supply; yield: &quot;, &quot;example&quot;: &quot;a mine producing silver.&quot; }, { &quot;definition&quot;: &quot;Finance. &quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;to cause to accrue: &quot;, &quot;example&quot;: &quot;stocks producing unexpected dividends.&quot; }, { &quot;definition&quot;: &quot;to bring forward; present to view or notice; exhibit: &quot;, &quot;example&quot;: &quot;to produce one&#39;s credentials.&quot; }, { &quot;definition&quot;: &quot;to bring (a play, movie, opera, etc.) before the public.&quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;to extend or prolong, as a line.&quot;, &quot;example&quot;: &quot;&quot; } ], &quot;verb (used without object),&quot;: [ { &quot;definition&quot;: &quot;to create, bring forth, or yield offspring, products, etc.: &quot;, &quot;example&quot;: &quot;Their mines are closed because they no longer produce.&quot; }, { &quot;definition&quot;: &quot;Economics. &quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;to create economic value; bring crops, goods, etc., to a point at which they will command a price.&quot;, &quot;example&quot;: &quot;&quot; } ], &quot;noun&quot;: [ { &quot;definition&quot;: &quot;something that is produced; yield; product. &quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;agricultural products collectively, especially vegetables and fruits.&quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;offspring, especially of a female animal: &quot;, &quot;example&quot;: &quot;the produce of a mare.&quot; } ] } Exercise 20.2 Now how to extend this short script to allow the users to perform searches of multiple words all at once, scrape all definitions and examples from the website of Dictionary.com, and save them to the Hard Drive as json files in a specific directory? checkwords(targets=[&quot;individual&quot;, &quot;wonderful&quot;], outdir = &#39;dictionary_results/&#39;) import json outdir = &#39;dictionary_results/&#39; with open(outdir+&#39;individual.json&#39;,&#39;r&#39;) as f: cur_word = json.load(f) print(json.dumps(cur_word, sort_keys=False, indent=4, ensure_ascii=False)) { &quot;headword&quot;: &quot;individual&quot;, &quot;pronunciation&quot;: &quot;/ ˌɪn dəˈvɪdʒ u əl /&quot;, &quot;noun&quot;: [ { &quot;definition&quot;: &quot;a single human being, as distinguished from a group.&quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;a person: &quot;, &quot;example&quot;: &quot;a strange individual.&quot; }, { &quot;definition&quot;: &quot;a distinct, indivisible entity; a single thing, being, instance, or item.&quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;a group considered as a unit.&quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;Biology. a single organism capable of independent existence. a member of a compound organism or colony.&quot;, &quot;example&quot;: &quot;&quot; }, { &quot;definition&quot;: &quot;Cards. a duplicate-bridge tournament in which each player plays the same number of hands in partnership with every other player, individual scores for each player being kept for each hand.&quot;, &quot;example&quot;: &quot;&quot; } ], &quot;adjective&quot;: [ { &quot;definition&quot;: &quot;single; particular; separate: &quot;, &quot;example&quot;: &quot;to number individual copies of a limited edition.&quot; }, { &quot;definition&quot;: &quot;intended for the use of one person only: &quot;, &quot;example&quot;: &quot;to serve individual portions of a pizza.&quot; }, { &quot;definition&quot;: &quot;of, relating to, or characteristic of a particular person or thing: &quot;, &quot;example&quot;: &quot;individual tastes.&quot; }, { &quot;definition&quot;: &quot;distinguished by special, singular, or markedly personal characteristics; exhibiting unique or unusual qualities: &quot;, &quot;example&quot;: &quot;a highly individual style of painting.&quot; }, { &quot;definition&quot;: &quot;existing as a distinct, indivisible entity, or considered as such; discrete: &quot;, &quot;example&quot;: &quot;individual parts of a tea set.&quot; }, { &quot;definition&quot;: &quot;of which each is different or of a different design from the others: &quot;, &quot;example&quot;: &quot;a set of individual coffee cups.&quot; } ] } 20.4 Reference Beautiful Soup Documentation "],["transcribing.html", "Chapter 21 Transcribing 21.1 Working with Audio Files 21.2 Working with Microphone Inputs 21.3 References", " Chapter 21 Transcribing In this unit, we look at an example of speech recognition using python. In language studies, we often need to transcribe the audio data into texts for further study. This unit will show you how we can utilize python to automate the speech-to-text transcribing. There are in general two sources of audio data: From a audio file (e.g., .wav) From the system microphone (e.g., speech created on the fly) 21.1 Working with Audio Files Recognizer: First we need to initilize a Recognizer object, which is mainly responsible for the speech-to-text recognition. AudioFile: Create an AudioFile object with a path to the audio file AudioData: Process the audio file and record the data from the AudioFile into an AudioData object. Choose the API for Speech-to-Text conversion: Use the Recognizer’s method, Recognizer.recognize_google(), to recognize speech in the AudioData. import speech_recognition as sr print(sr.__version__) 3.10.4 r = sr.Recognizer() #r.recognize_google() havard = sr.AudioFile(&#39;demo_data/audio/語音測試.wav&#39;) with havard as source: ## adjust for noise #r.adjust_for_ambient_noise(source) audio = r.record(source) type(havard) &lt;class &#39;speech_recognition.AudioFile&#39;&gt; type(audio) &lt;class &#39;speech_recognition.audio.AudioData&#39;&gt; r.recognize_google(audio, language=&#39;zh-TW&#39;) &#39;我現在在做基本的語音測試謝謝&#39; The speech_recognition library uses the FLAC format internally for audio processing. The library relies on an external tool, the FLAC command-line utility, to handle FLAC audio files. If this utility is not installed on your system, the library will raise an OSError. Please install FLAC before using the library. 21.2 Working with Microphone Inputs Recognizer: First we need to initilize a Recognizer object, which is mainly responsible for the speech-to-text recognition. Microphone: Create an Microphone object with a specific index to the system microphone AudioData: Record the speech data from the Microphone into an AudioData object. Choose the API for Speech-to-Text conversion: Use the Recognizer’s method, Recognizer.recognize_google(), to recognize speech in the AudioData. import speech_recognition as sr r = sr.Recognizer() mic = sr.Microphone() sr.Microphone.list_microphone_names() mic = sr.Microphone(device_index=0) with mic as source: audio = r.listen(source) type(audio) try: r.recognize_google(audio, language=&#39;zh&#39;) except sr.UnknownValueError: print(&#39;Unable to recognize the speech.&#39;) 21.3 References The Ultimate Guide to Speech Recognition with Python Harvard Sentences: These phrases were published by the IEEE in 1965 for use in speech intelligibility testing of telephone lines. They are still used in VoIP and cellular testing today. Available recordings of these sentences can be found on the Open Speech Repository website. "],["the-shell.html", "A The Shell A.1 Why do you need to know shell commands? A.2 Shebang Line A.3 Basic Shell Commands A.4 Text-Analytic Related Commands A.5 References", " A The Shell A.1 Why do you need to know shell commands? The command line has many great advantages that can make you a more efficient and productive data scientist. Janssens (2014) has nicely summarized the strengths of command lines in five points: The command line is agile The command line is augmenting The command line is scalable The command line is extensible The command line is ubiquitouos A.2 Shebang Line Like in R console, you can interact with the R console line by line or you can package all your R scripts in a file. For command line, you can also combine several shell commands into a script file. For a shell script file, you need a shebang line, which is the character sequence consisting of the characters number sign and exclamation mark (#!) at the beginning of a script. This line would indicate to the shell engine which interpreter (language environment) is needed to parse the script file. A shell script often takes a shebang line as below. #!/bin/sh #!/bin/bash A.3 Basic Shell Commands Linux Man Pages The most basic commands are listed below: pwd (print working directory). Shows directory or “folder” you are currently operating in. This is not necessarily the same as the R working directory you get from getwd(). ls (list files). Shows the files in the current working directory. This is equivalent to looking at the files in your Finder/Explorer/File Manager. Use ls -a to also list hidden files, such as .Rhistory and .git. cd (change directory). Allows you to navigate through your directories by changing the shell’s working directory. You can navigate like so: go to subdirectory foo of current working directory: cd foo go to parent of current working directory: cd .. go to your “home” directory: cd ~ or simply cd go to directory using absolute path, works regardless of your current working directory: cd /home/my_username/Desktop. Windows uses a slightly different syntax with the slashes between the folder names reversed, \\, e.g. cd C:\\Users\\MY_USERNAME\\Desktop. Pro tip 1: Dragging and dropping a file or folder into the terminal window will paste the absolute path into the window. Pro tip 2: Use the tab key to autocomplete unambiguous directory and file names. Hit tab twice to see all ambiguous options. Use arrow-up and arrow-down to repeat previous commands. Or search for previous commands with CTRL + r. which Show the full path of a shell commands which python: Check which version of python your system uses which r: Check which version of R your system uses cp Copy files and directories rm Remove files and directories mv Move files and directories mkdir Make directories A.4 Text-Analytic Related Commands gzip [-cd#] FILENAME. Zips/Unzips a file. The zipped file created by gzip is often with the extension *.gz. -c: write output on standard output -d: decompress -#: Regulate the speed of compression using the specified digit #, where -1 or –fast indicates the fastest compression method (less compression) and -9 or –best indicates the slowest compression method (best compression). The default compression level is -6. tar [-j|-z] [cv] [-f FILENAME] filename.... Archive many files into one single file -j: Use bzip2 compression -z: Use gzip compression -c: Create a new archive -v: Print all files processed verbosely tar [-j|-z] [xv] [-f FILENAME] [-C PATH. Restore the original files from the achive file -j: Use bzip2 compression -z: Use gzip compression -x: Extract files from archive -v: Print all files processed verbosely C: Extract files to a particular path So when you see a file with the extention of *.tar.gz, this indicates that this file is a zipped file, which compresses a list of multiple files into one tar archive. Usually people pass their collection of multiple text files (i.e., a corpus) in this way because it is often easier to share one file instead of tons of files at a time. A.5 References If you are interested in more functions and potentials of shell commands, I would highly recommend the book Data Science at the Command Line. References Janssens, J. (2014). Data science at the command line: Facing the future with time-tested tools. \" O’Reilly Media, Inc.\". "],["reproducible-report.html", "B Reproducible Report B.1 R Markdown B.2 Open Science B.3 Installing R Markdown B.4 R Markdown Components B.5 Tips B.6 References", " B Reproducible Report Creating reproducible reports is important in data analysis, or in general, open science. The idea behind the reproducible reports is that any readers should be able to implement the same analysis on the dataset and obtain the same results. A reproducible report usually includes: Your codes for data processing and analysis Your results of the data analysis Your descriptions, summaries and/or interpretation of the analysis A reproducible report can be presented in many different forms: HTML PDF MS-DOC Slide Formats Website B.1 R Markdown R markdown is a variant of the Markdown language, which is a markup language that provides a way of creating easy to read plain text file which can incorporate formatted text, images, headers and links to other documents. Another classic example of the markup language is the HTML (Hypertext Markup Language) B.2 Open Science Open science is a common goal in the academia. As a member of this scientific communicity, we hope that everyone can do as much as they can to make their data, methods, results and inferences transparent and available to everyone. There are several important tenets for open science (See Open science): Transparency in experimental methodology, observation, collection of data and analytical methods. Public availability and re-usability of scientific data Public accessibility and transparency of scientific communication Using web-based tools to facilitate scientific collaboration Scenario I After data collection, you load the data into R and write R code to explore and analyze the data and save the code in an R script. Then you save the analysis results and plots as external files and manually combine all of these and your written prose into an MS Word Document. Scenario II After data collection, you use R for data exploration and analysis as well. But this time you include all the R code used for exploration and analysis, as well as the analysis outputs (statistical reports and graphs) and your written text in one single R markdown document. This R markdown document can be used to automatically create the final document (e.g., PDF, DOC, HTML etc.). B.3 Installing R Markdown # Install from CRAN install.packages(&#39;rmarkdown&#39;, dep = TRUE) If you need to generate PDF output from R markdown, you will need to install LaTeX. If you don’t have LaTeX yet, it is recommended that you install TinyTeX. ## Optional (Only needed for PDF format) install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() # install TinyTeX B.4 R Markdown Components Let’s check the R Markdown Reference Guide. YAML header: metadata and options for the entire document Formatted text: texts with markup formatting Code chunks: R code (or any other language code) Adding figures: R-generated graphs Adding tables: R-generated tables Inline R code: In-text R code B.5 Tips We can control the text length to make sure that the codes do not run off the page edge when rendering the R markdown to PDF. knitr::opts_chunk$set( message = FALSE, warning = FALSE, tidy.opts = list(width.cutoff = 60) ) We can suppress the startup messages and/or warnings. suppressPackageStartupMessages(library(ggplot2)) We can re-format the R code in the markdown in a tidy way. knitr::opts_chunk$set(message=FALSE, tidy.opts=list(width.cutoff=60), tidy=TRUE) B.6 References R Markdown: The Definitive Guide R Markdown Reference Guide R Markdown Cheatsheet "],["references-5.html", "References", " References Davies, T. M. (2016). The book of R: A first course in programming and statistics (1st ed.). No Starch Press, Inc. Gerrard, P. (2016). Lean python: Learn just enough python to build useful tools. Apress. Janssens, J. (2014). Data science at the command line: Facing the future with time-tested tools. \" O’Reilly Media, Inc.\". Sweigart, A. (2020). Automate the boring stuff with python: Practical programming for total beginners. 2nd edition. No Starch Press. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
